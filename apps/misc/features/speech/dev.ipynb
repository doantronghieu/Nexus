{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Whisper"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from enum import Enum\n",
				"from loguru import logger\n",
				"from pprint import pprint\n",
				"\n",
				"class Model(Enum):\n",
				"\t\"\"\"\n",
				"\twhisper.available_models()\n",
				" \n",
				"\thttps://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages\n",
				" \t\"\"\"\n",
				"\tTINY_EN = \"tiny.en\"\n",
				"\tTINY = \"tiny\"\n",
				"\tBASE_EN = \"base.en\"\n",
				"\tBASE = \"base\"\n",
				"\tSMALL_EN = \"small.en\"\n",
				"\tSMALL = \"small\"\n",
				"\tMEDIUM_EN = \"medium.en\"\n",
				"\tMEDIUM = \"medium\"\n",
				"\tLARGE_V1 = \"large-v1\"\n",
				"\tLARGE_V2 = \"large-v2\"\n",
				"\tLARGE_V3 = \"large-v3\"\n",
				"\tLARGE = \"large\"\n",
				"\n",
				"class Device(Enum):\n",
				"\tCPU = \"cpu\"\n",
				"\tCUDA = \"cuda\"\n",
				"\n",
				"class ComputeType(Enum):\n",
				"\tINT8 = \"int8\"\n",
				"\tINT8_FLOAT16 = \"int8_float16\"\n",
				"\tFLOAT16 = \"float16\"\n",
				"\n",
				"# model = whisper.load_model(name=Model.TINY.value)\n",
				"\n",
				"path_audio = f\"{packages.ROOT_PATH}/data/assets/Car_AI_Assistant_Intro.mp3\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from faster_whisper import WhisperModel\n",
				"\n",
				"model = WhisperModel(\n",
				"\t# Model.TINY.value, device=Device.CUDA.value, compute_type=ComputeType.FLOAT16.value,\n",
				"\t# Model.TINY.value, device=Device.CUDA.value, compute_type=ComputeType.INT8_FLOAT16.value,\n",
				"\tModel.TINY.value, device=Device.CPU.value, compute_type=ComputeType.INT8.value,\n",
				")\n",
				"\n",
				"segments, info = model.transcribe(\n",
				"  path_audio, beam_size=5, \n",
				"\tvad_filter=False, vad_parameters=dict(min_silence_duration_ms=500),\n",
				")\n",
				"\n",
				"logger.info(f\"Detected language: {info.language} ({info.language_probability:.3f})%\")\n",
				"\n",
				"result = \"\"\n",
				"for seg in segments:\n",
				"\tresult += seg.text\n",
				"\n",
				"pprint(result)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy as np\n",
				"import subprocess\n",
				"import os\n",
				"from concurrent.futures import ThreadPoolExecutor\n",
				"import math\n",
				"\n",
				"def export_chunk(start_time, duration, input_file, output_path):\n",
				"    command = [\n",
				"        'ffmpeg',\n",
				"        '-ss', str(start_time),\n",
				"        '-i', input_file,\n",
				"        '-t', str(duration),\n",
				"        '-acodec', 'libmp3lame',\n",
				"        '-ar', '44100',\n",
				"        '-ab', '192k',\n",
				"        '-y',\n",
				"        output_path\n",
				"    ]\n",
				"    subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
				"\n",
				"def get_audio_duration(input_file):\n",
				"    command = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', input_file]\n",
				"    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
				"    return float(result.stdout)\n",
				"\n",
				"def split_audio(input_file, split_duration):\n",
				"    # Get the total duration of the audio file\n",
				"    total_duration = get_audio_duration(input_file)\n",
				"\n",
				"    # Calculate the number of chunks\n",
				"    num_chunks = math.ceil(total_duration / split_duration)\n",
				"\n",
				"    # Generate start times for each chunk\n",
				"    start_times = np.linspace(0, total_duration - split_duration, num_chunks)\n",
				"\n",
				"    # Get the base name of the input file (without the extension)\n",
				"    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
				"\n",
				"    # Create output directory if it doesn't exist\n",
				"    output_dir = f\"{base_name}_chunks\"\n",
				"    os.makedirs(output_dir, exist_ok=True)\n",
				"\n",
				"    # Use ThreadPoolExecutor for I/O bound operations\n",
				"    with ThreadPoolExecutor() as executor:\n",
				"        # Submit tasks to the executor\n",
				"        futures = []\n",
				"        for i, start_time in enumerate(start_times):\n",
				"            output_path = os.path.join(output_dir, f\"{base_name}_{i}.mp3\")\n",
				"            futures.append(executor.submit(export_chunk, start_time, split_duration, input_file, output_path))\n",
				"\n",
				"        # Wait for all tasks to complete\n",
				"        for future in futures:\n",
				"            future.result()\n",
				"\n",
				"# Usage\n",
				"split_audio(path_audio, 2)  # Split the file into 2-second chunks"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import numpy as np\n",
				"from pydub import AudioSegment\n",
				"from faster_whisper import WhisperModel\n",
				"from concurrent.futures import ProcessPoolExecutor, as_completed\n",
				"import logging\n",
				"import time\n",
				"\n",
				"# Set up logging\n",
				"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
				"logger = logging.getLogger(__name__)\n",
				"\n",
				"def initialize_model():\n",
				"    return WhisperModel(\"tiny\", device=\"cpu\", compute_type=\"int8\")\n",
				"\n",
				"def transcribe_chunk(args):\n",
				"    chunk_path, chunk_index = args\n",
				"    try:\n",
				"        model = initialize_model()\n",
				"        logger.info(f\"Starting transcription of {chunk_path}\")\n",
				"        start_time = time.time()\n",
				"        segments, info = model.transcribe(\n",
				"            chunk_path, beam_size=5,\n",
				"            vad_filter=False, vad_parameters=dict(min_silence_duration_ms=500),\n",
				"        )\n",
				"        duration = time.time() - start_time\n",
				"        logger.info(f\"Finished transcription of {chunk_path} in {duration:.2f} seconds\")\n",
				"        return chunk_index, \"\".join(segment.text for segment in segments), info.language, info.language_probability\n",
				"    except Exception as e:\n",
				"        logger.error(f\"Error transcribing {chunk_path}: {str(e)}\")\n",
				"        return chunk_index, \"\", \"\", 0.0\n",
				"\n",
				"def split_and_transcribe(input_file, chunk_duration=30):\n",
				"    try:\n",
				"        logger.info(f\"Loading audio file: {input_file}\")\n",
				"        audio = AudioSegment.from_file(input_file)\n",
				"        logger.info(f\"Audio duration: {len(audio) / 1000:.2f} seconds\")\n",
				"\n",
				"        chunk_length_ms = chunk_duration * 1000\n",
				"        chunks = [audio[i:i+chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
				"        logger.info(f\"Split audio into {len(chunks)} chunks\")\n",
				"\n",
				"        temp_dir = \"temp_chunks\"\n",
				"        os.makedirs(temp_dir, exist_ok=True)\n",
				"\n",
				"        chunk_paths = []\n",
				"        for i, chunk in enumerate(chunks):\n",
				"            chunk_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
				"            chunk.export(chunk_path, format=\"wav\")\n",
				"            chunk_paths.append((chunk_path, i))\n",
				"            logger.info(f\"Exported chunk {i+1}/{len(chunks)}\")\n",
				"\n",
				"        results = [None] * len(chunks)\n",
				"        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
				"            future_to_path = {executor.submit(transcribe_chunk, args): args for args in chunk_paths}\n",
				"            for future in as_completed(future_to_path):\n",
				"                path, index = future_to_path[future]\n",
				"                try:\n",
				"                    chunk_index, text, lang, prob = future.result()\n",
				"                    results[chunk_index] = (text, lang, prob)\n",
				"                    logger.info(f\"Completed transcription of {path}\")\n",
				"                except Exception as e:\n",
				"                    logger.error(f\"Exception occurred for {path}: {str(e)}\")\n",
				"\n",
				"        full_transcript = \"\".join(result[0] for result in results if result[0])\n",
				"        languages = [result[1] for result in results if result[1]]\n",
				"        probabilities = [result[2] for result in results if result[2] > 0]\n",
				"        \n",
				"        detected_language = max(set(languages), key=languages.count) if languages else \"\"\n",
				"        language_probability = sum(probabilities) / len(probabilities) if probabilities else 0.0\n",
				"\n",
				"        for path, _ in chunk_paths:\n",
				"            os.remove(path)\n",
				"        os.rmdir(temp_dir)\n",
				"\n",
				"        return full_transcript, detected_language, language_probability\n",
				"    except Exception as e:\n",
				"        logger.error(f\"Error in split_and_transcribe: {str(e)}\")\n",
				"        return \"\", \"\", 0.0\n",
				"\n",
				"def experiment_chunk_durations(input_file, durations):\n",
				"    results = []\n",
				"    for duration in durations:\n",
				"        start_time = time.time()\n",
				"        transcript, language, probability = split_and_transcribe(input_file, chunk_duration=duration)\n",
				"        end_time = time.time()\n",
				"        processing_time = end_time - start_time\n",
				"        results.append((duration, processing_time, len(transcript)))\n",
				"        print(f\"Chunk duration: {duration}s, Processing time: {processing_time:.2f}s, Transcript length: {len(transcript)}\")\n",
				"    \n",
				"    # Find the optimal duration\n",
				"    optimal_duration = min(results, key=lambda x: x[1])\n",
				"    print(f\"\\nOptimal chunk duration: {optimal_duration[0]}s with processing time: {optimal_duration[1]:.2f}s\")\n",
				"\n",
				"    return results\n",
				"\n",
				"input_file = path_audio\n",
				"\n",
				"# transcript, language, probability = split_and_transcribe(input_file, 30)\n",
				"# logger.info(f\"Detected language: {language} ({probability:.3f})\")\n",
				"# logger.info(\"Transcript:\")\n",
				"# logger.info(transcript)\n",
				"\n",
				"durations_to_test = [5, 10, 15, 20, 30, 45, 60]  # Durations in seconds\n",
				"experiment_results = experiment_chunk_durations(input_file, durations_to_test)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## [openai/whisper](https://github.com/openai/whisper)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"# import whisper\n",
				"\n",
				"# audio = whisper.load_audio(path_audio)\n",
				"# audio = whisper.pad_or_trim(audio) # 30s\n",
				"\n",
				"# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
				"\n",
				"# _, probs = model.detect_language(mel)\n",
				"# logger.info(f\"Detected language: {max(probs, key=probs.get)}\")\n",
				"\n",
				"# options = whisper.DecodingOptions()\n",
				"# result = whisper.decode(model, mel, options)\n",
				"\n",
				"# pprint(result.text)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"---"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [KoljaB/RealtimeSTT](https://github.com/KoljaB/RealtimeSTT)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# [dscripka/openWakeWord](https://github.com/dscripka/openWakeWord)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from openwakeword import utils, model\n",
				"# from openwakeword.model import Model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# One-time download of all pre-trained models (or only select models)\n",
				"utils.download_models(target_directory=f\"{packages.ROOT_PATH}/data/assets/repos/openWakeWord/models\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_audio_frame():\n",
				"\tpass\n",
				"\n",
				"# Get audio data containing 16-bit 16khz PCM audio data from a file, microphone, network stream, etc.\n",
				"# For the best efficiency and latency, audio frames should be multiples of 80 ms, with longer frames\n",
				"# increasing overall efficiency at the cost of detection latency\n",
				"frame = get_audio_frame()\n",
				"\n",
				"model = model.Model(inference_framework=\"onnx\")\n",
				"\n",
				"pred = model.predict(frame)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

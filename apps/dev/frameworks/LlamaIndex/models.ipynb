{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Imports"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from configs import settings, const, components\n",
				"from configs.settings import logger\n",
				"import asyncio, os, time, yaml, json, datetime, copy, random\n",
				"from typing import Any, AsyncGenerator, Generator, Callable, Literal, Optional, TypeAlias, Union\n",
				"from tqdm import tqdm\n",
				"from pprint import pprint\n",
				"\n",
				"from toolkit.llm.llama_index import (\n",
				"\tagents, cores, deploys as dpls, evaluation, messages, models, \n",
				"\tobservability, types, utils as utils_llama_index, workflows as wfs\n",
				")\n",
				"from toolkit.llm.llama_index.data import loading, querying, storing\n",
				"\n",
				"from features.agents.car.tools import VehicleDB\n",
				"from features.agents.tools import map\n",
				"\n",
				"from toolkit.utils import utils, typer as t\n",
				"from toolkit.utils.llm import measure_performance, main as utils_llm\n",
				"from toolkit.utils.utils import rp_print"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Test Model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"queries = [\n",
				"\t\"Hello\",\n",
				"\t\"Tell me a joke\",\n",
				"\t\"Tell me a long joke\",\n",
				"\t\"Tell me a super long joke\",\n",
				"\t\"My name is John\",\n",
				"\t\"What is my name\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def messages_to_prompt(messages):\n",
				"    \"\"\"\n",
				"    Format messages for Mistral model instruction format.\n",
				"    Prevents prompt contamination and ensures proper message structure.\n",
				"    \"\"\"\n",
				"    # Clean and validate the messages\n",
				"    cleaned_messages = []\n",
				"    for msg in messages:\n",
				"        # Remove any existing instruction tags to prevent contamination\n",
				"        content = str(msg).replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
				"        cleaned_messages.append(content)\n",
				"    \n",
				"    # Join messages with clear separation\n",
				"    prompt = \" \".join(cleaned_messages)\n",
				"    \n",
				"    # Apply single instruction wrapper\n",
				"    return f\"<s>[INST] {prompt} [/INST]</s>\"\n",
				"\n",
				"def completion_to_prompt(completion):\n",
				"    \"\"\"\n",
				"    Format completion for Mistral model instruction format.\n",
				"    Prevents prompt contamination.\n",
				"    \"\"\"\n",
				"    # Clean the completion of any instruction tags\n",
				"    cleaned_completion = str(completion).replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
				"    return f\"<s>[INST] {cleaned_completion} [/INST]</s>\"\n",
				"\n",
				"llm_vllm = models.OpenLLM(\n",
				"\tapi_base=f'http://localhost:{os.getenv(\"PORT_SVC_LLM_VLLM\")}/v1',\n",
				"\t# model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
				"\tmodel=const.Model.Vllm.LLAMA_3_2_1B_INST,\n",
				"\tmax_tokens=512,\n",
				"\ttemperature=0.0,\n",
				"\tadditional_kwargs={\n",
				"\t\t\"frequency_penalty\": 0.5,\n",
				"\t\t# \"repetition_penalty\": 1.2,\n",
				"\t\t# \"length_penalty\": 1.1,\n",
				"\t},\n",
				"\tis_chat_model=True,\n",
				"\tis_function_calling_model=True,\n",
				"\tstrict=True,\n",
				"\t# messages_to_prompt=messages_to_prompt,\n",
				"\t# completion_to_prompt=completion_to_prompt,\n",
				")\n",
				"\n",
				"llm_nano = models.OpenAI(\n",
				"\tapi_base=f'http://localhost:{os.getenv(\"PORT_SVC_LLM_NANO\")}/v1',\n",
				")\n",
				"\n",
				"llm = llm_vllm"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query = queries[2]\n",
				"\n",
				"token_generator = utils_llama_index.interact_model(\n",
				"  prompt=user_query, mode=\"astream\", user_query=None,\n",
				"  measure_performance=True,\n",
				"  llm=llm,\n",
				"\t# llm=None, # Use cores.Settings.llm\n",
				")\n",
				"\n",
				"async for token in await token_generator:\n",
				"  print(token, end=\"\", flush=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = await utils_llama_index.interact_model(\n",
				"\tllm=llm,\n",
				"\tuser_query=queries[2],\n",
				"\tmode=\"chat\",\n",
				")\n",
				"\n",
				"rp_print(result)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

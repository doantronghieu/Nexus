{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Imports"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from configs import settings, const, components\n",
				"\n",
				"from configs.settings import logger\n",
				"import asyncio, os, time, yaml, json, datetime, copy\n",
				"from typing import Any, AsyncGenerator, Generator, Callable, Literal, Optional, TypeAlias, Union\n",
				"from tqdm import tqdm\n",
				"from pprint import pprint\n",
				"from rich import print as rprint\n",
				"\n",
				"from toolkit.llm.llama_index import (\n",
				"\tagents, cores, deploys as dpls, evaluation, messages, models, \n",
				"\tobservability, types, utils as utils_llama_index, workflows as wfs\n",
				")\n",
				"from toolkit.llm.llama_index.data import loading, querying, storing\n",
				"\n",
				"from toolkit.utils import utils, typer as t\n",
				"from toolkit.utils.llm import measure_performance, main as utils_llm\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# User Query Categorization"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import json\n",
				"from typing import Dict, List, Tuple, Set, TypeAlias, Literal, Optional, Callable\n",
				"from collections import defaultdict\n",
				"import yaml\n",
				"from tqdm import tqdm\n",
				"from rich.console import Console\n",
				"from rich.table import Table\n",
				"from rich import print as rprint\n",
				"\n",
				"def predict_categories(\n",
				"\t\tinput_file: str,\n",
				"\t\toutput_file: str,\n",
				"\t\tpredict_fn: Callable[[str], str],\n",
				"\t\tvalid_categories: Set[str],\n",
				"\t\tquery_field: str = \"user_query\",\n",
				"\t\ttrue_category_field: str = \"category\",\n",
				"\t\tpred_category_field: str = \"pred_category\",\n",
				"\t\tdefault_category: Optional[str] = None\n",
				") -> None:\n",
				"\t\t\"\"\"\n",
				"\t\tGeneric function to predict categories for items in a JSON file.\n",
				"\t\t\n",
				"\t\tArgs:\n",
				"\t\t\t\tinput_file: Path to input JSON file with items to classify\n",
				"\t\t\t\toutput_file: Path to output JSON file for results\n",
				"\t\t\t\tpredict_fn: Function that takes a query string and returns a predicted category\n",
				"\t\t\t\tvalid_categories: Set of valid category names\n",
				"\t\t\t\tquery_field: Name of the field containing the query in input JSON\n",
				"\t\t\t\ttrue_category_field: Name of the field containing the true category\n",
				"\t\t\t\tpred_category_field: Name of the field to store predicted category\n",
				"\t\t\t\tdefault_category: Default category to use when prediction fails (if None, will raise error)\n",
				"\t\t\t\t\n",
				"\t\tRaises:\n",
				"\t\t\t\tFileNotFoundError: If input file doesn't exist\n",
				"\t\t\t\tjson.JSONDecodeError: If input file is not valid JSON\n",
				"\t\t\t\tValueError: If input data is invalid\n",
				"\t\t\t\tIOError: If writing to output file fails\n",
				"\t\t\"\"\"\n",
				"\t\t# 1. Input Validation\n",
				"\t\tif not input_file or not output_file:\n",
				"\t\t\t\traise ValueError(\"Input and output file paths must be provided\")\n",
				"\t\t\n",
				"\t\tif not valid_categories:\n",
				"\t\t\t\traise ValueError(\"Valid categories set must not be empty\")\n",
				"\n",
				"\t\t# 2. Load and Validate Input Data\n",
				"\t\ttry:\n",
				"\t\t\t\twith open(input_file, 'r') as f:\n",
				"\t\t\t\t\t\titems = json.load(f)\n",
				"\t\texcept FileNotFoundError:\n",
				"\t\t\t\traise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
				"\t\texcept json.JSONDecodeError:\n",
				"\t\t\t\traise ValueError(f\"Invalid JSON in input file: {input_file}\")\n",
				"\t\t\n",
				"\t\tif not items or not isinstance(items, list):\n",
				"\t\t\t\traise ValueError(\"Input file must contain a non-empty list of items\")\n",
				"\t\t\n",
				"\t\t# Validate required fields in first item\n",
				"\t\trequired_fields = {query_field, true_category_field}\n",
				"\t\tif not items[0] or not all(field in items[0] for field in required_fields):\n",
				"\t\t\t\traise ValueError(f\"Missing required fields in input data: {required_fields}\")\n",
				"\t\t\n",
				"\t\t# 3. Process Items\n",
				"\t\tprocessed_items = []\n",
				"\t\tfor item in tqdm(items, desc=\"Processing items\"):\n",
				"\t\t\t\ttry:\n",
				"\t\t\t\t\t\tquery = item[query_field]\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\t\t\t# Get prediction\n",
				"\t\t\t\t\t\tpred_category = predict_fn(query)\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\t\t\t# Validate prediction\n",
				"\t\t\t\t\t\tif pred_category not in valid_categories:\n",
				"\t\t\t\t\t\t\t\tif default_category is None:\n",
				"\t\t\t\t\t\t\t\t\t\traise ValueError(f\"Invalid prediction '{pred_category}' and no default category provided\")\n",
				"\t\t\t\t\t\t\t\tprint(f\"Warning: Invalid prediction '{pred_category}' for query: {query}\")\n",
				"\t\t\t\t\t\t\t\tpred_category = default_category\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\t\t\t# Add to processed items\n",
				"\t\t\t\t\t\titem[pred_category_field] = pred_category\n",
				"\t\t\t\t\t\tprocessed_items.append(item)\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\texcept Exception as e:\n",
				"\t\t\t\t\t\tif default_category is None:\n",
				"\t\t\t\t\t\t\t\traise Exception(f\"Error processing query '{query}': {str(e)}\")\n",
				"\t\t\t\t\t\tprint(f\"Error processing query '{query}': {str(e)}\")\n",
				"\t\t\t\t\t\titem[pred_category_field] = default_category\n",
				"\t\t\t\t\t\tprocessed_items.append(item)\n",
				"\t\t\n",
				"\t\t# 4. Write Results\n",
				"\t\ttry:\n",
				"\t\t\t\twith open(output_file, 'w') as f:\n",
				"\t\t\t\t\t\tjson.dump(processed_items, f, indent=2)\n",
				"\t\texcept Exception as e:\n",
				"\t\t\t\traise IOError(f\"Error writing to output file {output_file}: {str(e)}\")\n",
				"\n",
				"def evaluate_predictions(\n",
				"\t\tprediction_file: str,\n",
				"\t\ttrue_category_field: str = \"category\",\n",
				"\t\tpred_category_field: str = \"pred_category\",\n",
				"\t\tquery_field: str = \"user_query\",\n",
				"\t\tvalid_categories: Optional[Set[str]] = None\n",
				") -> Tuple[float, Dict]:\n",
				"\t\t\"\"\"\n",
				"\t\tGeneric function to evaluate classification predictions.\n",
				"\t\t\n",
				"\t\tArgs:\n",
				"\t\t\t\tprediction_file: Path to JSON file containing true and predicted categories\n",
				"\t\t\t\ttrue_category_field: Name of the field containing true category\n",
				"\t\t\t\tpred_category_field: Name of the field containing predicted category\n",
				"\t\t\t\tquery_field: Name of the field containing the query text\n",
				"\t\t\t\tvalid_categories: Optional set of valid categories for validation\n",
				"\t\t\t\t\n",
				"\t\tReturns:\n",
				"\t\t\t\tTuple containing:\n",
				"\t\t\t\t- overall accuracy (float)\n",
				"\t\t\t\t- detailed metrics dictionary with confusion matrix and per-category metrics\n",
				"\t\t\t\t\n",
				"\t\tRaises:\n",
				"\t\t\t\tFileNotFoundError: If prediction file doesn't exist\n",
				"\t\t\t\tValueError: If prediction file is empty or missing required fields\n",
				"\t\t\"\"\"\n",
				"\t\tconsole = Console()\n",
				"\t\t\n",
				"\t\t# 1. Load and Validate Input Data\n",
				"\t\ttry:\n",
				"\t\t\t\twith open(prediction_file, 'r') as f:\n",
				"\t\t\t\t\t\tpredictions = json.load(f)\n",
				"\t\texcept FileNotFoundError:\n",
				"\t\t\t\traise FileNotFoundError(f\"Prediction file not found: {prediction_file}\")\n",
				"\t\texcept json.JSONDecodeError:\n",
				"\t\t\t\traise ValueError(f\"Invalid JSON in prediction file: {prediction_file}\")\n",
				"\t\t\n",
				"\t\tif not predictions:\n",
				"\t\t\t\traise ValueError(\"Empty prediction file\")\n",
				"\t\t\n",
				"\t\t# Validate required fields\n",
				"\t\trequired_fields = {query_field, true_category_field, pred_category_field}\n",
				"\t\tif not all(field in predictions[0] for field in required_fields):\n",
				"\t\t\t\traise ValueError(f\"Missing required fields: {required_fields}\")\n",
				"\t\t\n",
				"\t\t# 2. Initialize Data Structures\n",
				"\t\ttotal = len(predictions)\n",
				"\t\tcorrect = 0\n",
				"\t\tconfusion_matrix = defaultdict(lambda: defaultdict(int))\n",
				"\t\tcategory_metrics = defaultdict(lambda: {\n",
				"\t\t\t\t'correct': 0,\n",
				"\t\t\t\t'total': 0,\n",
				"\t\t\t\t'incorrect_examples': [],\n",
				"\t\t\t\t'precision': 0.0,\n",
				"\t\t\t\t'recall': 0.0,\n",
				"\t\t\t\t'f1': 0.0\n",
				"\t\t})\n",
				"\t\t\n",
				"\t\t# Get all unique categories\n",
				"\t\tall_categories: Set[str] = set()\n",
				"\t\tfor item in predictions:\n",
				"\t\t\t\tall_categories.add(item[true_category_field])\n",
				"\t\t\t\tall_categories.add(item[pred_category_field])\n",
				"\t\t\n",
				"\t\t# Validate categories if provided\n",
				"\t\tif valid_categories:\n",
				"\t\t\t\tinvalid_categories = all_categories - valid_categories\n",
				"\t\t\t\tif invalid_categories:\n",
				"\t\t\t\t\t\tprint(f\"Warning: Found invalid categories: {invalid_categories}\")\n",
				"\t\t\n",
				"\t\t# 3. Calculate Basic Metrics\n",
				"\t\tfor item in predictions:\n",
				"\t\t\t\ttrue_category = item[true_category_field]\n",
				"\t\t\t\tpred_category = item[pred_category_field]\n",
				"\t\t\t\tquery = item[query_field]\n",
				"\t\t\t\t\n",
				"\t\t\t\tcategory_metrics[true_category]['total'] += 1\n",
				"\t\t\t\tconfusion_matrix[true_category][pred_category] += 1\n",
				"\t\t\t\t\n",
				"\t\t\t\tif true_category == pred_category:\n",
				"\t\t\t\t\t\tcorrect += 1\n",
				"\t\t\t\t\t\tcategory_metrics[true_category]['correct'] += 1\n",
				"\t\t\t\telse:\n",
				"\t\t\t\t\t\tcategory_metrics[true_category]['incorrect_examples'].append({\n",
				"\t\t\t\t\t\t\t\t'query': query,\n",
				"\t\t\t\t\t\t\t\t'true': true_category,\n",
				"\t\t\t\t\t\t\t\t'predicted': pred_category\n",
				"\t\t\t\t\t\t})\n",
				"\t\t\n",
				"\t\t# 4. Calculate Advanced Metrics\n",
				"\t\taccuracy = correct / total if total > 0 else 0\n",
				"\t\t\n",
				"\t\tfor category in category_metrics:\n",
				"\t\t\t\tmetrics = category_metrics[category]\n",
				"\t\t\t\ttotal_cat = metrics['total']\n",
				"\t\t\t\tmetrics['accuracy'] = metrics['correct'] / total_cat if total_cat > 0 else 0\n",
				"\t\t\t\t\n",
				"\t\t\t\ttrue_positives = confusion_matrix[category][category]\n",
				"\t\t\t\tfalse_positives = sum(confusion_matrix[other_cat][category] \n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor other_cat in all_categories if other_cat != category)\n",
				"\t\t\t\tfalse_negatives = sum(confusion_matrix[category][other_cat] \n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor other_cat in all_categories if other_cat != category)\n",
				"\t\t\t\t\n",
				"\t\t\t\tmetrics['precision'] = (true_positives / (true_positives + false_positives) \n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif (true_positives + false_positives) > 0 else 0)\n",
				"\t\t\t\tmetrics['recall'] = (true_positives / (true_positives + false_negatives)\n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\tif (true_positives + false_negatives) > 0 else 0)\n",
				"\t\t\t\t\n",
				"\t\t\t\tif metrics['precision'] + metrics['recall'] > 0:\n",
				"\t\t\t\t\t\tmetrics['f1'] = (2 * metrics['precision'] * metrics['recall'] / \n",
				"\t\t\t\t\t\t\t\t\t\t\t\t\t(metrics['precision'] + metrics['recall']))\n",
				"\t\t\t\telse:\n",
				"\t\t\t\t\t\tmetrics['f1'] = 0.0\n",
				"\t\t\n",
				"\t\t# 5. Prepare Output Metrics\n",
				"\t\tdetailed_metrics = {\n",
				"\t\t\t\t'total_samples': total,\n",
				"\t\t\t\t'correct_predictions': correct,\n",
				"\t\t\t\t'overall_accuracy': accuracy,\n",
				"\t\t\t\t'per_category_metrics': dict(category_metrics),\n",
				"\t\t\t\t'confusion_matrix': dict(confusion_matrix)\n",
				"\t\t}\n",
				"\t\t\n",
				"\t\t# 6. Print Results\n",
				"\t\t# Overall metrics table\n",
				"\t\tconsole.print(\"\\n[bold cyan]Evaluation Results:[/bold cyan]\")\n",
				"\t\toverall_table = Table(show_header=True, header_style=\"bold magenta\")\n",
				"\t\toverall_table.add_column(\"Metric\", style=\"bright_blue\")\n",
				"\t\toverall_table.add_column(\"Value\", justify=\"right\", style=\"bright_yellow\")\n",
				"\t\t\n",
				"\t\toverall_table.add_row(\"Total samples\", str(total))\n",
				"\t\toverall_table.add_row(\"Correct predictions\", str(correct))\n",
				"\t\toverall_table.add_row(\"Overall accuracy\", f\"{accuracy:.2%}\")\n",
				"\t\t\n",
				"\t\tconsole.print(overall_table)\n",
				"\t\t\n",
				"\t\t# Per-category performance table\n",
				"\t\tconsole.print(\"\\n[bold cyan]Per-category Performance:[/bold cyan]\")\n",
				"\t\tperf_table = Table(show_header=True, header_style=\"bold magenta\")\n",
				"\t\tperf_table.add_column(\"Category\", style=\"cyan\")\n",
				"\t\tperf_table.add_column(\"Accuracy\", justify=\"right\")\n",
				"\t\tperf_table.add_column(\"Precision\", justify=\"right\")\n",
				"\t\tperf_table.add_column(\"Recall\", justify=\"right\")\n",
				"\t\tperf_table.add_column(\"F1 Score\", justify=\"right\")\n",
				"\t\tperf_table.add_column(\"Correct/Total\", justify=\"right\")\n",
				"\t\t\n",
				"\t\tfor category in sorted(category_metrics.keys()):\n",
				"\t\t\t\tmetrics = category_metrics[category]\n",
				"\t\t\t\tperf_table.add_row(\n",
				"\t\t\t\t\t\tcategory,\n",
				"\t\t\t\t\t\tf\"{metrics['accuracy']:.2%}\",\n",
				"\t\t\t\t\t\tf\"{metrics['precision']:.2%}\",\n",
				"\t\t\t\t\t\tf\"{metrics['recall']:.2%}\",\n",
				"\t\t\t\t\t\tf\"{metrics['f1']:.2%}\",\n",
				"\t\t\t\t\t\tf\"{metrics['correct']}/{metrics['total']}\"\n",
				"\t\t\t\t)\n",
				"\t\t\n",
				"\t\tconsole.print(perf_table)\n",
				"\t\t\n",
				"\t\t# Confusion matrix\n",
				"\t\tconsole.print(\"\\n[bold cyan]Confusion Matrix:[/bold cyan]\")\n",
				"\t\tmatrix_table = Table(show_header=True, header_style=\"bold magenta\")\n",
				"\t\tmatrix_table.add_column(\"True ↓ Pred →\", style=\"cyan\")\n",
				"\t\tfor category in sorted(all_categories):\n",
				"\t\t\t\tmatrix_table.add_column(category, justify=\"right\")\n",
				"\t\t\n",
				"\t\tfor true_cat in sorted(all_categories):\n",
				"\t\t\t\trow = [true_cat]\n",
				"\t\t\t\tfor pred_cat in sorted(all_categories):\n",
				"\t\t\t\t\t\trow.append(str(confusion_matrix[true_cat][pred_cat]))\n",
				"\t\t\t\tmatrix_table.add_row(*row)\n",
				"\t\t\n",
				"\t\tconsole.print(matrix_table)\n",
				"\t\t\n",
				"\t\treturn accuracy, detailed_metrics\n",
				"\n",
				"# Example usage for the car query classification\n",
				"if __name__ == \"__main__\":\n",
				"\t\ttry:\n",
				"\t\t\t\tprompts_agent_car = settings.prompts_agent_car\n",
				"\t\t\n",
				"\t\t\t\t# Define prediction function for car queries\n",
				"\t\t\t\tdef predict_car_query_category(query: str) -> str:\n",
				"\t\t\t\t\t\texamples = components.retriever_user_query_category.retrieve(query)\n",
				"\t\t\t\t\t\texamples = str(await utils_llama_index.extract_retriever_results(examples))\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\t\t\tpred_category = await utils_llama_index.interact_model(\n",
				"\t\t\t\t\t\t\t\tprompt=prompts_agent_car[\"CategorizeQuestion\"][\"dev\"],\n",
				"\t\t\t\t\t\t\t\tmode=\"achat\",\n",
				"\t\t\t\t\t\t\t\tuser_query=query,\n",
				"\t\t\t\t\t\t\t\texamples=examples,\n",
				"\t\t\t\t\t\t)\n",
				"\t\t\t\t\t\t\n",
				"\t\t\t\t\t\treturn await utils_llm.post_process_llm_output(\n",
				"\t\t\t\t\t\t\t\tpred_category,\n",
				"\t\t\t\t\t\t\t\tmode=[\"remove_quotes\", \"remove_brackets\"],\n",
				"\t\t\t\t\t\t)\n",
				"\t\t\t\t\n",
				"\t\t\t\t# Set up parameters\n",
				"\t\t\t\tVALID_CATEGORIES = {\"car_control\", \"car_manual\"}\n",
				"\t\t\t\tinput_file = f\"{packages.APP_PATH}/data/QnAs/user_query_category.json\"\n",
				"\t\t\t\toutput_file = f\"{packages.APP_PATH}/data/QnAs/pred-user_query_category.json\"\n",
				"\t\t\t\t\n",
				"\t\t\t\t# Predict categories\n",
				"\t\t\t\tpredict_categories(\n",
				"\t\t\t\t\t\tinput_file=input_file,\n",
				"\t\t\t\t\t\toutput_file=output_file,\n",
				"\t\t\t\t\t\tpredict_fn=predict_car_query_category,\n",
				"\t\t\t\t\t\tvalid_categories=VALID_CATEGORIES,\n",
				"\t\t\t\t\t\tquery_field=\"user_query\",\n",
				"\t\t\t\t\t\ttrue_category_field=\"user_query_category\",\n",
				"\t\t\t\t\t\tpred_category_field=\"pred_user_query_category\",\n",
				"\t\t\t\t\t\tdefault_category=\"car_manual\"\n",
				"\t\t\t\t)\n",
				"\t\t\t\t\n",
				"\t\t\t\t# Evaluate predictions\n",
				"\t\t\t\taccuracy, metrics = evaluate_predictions(\n",
				"\t\t\t\t\t\tprediction_file=output_file,\n",
				"\t\t\t\t\t\ttrue_category_field=\"user_query_category\",\n",
				"\t\t\t\t\t\tpred_category_field=\"pred_user_query_category\",\n",
				"\t\t\t\t\t\tquery_field=\"user_query\",\n",
				"\t\t\t\t\t\tvalid_categories=VALID_CATEGORIES\n",
				"\t\t\t\t)\n",
				"\t\t\t\t\n",
				"\t\texcept Exception as e:\n",
				"\t\t\t\trprint(f\"[bold red]Error during execution:[/bold red] {str(e)}\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

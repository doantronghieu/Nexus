{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Imports"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from configs import settings, const, components\n",
				"from configs.settings import logger\n",
				"import asyncio, os, time, yaml, json, datetime, copy, random\n",
				"from typing import Any, AsyncGenerator, Generator, Callable, Literal, Optional, TypeAlias, Union\n",
				"from tqdm import tqdm\n",
				"from pprint import pprint\n",
				"\n",
				"from toolkit.llm.llama_index import (\n",
				"\tagents, cores, deploys as dpls, evaluation, messages, models, \n",
				"\tobservability, types, utils as utils_llama_index, workflows as wfs\n",
				")\n",
				"from toolkit.llm.llama_index.data import loading, querying, storing\n",
				"\n",
				"from features.agents.car.tools import VehicleDB\n",
				"from features.agents.tools import map\n",
				"\n",
				"from toolkit.utils import utils, typer as t\n",
				"from toolkit.utils.llm import measure_performance, main as utils_llm\n",
				"from toolkit.utils.utils import rp_print"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Measure Performance"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def run_query_engine(\n",
				"  query: str, \n",
				"  query_engine: querying.BaseQueryEngine, \n",
				"  print_in_streaming=True\n",
				"):\n",
				"    start_time = time.time()\n",
				"    first_token_time = None\n",
				"    total_response = \"\"\n",
				"\n",
				"    if not query_engine._response_synthesizer._streaming:\n",
				"        response = query_engine.query(query)\n",
				"        total_time = time.time() - start_time\n",
				"        return response, None, total_time\n",
				"\n",
				"    else:\n",
				"        response = query_engine.query(query)\n",
				"        \n",
				"        for i, token in enumerate(response.response_gen):\n",
				"            if i == 0:\n",
				"                first_token_time = time.time() - start_time\n",
				"            \n",
				"            total_response += token\n",
				"            if print_in_streaming:\n",
				"                print(token, end=\"\", flush=True)\n",
				"        \n",
				"        total_time = time.time() - start_time\n",
				"        \n",
				"        \n",
				"        return total_response, first_token_time, total_time\n",
				"\n",
				"# Example usage:\n",
				"# query = \"What did the author do growing up?\"\n",
				"query = \"What are some notes before driving the car?\"\n",
				"# response, first_token_time, total_time = run_query_engine(query, query_engine, print_in_streaming=False)\n",
				"\n",
				"# print(f\"Time to first token: {first_token_time:.4f} seconds\")\n",
				"# print(f\"Total response time: {total_time:.4f} seconds\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"notebookRunGroups": {
					"groupValue": "1"
				}
			},
			"outputs": [],
			"source": [
				"query = \"What are some notes before driving the car?\"\n",
				"\n",
				"metadata = measure_performance.Metadata(\n",
				"\tdataset=\"car_manual\",\n",
				"\talgorithm=\"basic\",\n",
				"\tmodel=chosen_model,\n",
				"\tinference_server=\"Ollama\",\n",
				")\n",
				"\n",
				"is_run_loop = False # <-- Set to True to run the loop\n",
				"\n",
				"if is_run_loop:\n",
				"\tfor i in range(0, 20):\n",
				"\t\tresult = measure_performance.execute_query(query_engine, run_query_engine, query, metadata=metadata)\n",
				"else:\n",
				"\tresult = measure_performance.execute_query(query_engine, run_query_engine, query, metadata=metadata)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"measure_performance.json_to_csv(\n",
				"\tjson_file_path=f\"{packages.APP_PATH}/data/logs/performance_logs.json\",\n",
				"\tcsv_file_path=f\"{packages.APP_PATH}/data/logs/performance_logs.csv\"\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Tmp"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from toolkit.llm.langchain import document_loaders\n",
				"\n",
				"file_path = f\"{packages.APP_PATH}/data/org/test/manual_toyota_corolla_cross_2023.pdf\"\n",
				"loader = document_loaders.UnstructuredPDFLoader(file_path)\n",
				"documents = loader.load()\n",
				"\n",
				"docs_content = documents[0].page_content\n",
				"docs_first_half = docs_content[:len(docs_content)//2]\n",
				"docs_second_half = docs_content[len(docs_content)//2:]\n",
				"\n",
				"print(docs_second_half)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"\n",
				"with open(f'{packages.APP_PATH}/data/org/raw-manual_toyota_corolla_cross_2023.txt', 'r') as file:\n",
				"\tcontent = file.read()\n",
				"\n",
				"# Get total length of content\n",
				"total_length = len(content)\n",
				"\n",
				"# Calculate the size of each part\n",
				"part_size = total_length // 4\n",
				"\n",
				"# Create 4 variables with roughly equal parts\n",
				"part1 = content[0:part_size]\n",
				"part2 = content[part_size:part_size*2]\n",
				"part3 = content[part_size*2:part_size*3]\n",
				"part4 = content[part_size*3:]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Timing"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### LLM"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"llm = models.OpenAI(api_base=\"http://localhost:8767/v1\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"token_generator = utils_llama_index.interact_model(\n",
				"\tllm=llm,\n",
				"  prompt=\"Tell me a joke\", mode=\"astream\", user_query=None,\n",
				"  measure_performance=True,\n",
				")\n",
				"\n",
				"async for token in await token_generator:\n",
				"  print(token, end=\"\", flush=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# response = llm.complete(\"Tell me a joke\")  # Uses completions endpoint\n",
				"\n",
				"response = llm.chat([\n",
				"\tmessages.ChatMessage(role=messages.MessageRole.USER, content=\"Hello\")\n",
				"])\n",
				"\n",
				"# response = await utils_llama_index.interact_model(\n",
				"# \tllm=llm,\n",
				"# \tuser_query=\"Tell me a joke\",\n",
				"# \tmode=\"chat\"\n",
				"# )\n",
				"\n",
				"rp_print(response)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Pipeline"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompts_agent_car = settings.prompts_agent_car\n",
				"\n",
				"with open(f\"{packages.APP_PATH}/use_cases/dev/features/rag/prompts.yaml\", 'r') as file:\n",
				"  prompts_rag = yaml.safe_load(file)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### user_query_category"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompt_categorize_query = prompts_agent_car[\"CategorizeQuestion\"][\"dev\"]\n",
				"\n",
				"# user_query = \"Is the car trunk opened?\"\n",
				"user_query = \"How can parents prevent children from accidentally opening doors or windows while driving?\"\n",
				"\n",
				"examples = await components.retriever_user_query_category.aretrieve(user_query)\n",
				"examples = str(await utils_llama_index.extract_retriever_results(examples))\n",
				"\n",
				"user_query_category: t.UserQueryCategory = await utils_llama_index.interact_model(\n",
				"\tprompt=prompt_categorize_query, mode=\"chat\", user_query=user_query,\n",
				"\texamples=examples,\n",
				"\tllm=llm,\n",
				")\n",
				"user_query_category = await utils_llm.post_process_llm_output(\n",
				"\tuser_query_category, mode=[\"remove_quotes\", \"remove_brackets\", \"remove_tokens\"],\n",
				")\n",
				"\n",
				"rp_print(user_query_category)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### RAG"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"queries = [\n",
				"\t\"What are the main safety procedures to follow before driving the vehicle?\",\n",
				"\t\"What is the correct driving posture recommended in the manual?\",\n",
				"]\n",
				"\n",
				"user_query = queries[0]\n",
				"\n",
				"retrieved_data = await components.retriever_car_manual.aretrieve(user_query)\n",
				"retrieved_data_texts = await utils_llama_index.extract_text(retrieved_data)\n",
				"\n",
				"response = await utils_llama_index.interact_model(\n",
				"\tprompt=prompts_rag[\"generate_result\"], system_prompt=prompts_rag[\"system\"][\"car_manual\"],\n",
				"\tmode=\"achat\", \n",
				"\tuser_question=user_query, retrieved_data=retrieved_data_texts,\n",
				"\t# llm=llm,\n",
				")\n",
				"\n",
				"response = await utils_llm.post_process_llm_output(\n",
				"\tresponse, mode=[\"remove_tokens\"],\n",
				")\n",
				"rp_print(response)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"components.retriever_car_manual._vector_store.client.__dir__()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"rp_print(components.retriever_car_manual.__dir__())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"await apis_rag.do_querying(user_query=user_query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### separate_tasks"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query = \"Is the car locked? Is the car trunk opened?. Increase front wiper speed\"\n",
				"\n",
				"prompt_separate_tasks = prompts_agent_car[\"control\"][\"SeparateTasks\"][\"dev\"]\n",
				"\n",
				"tasks = await utils_llama_index.interact_model(\n",
				"\tprompt=prompt_separate_tasks, mode=\"achat\", user_query=user_query,\n",
				"\tllm=llm,\n",
				")\n",
				"tasks = await utils_llm.post_process_llm_output(\n",
				"\ttasks, mode=[\"remove_tokens\"],\n",
				")\n",
				"tasks = await utils_llm.parse_json(tasks)\n",
				"\n",
				"rp_print(tasks)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### control"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"tools = agents.add_tools(\n",
				"\t[agents.FunctionTool.from_defaults(async_fn=VehicleDB.db_mongo_vehicle.process_user_query)],\n",
				")\n",
				"\n",
				"llm_control = llm\n",
				"llm_control.system_prompt = prompts_agent_car[\"control\"][\"System_VehicleDB\"][\"dev\"]\n",
				"\n",
				"# agent = agents.OpenAIAgent.from_tools(\n",
				"# \ttools=tools,\n",
				"# \tverbose=True,\n",
				"# \tllm=llm,\n",
				"# \tsystem_prompt=prompts_agent_car[\"control\"][\"System_VehicleDB\"][\"dev\"], # System prompt\n",
				"# )\n",
				"\n",
				"agent = agents.AgentRunner.from_llm(\n",
				"\t\tllm=llm,\n",
				"\t\ttools=tools, # type: ignore\n",
				"\t\tverbose=True,\n",
				"\t\tcontext=prompts_agent_car[\"control\"][\"System_VehicleDB\"][\"dev\"], # System prompt\n",
				"\t)\n",
				"\n",
				"#*==============================================================================\n",
				"\n",
				"user_query = \"Is the car locked?\"\n",
				"\n",
				"await utils_llama_index.interact_agent(\n",
				"\tagent=agent,\n",
				"\tuser_query=user_query,\n",
				"\tmode=\"achat\",\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"rp_print(llm.metadata)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Setup"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\"\"\"\n",
				"HF_API_KEY\n",
				"\n",
				"docker run --runtime nvidia --gpus all \\\n",
				"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
				"    --env \"HUGGING_FACE_HUB_TOKEN=HF_API_KEY\" \\\n",
				"    -p 8000:8000 \\\n",
				"    --ipc=host \\\n",
				"    vllm/vllm-openai:latest \\\n",
				"    --model mistralai/Mistral-7B-v0.1\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Sources"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"- **getting_started**\n",
				"  - https://docs.llamaindex.ai/en/stable/#introduction ✅\n",
				"  - https://docs.llamaindex.ai/en/stable/getting_started/starter_example/ ✅\n",
				"  - https://docs.llamaindex.ai/en/stable/getting_started/concepts/ ✅\n",
				"  - https://docs.llamaindex.ai/en/stable/getting_started/installation/ ✅\n",
				"  - https://docs.llamaindex.ai/en/stable/getting_started/customization/ ⌛\n",
				"\n",
				"- **Models**\n",
				"  - **Understanding**\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/ ✅\n",
				"  - **Module Guides**\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/models/llms/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/\n",
				"\n",
				"- **Data**\n",
				"  - **Understanding**\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/loading/loading/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/loading/llamahub/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/storing/storing/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/querying/querying/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/llamaparse/ ✅\n",
				"\n",
				"  - **Loading**: https://docs.llamaindex.ai/en/stable/module_guides/loading/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/usage_pattern/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/llama_parse/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/\n",
				"\n",
				"  - **Indexing**: https://docs.llamaindex.ai/en/stable/module_guides/indexing/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/document_management/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/llama_cloud_index/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/indexing/modules/\n",
				"\n",
				"  - **Storing**: https://docs.llamaindex.ai/en/stable/module_guides/storing/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/docstores/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/index_stores/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/chat_stores/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/kv_stores/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/save_load/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/storing/customization/\n",
				"\n",
				"  - **Querying**: https://docs.llamaindex.ai/en/stable/module_guides/querying/\n",
				"    - **Query Engines**: https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/streaming/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/modules/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/supporting_modules/\n",
				"    - **Chat Engines**: https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/usage_pattern/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/modules/\n",
				"    - **Retrieval**: https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retrievers/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retriever_modes/\n",
				"    - **Node Postprocessors**: https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/\n",
				"    - **Response Synthesis**: https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers/\n",
				"    - **Routing**: https://docs.llamaindex.ai/en/stable/module_guides/querying/router/\n",
				"    - **Structured Outputs**: https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser/\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program/\n",
				"\n",
				"- **Agents**\n",
				"  - **Understanding**\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/local_models/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/rag_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/memory/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/agent/tools/ ✅\n",
				"  - **Module Guides**\n",
				"    - https://medium.com/llamaindex-blog/data-agents-eed797d7972f ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/modules/ ⌛\n",
				"  - **Examples**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_retrieval/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_cookbook/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/query_engine/recursive_retriever_agents/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/agent_builder/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/mistral_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/examples/objects/object_index/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/query_pipeline_agent/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/structured_planner/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/agent_runner_rag_controllable/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/ ⌛\n",
				"\n",
				"- **Workflows**\n",
				"  - **Understanding**\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/basic_flow/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/branches_and_loops/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/state/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/stream/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/ ⌛\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/nested/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/observability/\n",
				"    - https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/\n",
				"  - **Module Guides**\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/workflow/ ✅\n",
				"  - **Examples**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/self_discover_workflow/ ⌛\n",
				"  - **RAG**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/rag/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/corrective_rag_pack/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/advanced_text_to_sql/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/JSONalyze_query_engine/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/long_rag_pack/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/multi_step_query_engine/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/router_query_engine/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/sub_question_query_engine/\n",
				"  - **Agents**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/function_calling_agent/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/react_agent/\n",
				"  - **Techniques**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/parallel_execution/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/human_in_the_loop_story_crafting/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/reflection/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/multi_strategy_workflow/\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/workflow/self_discover_workflow/\n",
				"\n",
				"- **Llama Deploy**\n",
				"  - **Module Guides**\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/10_getting_started/ ✅\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/20_core_components/ ✅\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/30_manual_orchestration/ ✅\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/40_python_sdk/ ✅\n",
				"      - https://docs.llamaindex.ai/en/stable/module_guides/llama_deploy/50_llamactl/\n",
				"  - **[git] run-llama/llama_deploy**\n",
				"    - https://github.com/run-llama/llama_deploy\n",
				"      - https://github.com/run-llama/llama_deploy/tree/main/examples\n",
				"        - https://github.com/run-llama/llama_deploy/tree/main/examples/quick_start ✅\n",
				"        - https://github.com/run-llama/llama_deploy/tree/main/examples/python_fullstack\n",
				"\n",
				"  - **llama_agents**\n",
				"    - https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
				"\n",
				"- **understanding**: https://docs.llamaindex.ai/en/stable/understanding/\n",
				"  - https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/ ⌛\n",
				"\n",
				"- **use_cases**: https://docs.llamaindex.ai/en/stable/use_cases/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/extraction/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/chatbots/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/agents/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/multimodal/ ⌛\n",
				"  - https://docs.llamaindex.ai/en/stable/use_cases/fine_tuning/ ⌛\n",
				"\n",
				"- **module_guides**: https://docs.llamaindex.ai/en/stable/module_guides/\n",
				"  - **deploying**\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/\n",
				"    - https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/\n",
				"  - https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/\n",
				"\n",
				"- **Others**:\n",
				"  - https://llamahub.ai/\n",
				"  - https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo/\n",
				"\n",
				"- **Third-parties**\n",
				"  - **Qdrant**\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/ ✅\n",
				"    - https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/ \n",
				"    - https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

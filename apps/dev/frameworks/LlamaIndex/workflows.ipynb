{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Imports"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"from configs import settings, const, components\n",
				"from configs.settings import logger\n",
				"import asyncio, os, time, yaml, json, datetime, copy, random\n",
				"from typing import Any, AsyncGenerator, Generator, Callable, Literal, Optional, TypeAlias, Union\n",
				"from tqdm import tqdm\n",
				"from pprint import pprint\n",
				"\n",
				"from toolkit.llm.llama_index import (\n",
				"\tagents, cores, deploys as dpls, evaluation, messages, models, \n",
				"\tobservability, types, utils as utils_llama_index, workflows as wfs\n",
				")\n",
				"from toolkit.llm.llama_index.data import loading, querying, storing\n",
				"\n",
				"from features.agents.car.tools import VehicleDB\n",
				"from features.agents.tools import map\n",
				"\n",
				"from toolkit.utils import utils, typer as t\n",
				"from toolkit.utils.llm import measure_performance, main as utils_llm\n",
				"from toolkit.utils.utils import rp_print"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Dev"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Starter"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Basic"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class MyWorkflow(wfs.Workflow):\n",
				"\t@wfs.step\n",
				"\tasync def my_step(self, ev: wfs.StartEvent) -> wfs.StopEvent:\n",
				"\t\treturn wfs.StopEvent(result=\"Hello, world!\")\n",
				"\n",
				"async def main():\n",
				"\tworkflow = MyWorkflow(timeout=10, verbose=False)\n",
				"\tresult = await workflow.run()\n",
				"\tpprint(result)\n",
				"\n",
				"# wfs.draw_all_possible_flows(MyWorkflow)\n",
				"\n",
				"asyncio.run(main())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class LlmGenerator(wfs.Workflow):\n",
				"\t@wfs.step()\n",
				"\tasync def generate(\n",
				"\t\tself, ctx: wfs.Context, ev: wfs.StartEvent,\n",
				"\t) -> wfs.StopEvent:\n",
				"\t\tllm = cores.Settings.llm\n",
				"\t\tresponse = await llm.acomplete(ev.get(\"query\"))\n",
				"\t\treturn wfs.StopEvent(result=str(response))\n",
				"\n",
				"async def main():\n",
				"\tworkflow = LlmGenerator()\n",
				"\tresult = await workflow.run(query=\"What's LlamaIndex?\")\n",
				"\tprint(result)\n",
				"\n",
				"asyncio.run(main())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import random\n",
				"\n",
				"class EventFailed(wfs.Event):\n",
				"\terror: str\n",
				"\n",
				"class EventQuery(wfs.Event):\n",
				"\tquery: str\n",
				"\n",
				"class MyWorkflow(wfs.Workflow):\n",
				"\t@wfs.step()\n",
				"\tasync def setup(\n",
				"\t\tself, ctx: wfs.Context, ev: wfs.StartEvent\n",
				"\t) -> EventQuery:\n",
				"\t\tif not hasattr(ev, \"data\"):\n",
				"\t\t\tawait ctx.set(\"data\", [\"val1\", \"val2\", \"val3\"])\n",
				"\n",
				"\t\treturn EventQuery(query=ev.get(\"query\"))\n",
				"\t\n",
				"\t@wfs.step()\n",
				"\tasync def answer_query(\n",
				"\t\tself, ctx: wfs.Context, ev: EventQuery,\n",
				"\t) -> EventFailed | wfs.StopEvent:\n",
				"\t\tquery = ev.get(\"query\")\n",
				"  \n",
				"\t\tdata = await ctx.get(\"data\") if hasattr(ev, \"data\") else None\n",
				"\t\tresult = data[random.randint(0, len(data))] if data else None\n",
				"\t\n",
				"\t\trandom_number = random.randint(0, 1)\n",
				"\n",
				"\t\tif random_number == 0:\n",
				"\t\t\treturn EventFailed(error=\"Failed to answer user query.\")\n",
				"\t\telse:\n",
				"\t\t\treturn wfs.StopEvent(result=f\"The answer to your query is {result}.\")\n",
				"\t\t\n",
				"\t@wfs.step()\n",
				"\tasync def improve_query(\n",
				"\t\tself, ctx: wfs.Context, ev: EventFailed,\n",
				"\t) -> EventQuery | wfs.StopEvent:\n",
				"\t\trandom_number = random.randint(0, 1)\n",
				"\n",
				"\t\tif random_number == 0:\n",
				"\t\t\treturn EventQuery(query=\"Here's a better query.\")\n",
				"\t\telse:\n",
				"\t\t\treturn wfs.StopEvent(result=\"Your query can't be fixed.\")\n",
				"\n",
				"async def main():\n",
				"\tworkflow = MyWorkflow()\n",
				"\tresult = await workflow.run(query=\"What's LlamaIndex?\")\n",
				"\tprint(result)\n",
				"\n",
				"asyncio.run(main())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import random\n",
				"\n",
				"class EventLoop(wfs.Event):\n",
				"\toutput_loop: str\n",
				"\n",
				"class EventSetupContext(wfs.Event):\n",
				"\tquery: str\n",
				"\n",
				"class EventFirst(wfs.Event):\n",
				"\toutput_first: str\n",
				"\n",
				"class EventSecond(wfs.Event):\n",
				"\toutput_second: str\n",
				"\tresponse: str\n",
				"\n",
				"class EventBranchA1(wfs.Event):\n",
				"\tpayload: str\n",
				"\n",
				"class EventBranchA2(wfs.Event):\n",
				"\tpayload: str\n",
				"\n",
				"class EventBranchB1(wfs.Event):\n",
				"  payload: str\n",
				"\n",
				"class EventBranchB2(wfs.Event):\n",
				"\tpayload: str\n",
				"\n",
				"class MyWorkflow(wfs.Workflow):\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def start(\n",
				"\t\tself, ctx: wfs.Context, \n",
				"  \tev: wfs.StartEvent,\n",
				"\t) -> EventSetupContext | EventFirst:\n",
				"\t\ttry:\n",
				"\t\t\tdb = await ctx.get(\"some_database\", default=None)\n",
				"\t\texcept:\n",
				"\t\t\tawait ctx.set(\"some_database\", None)\n",
				"\t\t\tdb = await ctx.get(\"some_database\", default=None)\n",
				"   \n",
				"\t\t\tif db is None:\n",
				"\t\t\t\tprint(f\"No database found, setting up...\")\n",
				"\t\t\t\treturn EventSetupContext(query=\"Setup\")\n",
				"\n",
				"\t\tprint(f\"Database found: {db}\")\n",
				"\t\treturn EventFirst(output_first=\"First step output\")\n",
				"\t\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def setup(\n",
				"\t\tself, ctx: wfs.Context, \n",
				"  \tev: EventSetupContext,\n",
				"\t) -> wfs.StartEvent:\n",
				"\t\tawait ctx.set(\"some_database\", [1, 2, 3])\n",
				"\t\tprint(f\"Database setup completed.\")\n",
				"\t\treturn wfs.StartEvent(query=ev.query)\n",
				" \n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_one(\n",
				"   \tself, ctx: wfs.Context, \n",
				"    ev: EventFirst | EventLoop\n",
				"  ) -> EventSecond | EventLoop:\n",
				"\t\tctx.write_event_to_stream(wfs.Event(msg=\"Step one is happening ...\"))\n",
				"\t\tif random.randint(0, 1) == 0:\n",
				"\t\t\tprint(\"Bad thing happened\")\n",
				"\t\t\treturn EventLoop(output_loop=\"Looping back to step one.\")\n",
				"\t\telse:\n",
				"\t\t\tprint(\"Good thing happened\")\n",
				"\t\t\t# llm = models.OpenAI(model=\"gpt-3.5-turbo\")\n",
				"\t\t\t# generator = await llm.astream_complete(\"Tell me a joke\")\n",
				"\t\t\t# async for response in generator:\n",
				"\t\t\t# \tctx.write_event_to_stream(wfs.Event(msg=response.delta))\n",
				"\t\t\tresponse = None\n",
				"\t\t\treturn EventSecond(\n",
				"\t\t\t\toutput_second=\"First step completed.\",\n",
				"\t\t\t\tresponse=str(response)\n",
				"\t\t\t)\n",
				"\t\t# print(ev.input_first)\n",
				"\t\t# return EventFirst(output_first=\"First step completed.\")\n",
				"\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_two(\n",
				"   self, ctx: wfs.Context,\n",
				"   ev: EventSecond\n",
				"  ) -> EventBranchA1 | EventBranchB1:\n",
				"\t\tctx.write_event_to_stream(wfs.Event(msg=\"Step two is happening ...\"))\n",
				"\t\t\n",
				"\t\tif random.randint(0, 1) == 0:\n",
				"\t\t\treturn EventBranchA1(payload=\"Branch A\")\n",
				"\t\telse:\n",
				"\t\t\treturn EventBranchB1(payload=\"Branch B\")\n",
				" \n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_branch_a1(\n",
				"   self, ctx: wfs.Context,\n",
				"   ev: EventBranchA1\n",
				"  ) -> EventBranchA2:\n",
				"\t\tprint(ev.payload)\n",
				"\t\treturn EventBranchA2(payload=\"Branch A2\")\n",
				"\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_branch_a2(\n",
				"   self, ctx: wfs.Context,\n",
				"   ev: EventBranchA2\n",
				"  ) -> wfs.StopEvent:\n",
				"\t\tprint(ev.payload)\n",
				"\t\treturn wfs.StopEvent(result=\"Workflow completed.\")\n",
				"\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_branch_b1(\n",
				"   self, ctx: wfs.Context,\n",
				"   ev: EventBranchB1\n",
				"  ) -> EventBranchB2:\n",
				"\t\tprint(ev.payload)\n",
				"\t\treturn EventBranchB2(payload=\"Branch B2\")\n",
				"\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_branch_b2(\n",
				"   self, ctx: wfs.Context,\n",
				"   ev: EventBranchB2\n",
				"  ) -> wfs.StopEvent:\n",
				"\t\tprint(ev.payload)\n",
				"\t\treturn wfs.StopEvent(result=\"Workflow completed.\")\n",
				"\n",
				"async def main():\n",
				"\tworkflow = MyWorkflow(timeout=10, verbose=False)\n",
				"\tresult = await workflow.run(query=\"Starting the workflow.\")\n",
				"\tpprint(result)\n",
				"\n",
				"wfs.draw_all_possible_flows(MyWorkflow)\n",
				"asyncio.run(main())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Concurrent/Parallel"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import asyncio\n",
				"import random\n",
				"from pprint import pprint\n",
				"from llama_index.core.workflow import (\n",
				"    step,\n",
				"    Context,\n",
				"    Workflow,\n",
				"    Event,\n",
				"    StartEvent,\n",
				"    StopEvent,\n",
				")\n",
				"\n",
				"# Event definitions\n",
				"class EventSetup(Event):\n",
				"    error: bool\n",
				"\n",
				"class EventInput(Event):\n",
				"    input: str\n",
				"\n",
				"class EventQuery(Event):\n",
				"    query: str\n",
				"\n",
				"class ProcessEvent(Event):\n",
				"    data: str\n",
				"\n",
				"class ResultEvent(Event):\n",
				"    result: str\n",
				"\n",
				"class ParallelWorkflow(Workflow):\n",
				"    @step()\n",
				"    async def setup(self, ctx: Context, ev: StartEvent) -> EventSetup:\n",
				"        if not hasattr(self, \"setup\") or not self.setup:\n",
				"            self.setup = True\n",
				"            print(\"Setup ✅\")\n",
				"        return EventSetup(error=False)\n",
				"\n",
				"    @step()\n",
				"    async def collect_input(self, ctx: Context, ev: StartEvent) -> EventInput:\n",
				"        if hasattr(ev, \"input\"):\n",
				"            print(\"Input ✅\")\n",
				"            return EventInput(input=ev.input)\n",
				"\n",
				"    @step()\n",
				"    async def parse_query(self, ctx: Context, ev: StartEvent) -> EventQuery:\n",
				"        if hasattr(ev, \"query\"):\n",
				"            print(\"Query ✅\")\n",
				"            return EventQuery(query=ev.query)\n",
				"\n",
				"    @step()\n",
				"    async def run_query(self, ctx: Context, ev: EventInput | EventSetup | EventQuery) -> ProcessEvent | None:\n",
				"        results_events = ctx.collect_events(ev, [EventInput, EventSetup, EventQuery])\n",
				"        if not results_events:\n",
				"            print(\"Not enough events yet\")\n",
				"            return None\n",
				"\n",
				"        my_query = results_events[2].query\n",
				"        my_input = results_events[0].input\n",
				"\n",
				"        print(\"Events collected 🫡\")\n",
				"        print(results_events)\n",
				"\n",
				"        # Generate multiple ProcessEvents based on the input and query\n",
				"        data_list = [f\"{my_query} on {my_input}-{i}\" for i in range(3)]\n",
				"        await ctx.set(\"num_to_collect\", len(data_list))\n",
				"        for item in data_list:\n",
				"            ctx.send_event(ProcessEvent(data=item))\n",
				"\n",
				"        return None\n",
				"\n",
				"    @step(num_workers=3)\n",
				"    async def process_data(self, ctx: Context, ev: ProcessEvent) -> ResultEvent:\n",
				"        # Simulate some time-consuming processing\n",
				"        await asyncio.sleep(random.randint(1, 2))\n",
				"        result = f\"Processed: {ev.data}\"\n",
				"        print(f\"Completed processing: {ev.data}\")\n",
				"        return ResultEvent(result=result)\n",
				"\n",
				"    @step()\n",
				"    async def combine_results(self, ctx: Context, ev: ResultEvent) -> StopEvent | None:\n",
				"        num_to_collect = await ctx.get(\"num_to_collect\")\n",
				"        results = ctx.collect_events(ev, [ResultEvent] * num_to_collect)\n",
				"        if results is None:\n",
				"            return None\n",
				"\n",
				"        combined_result = \", \".join([event.result for event in results])\n",
				"        return StopEvent(result=f\"Final result: {combined_result}\")\n",
				"\n",
				"async def main():\n",
				"    workflow = ParallelWorkflow(timeout=20, verbose=False)\n",
				"    result = await workflow.run(input=\"My Input\", query=\"My Question\")\n",
				"    pprint(result)\n",
				"\n",
				"if __name__ == \"__main__\":\n",
				"    asyncio.run(main())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Nested"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class WorkflowReflection(wfs.Workflow):\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def sub_start(\n",
				"\t\tself, ctx: wfs.Context,\n",
				"\t\tev: wfs.StartEvent,\n",
				"\t) -> wfs.StopEvent:\n",
				"\t\tprint(f\"Running reflection query: {ev.query}\")\n",
				"\t\treturn wfs.StopEvent(result=\"Reflection result\")\n",
				"\n",
				"class WorkflowReflectionDefault(wfs.Workflow):\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def sub_start(\n",
				"\t\tself, ctx: wfs.Context,\n",
				"\t\tev: wfs.StartEvent,\n",
				"\t) -> wfs.StopEvent:\n",
				"\t\tprint(\"Doing default reflection\")\n",
				"\t\treturn wfs.StopEvent(result=\"Reflection result\")\n",
				"\n",
				"class EventStep1(wfs.Event):\n",
				"\tquery: str\n",
				"\n",
				"class WorkflowMain(wfs.Workflow):\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def start(\n",
				"\t\tself, ctx: wfs.Context,\n",
				"\t\tev: wfs.StartEvent,\n",
				"\t\tworkflow_reflection: wfs.Workflow,\n",
				"\t) -> EventStep1:\n",
				"\t\tprint(\"Need to run reflection\")\n",
				"\t\tresult = await workflow_reflection.run(query=\"Reflection query\")\n",
				"\t\treturn EventStep1(query=result)\n",
				"\n",
				"\t@wfs.step(num_workers=4)\n",
				"\tasync def step_1(\n",
				"\t\tself, ctx: wfs.Context,\n",
				"\t\tev: EventStep1,\n",
				"\t) -> wfs.StopEvent:\n",
				"\t\tprint(f\"Running query: {ev.query}\")\n",
				"\t\treturn wfs.StopEvent(result=ev.query)\n",
				"\n",
				"workflow = WorkflowMain(timeout=10, verbose=False)\n",
				"workflow.add_workflows(workflow_reflection=WorkflowReflection())\n",
				"\n",
				"result = await workflow.run(query=\"Initial query\")\n",
				"print(result)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Human-in-the-loop"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Import necessary libraries and modules\n",
				"from typing import List, Optional\n",
				"from pydantic import BaseModel, Field\n",
				"from llama_index.core.workflow import Context, Event, StartEvent, StopEvent, Workflow, step\n",
				"from llama_index.llms.openai import OpenAI\n",
				"from llama_index.core.prompts import PromptTemplate\n",
				"import uuid\n",
				"\n",
				"# Define the data model for scenarios\n",
				"class Scenario(BaseModel):\n",
				"    \"\"\"Data model for generating scenarios in the workflow.\"\"\"\n",
				"    description: str = Field(description=\"The description of the current scenario.\")\n",
				"    options: List[str] = Field(default=[], description=\"The list of options available for the current scenario.\")\n",
				"\n",
				"# Define the Block class to represent a single step in the workflow\n",
				"class Block(BaseModel):\n",
				"    id_: str = Field(default_factory=lambda: str(uuid.uuid4()))  # Unique identifier for each block\n",
				"    scenario: Scenario  # The scenario presented in this block\n",
				"    choice: Optional[str] = None  # The choice made by the user, if any\n",
				"\n",
				"    def __str__(self):\n",
				"        \"\"\"String representation of the Block for logging and display purposes.\"\"\"\n",
				"        return f\"BLOCK\\n===\\nDESCRIPTION: {self.scenario.description}\\nOPTIONS: {', '.join(self.scenario.options)}\\nCHOICE: {self.choice or ''}\"\n",
				"\n",
				"# Define custom events for the workflow\n",
				"class NewBlockEvent(Event):\n",
				"    \"\"\"Event triggered when a new block is created.\"\"\"\n",
				"    block: Block\n",
				"\n",
				"class HumanChoiceEvent(Event):\n",
				"    \"\"\"Event triggered when a human makes a choice.\"\"\"\n",
				"    block_id: str\n",
				"\n",
				"# Template for generating new scenarios based on previous ones\n",
				"SCENARIO_GENERATION_TEMPLATE = \"\"\"\n",
				"You are assisting in an interactive decision-making process. Based on the previous scenarios and choices, generate the next scenario with a description and a set of options.\n",
				"\n",
				"PREVIOUS SCENARIOS:\n",
				"---\n",
				"{running_scenarios}\n",
				"\n",
				"Generate the next scenario and options. If there are no previous scenarios, start with an interesting initial scenario.\n",
				"\n",
				"Use the provided data model to structure your output.\n",
				"\"\"\"\n",
				"\n",
				"# Main workflow class\n",
				"class InteractiveDecisionMakingWorkflow(Workflow):\n",
				"    def __init__(self, max_steps: int = 3, **kwargs):\n",
				"        \"\"\"\n",
				"        Initialize the workflow.\n",
				"        \n",
				"        :param max_steps: Maximum number of steps in the workflow\n",
				"        :param kwargs: Additional keyword arguments\n",
				"        \"\"\"\n",
				"        super().__init__(**kwargs)\n",
				"        self.llm = OpenAI()  # Initialize the language model\n",
				"        self.max_steps = max_steps\n",
				"\n",
				"    @step\n",
				"    async def create_scenario(self, ctx: Context, ev: StartEvent | HumanChoiceEvent) -> NewBlockEvent | StopEvent:\n",
				"        \"\"\"\n",
				"        Create a new scenario based on previous scenarios and choices.\n",
				"        \n",
				"        :param ctx: The context of the workflow\n",
				"        :param ev: The triggering event (either StartEvent or HumanChoiceEvent)\n",
				"        :return: Either a NewBlockEvent with the new scenario or a StopEvent if max steps reached\n",
				"        \"\"\"\n",
				"        blocks = await ctx.get(\"blocks\", [])\n",
				"        running_scenarios = \"\\n\".join(str(b) for b in blocks)\n",
				"\n",
				"        if len(blocks) < self.max_steps:\n",
				"            # Generate a new scenario using the language model\n",
				"            new_scenario = self.llm.structured_predict(\n",
				"                Scenario,\n",
				"                PromptTemplate(SCENARIO_GENERATION_TEMPLATE),\n",
				"                running_scenarios=running_scenarios,\n",
				"            )\n",
				"            new_block = Block(scenario=new_scenario)\n",
				"            blocks.append(new_block)\n",
				"            await ctx.set(\"blocks\", blocks)\n",
				"            return NewBlockEvent(block=new_block)\n",
				"        else:\n",
				"            # If max steps reached, stop the workflow\n",
				"            return StopEvent(result=blocks)\n",
				"\n",
				"    @step\n",
				"    async def prompt_human(self, ctx: Context, ev: NewBlockEvent) -> HumanChoiceEvent:\n",
				"        \"\"\"\n",
				"        Present the current scenario to the human and collect their choice.\n",
				"        \n",
				"        :param ctx: The context of the workflow\n",
				"        :param ev: The NewBlockEvent containing the current scenario\n",
				"        :return: A HumanChoiceEvent with the user's choice\n",
				"        \"\"\"\n",
				"        block = ev.block\n",
				"\n",
				"        # Prepare the prompt for the human\n",
				"        human_prompt = f\"\\n===\\n{block.scenario.description}\\n\\n\"\n",
				"        human_prompt += \"Choose an option:\\n\\n\"\n",
				"        human_prompt += \"\\n\".join(f\"{i+1}. {option}\" for i, option in enumerate(block.scenario.options))\n",
				"        human_prompt += \"\\n\\nEnter the number of your choice: \"\n",
				"        \n",
				"        # Loop until a valid choice is made\n",
				"        while True:\n",
				"            human_input = input(human_prompt)\n",
				"            try:\n",
				"                choice_index = int(human_input) - 1\n",
				"                if 0 <= choice_index < len(block.scenario.options):\n",
				"                    block.choice = block.scenario.options[choice_index]\n",
				"                    break\n",
				"                else:\n",
				"                    print(\"Invalid choice. Please try again.\")\n",
				"            except ValueError:\n",
				"                print(\"Please enter a valid number.\")\n",
				"\n",
				"        # Update the context with the human's choice\n",
				"        blocks = await ctx.get(\"blocks\")\n",
				"        blocks[-1] = block\n",
				"        await ctx.set(\"blocks\", blocks)\n",
				"\n",
				"        return HumanChoiceEvent(block_id=block.id_)\n",
				"\n",
				"# Example usage of the workflow\n",
				"async def main():\n",
				"    # Create and run the workflow\n",
				"    workflow = InteractiveDecisionMakingWorkflow(max_steps=3)\n",
				"    result = await workflow.run()\n",
				"    \n",
				"    # Print the final decision path\n",
				"    print(\"\\nFinal Decision Path:\")\n",
				"    for block in result:\n",
				"        print(f\"\\nScenario: {block.scenario.description}\")\n",
				"        print(f\"Choice: {block.choice}\")\n",
				"\n",
				"# Entry point of the script\n",
				"if __name__ == \"__main__\":\n",
				"    import asyncio\n",
				"    asyncio.run(main())  # Run the main function in an async context"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Reflection"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Structured Outputs (JSON-Extraction)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from typing import Type\n",
				"from pydantic import BaseModel\n",
				"\n",
				"class EventDataExtracted(wfs.Event):\n",
				"    output: str\n",
				"    passage: str\n",
				"\n",
				"class EventExtractionError(wfs.Event):\n",
				"    error: str\n",
				"    wrong_output: str\n",
				"    passage: str\n",
				"\n",
				"EXTRACTION_PROMPT = \"\"\"\n",
				"Context information is below:\n",
				"---------------------\n",
				"{passage}\n",
				"---------------------\n",
				"\n",
				"Given the context information and not prior knowledge, create a JSON object from the information in the context.\n",
				"The JSON object must follow the JSON schema:\n",
				"{schema}\n",
				"\n",
				"\"\"\"\n",
				"\n",
				"REFLECTION_PROMPT = \"\"\"\n",
				"You already created this output previously:\n",
				"---------------------\n",
				"{wrong_answer}\n",
				"---------------------\n",
				"\n",
				"This caused the JSON decode error: {error}\n",
				"\n",
				"Try again, the response must contain only valid JSON code. Do not add any sentence before or after the JSON object.\n",
				"Do not repeat the schema.\n",
				"\"\"\"\n",
				"\n",
				"class GenericReflectionWorkflow(wfs.Workflow):\n",
				"    def __init__(\n",
				"      self, \n",
				"      model_class: typer.Type[typer.BaseModel], \n",
				"      max_retries: int = 3, **kwargs\n",
				"    ):\n",
				"        super().__init__(**kwargs)\n",
				"        self.model_class = model_class\n",
				"        self.max_retries = max_retries\n",
				"        self.current_retries = 0\n",
				"\n",
				"    @wfs.step\n",
				"    async def extract(\n",
				"        self, ctx: wfs.Context, ev: wfs.StartEvent | EventExtractionError\n",
				"    ) -> wfs.StopEvent | EventDataExtracted:\n",
				"        if self.current_retries >= self.max_retries:\n",
				"            return wfs.StopEvent(result=\"Max retries reached\")\n",
				"        else:\n",
				"            self.current_retries += 1\n",
				"\n",
				"        if isinstance(ev, wfs.StartEvent):\n",
				"            passage = ev.get(\"passage\")\n",
				"            if not passage:\n",
				"                return wfs.StopEvent(result=\"Please provide some text in input\")\n",
				"            reflection_prompt = \"\"\n",
				"        elif isinstance(ev, EventExtractionError):\n",
				"            passage = ev.passage\n",
				"            reflection_prompt = REFLECTION_PROMPT.format(\n",
				"                wrong_answer=ev.wrong_output, error=ev.error\n",
				"            )\n",
				"\n",
				"        llm = cores.Settings.llm\n",
				"        prompt = EXTRACTION_PROMPT.format(\n",
				"            passage=passage, schema=self.model_class.schema_json()\n",
				"        )\n",
				"        if reflection_prompt:\n",
				"            prompt += reflection_prompt\n",
				"\n",
				"        output = await llm.acomplete(prompt)\n",
				"\n",
				"        return EventDataExtracted(output=str(output), passage=passage)\n",
				"\n",
				"    @wfs.step\n",
				"    async def validate(\n",
				"        self, ev: EventDataExtracted\n",
				"    ) -> wfs.StopEvent | EventExtractionError:\n",
				"        try:\n",
				"            self.model_class.model_validate_json(ev.output)\n",
				"        except Exception as e:\n",
				"            print(\"Validation failed, retrying...\")\n",
				"            return EventExtractionError(\n",
				"                error=str(e), wrong_output=ev.output, passage=ev.passage\n",
				"            )\n",
				"\n",
				"        return wfs.StopEvent(result=ev.output)\n",
				"\n",
				"# Example usage\n",
				"class Person(typer.BaseModel):\n",
				"    name: str\n",
				"    age: int\n",
				"    occupation: str\n",
				"\n",
				"class PersonCollection(typer.BaseModel):\n",
				"    people: list[Person]\n",
				"\n",
				"async def main():\n",
				"    workflow = GenericReflectionWorkflow(\n",
				"        model_class=PersonCollection,\n",
				"        max_retries=5,\n",
				"        timeout=120,\n",
				"        verbose=True\n",
				"    )\n",
				"\n",
				"    result = await workflow.run(\n",
				"        passage=\"John is a 35-year-old software engineer. His colleague Sarah, who is 42, works as a project manager.\"\n",
				"    )\n",
				"    print(result)\n",
				"\n",
				"if __name__ == \"__main__\":\n",
				"    import asyncio\n",
				"    asyncio.run(main())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Hybrid\n",
				"\n",
				"- Maintaining state\n",
				"- Branches, loops\n",
				"- Human-feedback\n",
				"- Concurrent execution\n",
				"- ..."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from toolkit.llm.llama_index import (\n",
				"    cores, workflows as wfs, messages,\n",
				")\n",
				"from toolkit.utils.llm import main as utils_llm\n",
				"\n",
				"from typing import List, Optional\n",
				"\n",
				"class EventEmptyJoke(wfs.Event):\n",
				"    pass\n",
				"\n",
				"class EventJoke(wfs.Event):\n",
				"    joke: str\n",
				"\n",
				"class EventMultiJoke(wfs.Event):\n",
				"    jokes: List[str]\n",
				"\n",
				"class EventBestJoke(wfs.Event):\n",
				"    joke: str\n",
				"    reason: str\n",
				"\n",
				"class InputRequiredEvent(wfs.Event):\n",
				"    prefix: str\n",
				"\n",
				"class HumanResponseEvent(wfs.Event):\n",
				"    response: str\n",
				"\n",
				"class EventRating(wfs.Event):\n",
				"    rating: int\n",
				"\n",
				"class EventGatherJokes(wfs.Event):\n",
				"    pass\n",
				"\n",
				"class FlowJoke(wfs.Workflow):\n",
				"    @wfs.step\n",
				"    async def dispatch_joke_generation(\n",
				"        self, ctx: wfs.Context, ev: wfs.StartEvent,\n",
				"    ) -> EventGatherJokes | EventEmptyJoke:\n",
				"        topic = ev.get(\"topic\")\n",
				"        await ctx.set(key=\"topic\", value=topic)\n",
				"        num_jokes = 3  # Generate 3 jokes\n",
				"\n",
				"        for _ in range(num_jokes):\n",
				"            ctx.send_event(EventEmptyJoke()) # Concurrent execution\n",
				"\n",
				"        return EventGatherJokes()\n",
				"\n",
				"    @wfs.step(\n",
				"      retry_policy=wfs.ConstantDelayRetryPolicy(delay=5, maximum_attempts=3),\n",
				"\t\t\tnum_workers=4,\n",
				"    )\n",
				"    async def generate_joke(\n",
				"        self, ctx: wfs.Context, ev: EventEmptyJoke,\n",
				"    ) -> EventJoke:\n",
				"        topic = await ctx.get(key=\"topic\")\n",
				"        prompt = f\"Write your best joke about {topic}\"\n",
				"        response = await cores.Settings.llm.acomplete(prompt)\n",
				"        return EventJoke(joke=str(response))\n",
				"\n",
				"    @wfs.step\n",
				"    async def gather_jokes(\n",
				"        self, ctx: wfs.Context, ev: EventGatherJokes | EventJoke,\n",
				"    ) -> Optional[EventMultiJoke]:\n",
				"        jokes = ctx.collect_events(ev, [EventJoke, EventJoke, EventJoke])\n",
				"        if jokes is None:\n",
				"            return None\n",
				"        return EventMultiJoke(jokes=[joke.joke for joke in jokes])\n",
				"\n",
				"    @wfs.step(retry_policy=wfs.ConstantDelayRetryPolicy(delay=5, maximum_attempts=3))\n",
				"    async def select_best_joke(\n",
				"        self, ctx: wfs.Context, ev: EventMultiJoke,\n",
				"    ) -> EventBestJoke:\n",
				"        jokes = ev.jokes\n",
				"        prompt = \"\"\"\\\n",
				"        Select the best joke from these options and explain why it's the best:\n",
				"\n",
				"        {jokes}\n",
				"\n",
				"        Please provide your response in JSON format as follows:\n",
				"\n",
				"\t\t\t\t```json\n",
				"        {\n",
				"            \"best_joke\": \"BEST_JOKE\",\n",
				"            \"reason\": \"REASON\"\n",
				"        }\n",
				"        ```\n",
				"\n",
				"        Just return the json, no other text.\n",
				"        \"\"\"\n",
				"        prompt = messages.PromptTemplate(prompt)\n",
				"        \n",
				"        response = await cores.Settings.llm.achat([\n",
				"            messages.ChatMessage(role=\"user\", content=prompt.format(jokes=jokes)) # type: ignore\n",
				"        ])\n",
				"        response = str(response).split(\"assistant: \")[1]\n",
				"        response = utils_llm.parse_json(response)\n",
				"\n",
				"        best_joke = response[\"best_joke\"]\n",
				"        reason = response[\"reason\"]\n",
				"\n",
				"        return EventBestJoke(joke=best_joke, reason=reason)\n",
				"\n",
				"    @wfs.step\n",
				"    async def request_user_rating(\n",
				"        self, ctx: wfs.Context, ev: EventBestJoke,\n",
				"    ) -> InputRequiredEvent:\n",
				"        joke = ev.joke\n",
				"        return InputRequiredEvent(prefix=f\"Please rate this joke from 1 to 5:\\n\\n{joke}\\n\\nYour rating: \")\n",
				"\n",
				"    @wfs.step\n",
				"    async def handle_human_response(\n",
				"        self, ctx: wfs.Context, ev: InputRequiredEvent,\n",
				"    ) -> HumanResponseEvent:\n",
				"        # Get real user input\n",
				"        user_input = input(ev.prefix)\n",
				"        return HumanResponseEvent(response=user_input)\n",
				"\n",
				"    @wfs.step\n",
				"    async def process_user_rating(\n",
				"        self, ctx: wfs.Context, ev: HumanResponseEvent,\n",
				"    ) -> EventRating | InputRequiredEvent:\n",
				"        try:\n",
				"            rating = int(ev.response)\n",
				"            if 1 <= rating <= 5:\n",
				"                return EventRating(rating=rating)\n",
				"            else:\n",
				"                raise ValueError\n",
				"        except ValueError:\n",
				"            return InputRequiredEvent(prefix=\"Invalid input. Please enter a number between 1 and 5: \")\n",
				"\n",
				"    @wfs.step(retry_policy=wfs.ConstantDelayRetryPolicy(delay=5, maximum_attempts=3))\n",
				"    async def prepare_critique(\n",
				"        self, ctx: wfs.Context, ev: EventBestJoke | EventRating,\n",
				"    ) -> Optional[wfs.StopEvent]:\n",
				"        data = ctx.collect_events(ev, [EventBestJoke, EventRating])\n",
				"        \n",
				"        if data is None:\n",
				"            return None\n",
				"        \n",
				"        best_joke_event, rating_event = data\n",
				"        \n",
				"        joke = best_joke_event.joke\n",
				"        reason = best_joke_event.reason\n",
				"        rating = rating_event.rating\n",
				"        \n",
				"        prompt = f\"Give a thorough analysis and critique of the following joke (rated {rating}/5 by a user):\\n\\nJoke: {joke}\\n\\nThis joke was selected as the best because: {reason}\"\n",
				"        response = await cores.Settings.llm.acomplete(prompt)\n",
				"        return wfs.StopEvent(result=str(response))\n",
				"\n",
				"# Example usage\n",
				"workflow = FlowJoke(timeout=60, verbose=True)\n",
				"\n",
				"wfs.draw_all_possible_flows(workflow)\n",
				"\n",
				"result = await workflow.run(topic=\"pirates\")\n",
				"print(str(result))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Agents"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class FieldsStateAgent(t.EnumCustom):\n",
				"\tUSER_QUERY = t.auto()\n",
				"\tCURRENT_REASONING = t.auto()\n",
				"\t\n",
				"class StateAgent(t.BaseModel):\n",
				"\tuser_query: str = t.Field(description=\"User Input\", default=\"\")\n",
				"\tcurrent_reasoning: list = t.Field(description=\"Current Agent Reasoning\", default=[])\n",
				"\n",
				"state_agent = StateAgent(\n",
				"\tuser_query=\"\",\n",
				"\tcurrent_reasoning=[],\n",
				")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Function Calling"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class EvInput(wfs.Event):\n",
				"\tinput: list[messages.ChatMessage]\n",
				"\n",
				"class EvToolCall(wfs.Event):\n",
				"\ttool_calls: list[agents.ToolSelection]\n",
				"\n",
				"class EvToolOutput(wfs.Event):\n",
				"\toutput: agents.ToolOutput\n",
				"\n",
				"class AgentFunctionCalling(wfs.Workflow):\n",
				"\tdef __init__(\n",
				"\t\tself,\n",
				"\t\tllm: models.FunctionCallingLLM = cores.Settings.llm,\n",
				"\t\ttools: t.List[agents.BaseTool] = [],\n",
				"\t\t*args: t.Any,\n",
				"\t\t**kwargs: t.Any,\n",
				"\t) -> None:\n",
				"\t\tsuper().__init__(*args, **kwargs)\n",
				"  \n",
				"\t\tself.tools = tools\n",
				"\t\tself.tools_dict = {tool.metadata.get_name(): tool for tool in self.tools}\n",
				"\n",
				"\t\tself.llm = llm\n",
				"\t\tassert self.llm.metadata.is_function_calling_model\n",
				"\n",
				"\t\tself.memory = messages.ChatMemoryBuffer.from_defaults(llm=self.llm)\n",
				"\t\tself.sources = []\n",
				"\t\n",
				"\t@wfs.step()\n",
				"\tasync def prepare_chat_history(\n",
				"\t\tself, ev: wfs.StartEvent,\n",
				"\t) -> EvInput:\n",
				"\t\tself.sources.clear()\n",
				"\n",
				"\t\tuser_query = ev.get(\n",
				"    \tFieldsStateAgent.USER_QUERY, \n",
				"    \tt.get_field_default(StateAgent, FieldsStateAgent.USER_QUERY),\n",
				"    )\n",
				"\t\tuser_msg = messages.ChatMessage(\n",
				"    \trole=messages.MessageRole.USER, content=user_query,\n",
				"    )\n",
				"\t\tself.memory.put(user_msg)\n",
				"\n",
				"\t\tchat_history = self.memory.get()\n",
				"\t\treturn EvInput(input=chat_history)\n",
				"\n",
				"\t@wfs.step()\n",
				"\tasync def handle_llm_input(\n",
				"\t\tself, ev: EvInput,\n",
				"\t) -> EvToolCall | wfs.StopEvent:\n",
				"\t\tchat_history = ev.input\n",
				"\n",
				"\t\tresponse = await self.llm.achat_with_tools(\n",
				"\t\t\ttools=self.tools, chat_history=chat_history,\n",
				"\t\t)\n",
				"\t\tself.memory.put(response.message)\n",
				"\n",
				"\t\ttool_calls = self.llm.get_tool_calls_from_response(\n",
				"\t\t\tresponse=response, error_on_no_tool_call=False,\n",
				"\t\t)\n",
				"\n",
				"\t\tif not tool_calls:\n",
				"\t\t\treturn wfs.StopEvent(\n",
				"\t\t\t\tresult={\n",
				"\t\t\t\t\t\"response\": response,\n",
				"\t\t\t\t\t\"sources\": [*self.sources],\n",
				"\t\t\t\t}\n",
				"\t\t\t)\n",
				"\t\telse:\n",
				"\t\t\treturn EvToolCall(tool_calls=tool_calls)\n",
				"\t\t\n",
				"\t@wfs.step()\n",
				"\tasync def handle_tool_calls(\n",
				"\t\tself, ev: EvToolCall\n",
				"\t) -> EvInput:\n",
				"\t\ttool_calls = ev.tool_calls\n",
				"\n",
				"\t\ttool_msgs = []\n",
				"\n",
				"\t\tfor tool_call in tool_calls:\n",
				"\t\t\ttool = self.tools_dict.get(tool_call.tool_name)\n",
				"\t\t\tadditional_kwargs = {\n",
				"\t\t\t\t\"tool_call_id\": tool_call.tool_id,\n",
				"\t\t\t\t\"name\": tool.metadata.get_name(),\n",
				"\t\t\t}\n",
				"\n",
				"\t\t\tif not tool:\n",
				"\t\t\t\ttool_msgs.append(\n",
				"\t\t\t\t\tmessages.ChatMessage(\n",
				"\t\t\t\t\t\trole=messages.MessageRole.TOOL,\n",
				"\t\t\t\t\t\tcontent=f\"Tool {tool_call.tool_name} does not exist\",\n",
				"\t\t\t\t\t\tadditional_kwargs=additional_kwargs,\n",
				"\t\t\t\t\t)\n",
				"\t\t\t\t)\n",
				"\t\t\t\tcontinue\n",
				"\n",
				"\t\t\ttry:\n",
				"\t\t\t\ttool_output = tool(**tool_call.tool_kwargs)\n",
				"\t\t\t\tself.sources.append(tool_output)\n",
				"\t\t\t\ttool_msgs.append(\n",
				"\t\t\t\t\tmessages.ChatMessage(\n",
				"\t\t\t\t\t\trole=messages.MessageRole.TOOL,\n",
				"\t\t\t\t\t\tcontent=tool_output.content,\n",
				"\t\t\t\t\t\tadditional_kwargs=additional_kwargs,\n",
				"\t\t\t\t\t)\n",
				"\t\t\t\t)\n",
				"\t\t\texcept Exception as e:\n",
				"\t\t\t\ttool_msgs.append(\n",
				"\t\t\t\t\tmessages.ChatMessage(\n",
				"\t\t\t\t\t\trole=messages.MessageRole.TOOL,\n",
				"\t\t\t\t\t\tcontent=f\"Encountered error in tool  call: {e}\",\n",
				"\t\t\t\t\t\tadditional_kwargs=additional_kwargs,\n",
				"\t\t\t\t\t)\n",
				"\t\t\t\t)\n",
				"\n",
				"\t\tfor msg in tool_msgs:\n",
				"\t\t\tself.memory.put(msg)\n",
				"\t\t\n",
				"\t\tchat_history = self.memory.get()\n",
				"\t\treturn EvInput(input=chat_history)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def add(x: int, y: int) -> int:\n",
				"\t\"\"\"Useful function to add two numbers.\"\"\"\n",
				"\treturn x + y\n",
				"\n",
				"\n",
				"def multiply(x: int, y: int) -> int:\n",
				"\t\"\"\"Useful function to multiply two numbers.\"\"\"\n",
				"\treturn x * y\n",
				"\n",
				"tools = [\n",
				"\tagents.FunctionTool.from_defaults(add),\n",
				"\tagents.FunctionTool.from_defaults(multiply),\n",
				"]\n",
				"\n",
				"agent = AgentFunctionCalling(\n",
				"\ttools=tools, timeout=120, verbose=True,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# user_query = \"Hello\"\n",
				"user_query = \"What is (2123 + 2321) * 312?\"\n",
				"# user_query = \"Tell me a joke\"\n",
				"result = await agent.run(user_query=user_query)\n",
				"\n",
				"pprint(result)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### REACT"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# App"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from features.agents.car import apis as apis_car\n",
				"from features.rag import apis as apis_rag"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"queries = [\n",
				"\t\"What are notes before driving car\",\n",
				"\t# \"Please provide a summary of the content found on page 200 of the manual\",\n",
				"\t\"How can parents prevent children from accidentally opening doors or windows while driving?\",\n",
				"\n",
				"\t\"Fold the mirrors\",\n",
				"\t\"Unfold the mirrors\",\n",
				"\t\"Direct air to feet only\",\n",
				"\t\"Direct air to face and feet\",\n",
				"\t\n",
				"\t\"Is the car locked\",\n",
				"\t\"Is the car trunk opened?\",\n",
				"\t\"Open the car trunk\",\n",
				" \n",
				"\t\"It's too hot\",\n",
				"\t\"I'm feeling sleepy\",\n",
				"\t\"The radio is too loud\",\n",
				"\t\"Can you turn on the AC?\",\n",
				"\t\"The windshield is getting foggy\",\n",
				"\t\"The headlights seem dim\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"prompts_agent_car = settings.prompts_agent_car\n",
				"\n",
				"class EvUserQueryCategorized(wfs.Event):\n",
				"\tuser_query_category: t.UserQueryCategory\n",
				"#*------------------------------------------------------------------------------\n",
				"class EvFlowStartedRag(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"class EvFlowDoneRag(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"class EvFlowStartedControl(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"# class EvFlCtrlProcessTaskStarted(wfs.Event):\n",
				"# \ttask: str\n",
				"# \tid: str\n",
				"\n",
				"# class EvFlCtrlProcessTaskDone(wfs.Event):\n",
				"# \tresult: str\n",
				"# \tid: str\n",
				"\n",
				"class EvFlowDoneControl(wfs.Event):\n",
				"\tpass\n",
				"#*------------------------------------------------------------------------------\n",
				"class EvFlowStartedGeneral(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"class EvFlowDoneGeneral(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"class EvFlowDone(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"class EvHumanFeedbackDone(wfs.Event):\n",
				"  human_feedback: dict[str, Any]\n",
				"\n",
				"class EvHumanSatisfied(wfs.Event):\n",
				"\tpass\n",
				"\n",
				"#*------------------------------------------------------------------------------\n",
				"class MyWorkflow(wfs.Workflow):\n",
				"\t@wfs.step()\n",
				"\tasync def categorize_user_query(\n",
				"\t\tself, ctx: wfs.Context, ev: wfs.StartEvent,\n",
				"\t) -> EvUserQueryCategorized:\n",
				"\t\tuser_query = ev.get(\"user_query\", \"\")\n",
				"\n",
				"\t\tuser_query_category = await apis_car.categorize_user_query(user_query)\n",
				"\t\trp_print(user_query_category)\n",
				"\t\t\n",
				"\t\tawait ctx.set(\"user_query\", user_query)\n",
				"\t\tawait ctx.set(\"user_query_category\", user_query_category)\n",
				" \n",
				"\t\treturn EvUserQueryCategorized(user_query_category=user_query_category)\n",
				"\t\n",
				"\t@wfs.step()\n",
				"\tasync def start_flow(\n",
				"\t\tself, ctx: wfs.Context, ev: EvUserQueryCategorized\n",
				"\t) -> EvFlowStartedRag | EvFlowStartedControl | EvFlowStartedGeneral:\n",
				"\t\tuser_query_category: t.UserQueryCategory = await ctx.get(\"user_query_category\")\n",
				"\t\t\n",
				"\t\tflow_mapping = {\n",
				"\t\t\t\"car_manual\": (\"rag\", EvFlowStartedRag),\n",
				"\t\t\t\"car_control\": (\"control\", EvFlowStartedControl),\n",
				"\t\t\t\"general\": (\"general\", EvFlowStartedGeneral),\n",
				"\t\t}\n",
				"\n",
				"\t\tif user_query_category in flow_mapping:\n",
				"\t\t\ttask, event_class = flow_mapping[user_query_category]\n",
				"\t\t\t\n",
				"\t\t\tflow_info = {\n",
				"\t\t\t\t\"activated\": True,\n",
				"\t\t\t\t\"task\": task\n",
				"\t\t\t}\n",
				"\t\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\t\t\treturn event_class()\n",
				"\t\n",
				"\t@wfs.step()\n",
				"\tasync def run_flow_rag(\n",
				"\t\tself, ctx: wfs.Context, ev: EvFlowStartedRag,\n",
				"\t) -> EvFlowDoneRag:\n",
				"\t\tflow_info = await ctx.get(\"flow_info\")\n",
				"\t\tuser_query = await ctx.get(\"user_query\")\n",
				"\n",
				"\t\tresult = await apis_rag.do_querying(user_query=user_query, mode=\"achat\")\n",
				"\n",
				"\t\tif flow_info:\n",
				"\t\t\tflow_info[\"completed\"] = True\n",
				"\t\t\tflow_info[\"result\"] = result\n",
				"   \n",
				"\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\n",
				"\t\treturn EvFlowDoneRag()\n",
				"\n",
				"\t@wfs.step()\n",
				"\tasync def run_flow_general(\n",
				"\t\tself, ctx: wfs.Context, ev: EvFlowStartedGeneral,\n",
				"\t) -> EvFlowDoneGeneral:\n",
				"\t\tflow_info = await ctx.get(\"flow_info\")\n",
				"\t\tuser_query = await ctx.get(\"user_query\")\n",
				"\n",
				"\t\tresult = await apis_car.do_general(user_query=user_query, mode=\"achat\")\n",
				"\n",
				"\t\tif flow_info:\n",
				"\t\t\tflow_info[\"completed\"] = True\n",
				"\t\t\tflow_info[\"result\"] = result\n",
				"   \n",
				"\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\n",
				"\t\treturn EvFlowDoneGeneral()\n",
				"\n",
				"\t@wfs.step()\n",
				"\tasync def run_flow_control(\n",
				"\t\tself, ctx: wfs.Context, ev: EvFlowStartedControl,\n",
				"\t) -> EvFlowDoneControl:\n",
				"\t\tflow_info = await ctx.get(\"flow_info\")\n",
				"\t\tuser_query = await ctx.get(\"user_query\")\n",
				"\n",
				"\t\t# Get tasks\n",
				"\t\ttasks = await apis_car.separate_tasks(user_query=user_query)\n",
				"\t\trp_print(tasks)\n",
				"\n",
				"\t\tif not tasks:\n",
				"\t\t\tif flow_info:\n",
				"\t\t\t\tflow_info[\"tasks\"] = {}\n",
				"\t\t\t\tflow_info[\"n_tasks\"] = 0\n",
				"\t\t\t\tflow_info[\"result\"] = \"No tasks were identified in your request.\"\n",
				"\t\t\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\t\t\treturn EvFlowDoneControl()\n",
				"\n",
				"\t\t# Initialize tasks map with IDs\n",
				"\t\ttask_map = {}\n",
				"\t\tfor task in tasks:\n",
				"\t\t\ttask_id = str(utils.uuid_utils.generate_uuid4())\n",
				"\t\t\ttask_map[task_id] = {\n",
				"\t\t\t\t\"task\": task,\n",
				"\t\t\t\t\"result\": None\n",
				"\t\t\t}\n",
				"\n",
				"\t\tflow_info[\"tasks\"] = task_map\n",
				"\t\tflow_info[\"n_tasks\"] = len(tasks)\n",
				"\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\n",
				"\t\t# Process all tasks concurrently\n",
				"\t\tasync def process_task(task_id: str, task: str) -> tuple[str, str]:\n",
				"\t\t\ttry:\n",
				"\t\t\t\tresult = await apis_car.do_controlling(user_query=task, mode=\"achat\")\n",
				"\t\t\t\treturn task_id, result\n",
				"\t\t\texcept Exception as e:\n",
				"\t\t\t\tlogger.error(f\"Error processing task {task_id}: {str(e)}\")\n",
				"\t\t\t\treturn task_id, f\"Error: {str(e)}\"\n",
				"\n",
				"\t\t# Create and gather all task coroutines\n",
				"\t\tcoroutines = [\n",
				"\t\t\tprocess_task(task_id, task_info[\"task\"]) \n",
				"\t\t\tfor task_id, task_info in task_map.items()\n",
				"\t\t]\n",
				"\t\t\n",
				"\t\t# Execute all tasks concurrently\n",
				"\t\tresults = await asyncio.gather(*coroutines)\n",
				"\n",
				"\t\t# Update flow info with results\n",
				"\t\tfor task_id, result in results:\n",
				"\t\t\tflow_info[\"tasks\"][task_id][\"result\"] = result\n",
				"\n",
				"\t\t# Combine all results\n",
				"\t\tresult_texts = [\n",
				"\t\t\ttask_info[\"result\"] \n",
				"\t\t\tfor task_info in flow_info[\"tasks\"].values() \n",
				"\t\t\tif task_info[\"result\"]\n",
				"\t\t]\n",
				"\t\t\n",
				"\t\tflow_info[\"result\"] = \"\\n\".join(result_texts)\n",
				"\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"\n",
				"\t\treturn EvFlowDoneControl()\n",
				"\t\t\t\n",
				"\t@wfs.step()\n",
				"\tasync def complete_flow(\n",
				"\t\tself, ctx: wfs.Context, ev: EvFlowDoneRag | EvFlowDoneControl | EvFlowDoneGeneral\n",
				"\t) -> EvFlowDone:\n",
				"\t\tflow_info = await ctx.get(\"flow_info\")\n",
				"\n",
				"\t\tif flow_info:\n",
				"\t\t\tflow_info[\"confirmed\"] = True\n",
				"\t\tawait ctx.set(\"flow_info\", flow_info)\n",
				"  \n",
				"\t\treturn EvFlowDone()\n",
				"\n",
				"\t@wfs.step()\n",
				"\tasync def human_feedback(\n",
				"\t\tself, ctx: wfs.Context, ev: EvFlowDone,\n",
				"\t) -> EvHumanFeedbackDone:\n",
				"\t\thuman_feedback = {\n",
				"\t\t\t\"feedback\": \"OK!\",\n",
				"\t\t\t\"retry\": False,\n",
				"\t\t}\n",
				"\n",
				"\t\tawait ctx.set(\"human_feedback\", human_feedback)\n",
				"\t\treturn EvHumanFeedbackDone(human_feedback=human_feedback)\n",
				"\t\n",
				"\t@wfs.step()\n",
				"\tasync def retry(\n",
				"\t\tself, ctx: wfs.Context, ev: EvHumanFeedbackDone\n",
				"\t) -> EvHumanSatisfied | EvUserQueryCategorized:\n",
				"\t\thuman_feedback = await ctx.get(\"human_feedback\")\n",
				"  \n",
				"\t\tif human_feedback[\"retry\"] == True:\n",
				"\t\t\treturn EvUserQueryCategorized(\n",
				"\t\t\t\tuser_query_category=await ctx.get(\"user_query_category\")\n",
				"\t\t\t)\n",
				"\t\telse:\n",
				"\t\t\treturn EvHumanSatisfied()\n",
				" \n",
				"\t@wfs.step()\n",
				"\tasync def stop(\n",
				"\t\tself, ctx: wfs.Context, ev: EvHumanSatisfied,\n",
				"\t) -> wfs.StopEvent:\n",
				"\t\t\n",
				"\t\trp_print(ctx.data)\n",
				"  \n",
				"\t\tflow_info: dict = await ctx.get(\"flow_info\")\n",
				"\t\t\n",
				"\t\tif flow_info:\n",
				"\t\t\tresult = flow_info[\"result\"]\n",
				"\t\t\n",
				"\t\treturn wfs.StopEvent(result=result)\n",
				"\n",
				"async def main():\n",
				"\tworkflow = MyWorkflow(timeout=60, verbose=True)\n",
				"\n",
				"\t# user_query = queries[0]\n",
				"\t# user_query = \"It's too hot\"\n",
				"\t# user_query = \"Is the car trunk opened?\"\n",
				"\t# user_query =  \"Is the car locked?\"\n",
				"\t# user_query = \"Yes\"\n",
				"\t# user_query = \"Activate the AC mode. Increase front wiper speed\"\n",
				"\tuser_query = \"Is the car locked? Is the car trunk opened?\"\n",
				"\t# user_query = \"Is the car locked? Is the car trunk opened?. Increase front wiper speed\"\n",
				"\t# user_query = \"Decrease front wiper speed\"\n",
				"\tresult = await workflow.run(user_query=user_query)\n",
				"\n",
				"wfs.draw_all_possible_flows(MyWorkflow)\n",
				"\n",
				"asyncio.run(main())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query = \"Is the car trunk opened?\"\n",
				"\n",
				"await apis_car.do_controlling(\n",
				"\tuser_query=user_query,\n",
				"\tmode=\"achat\",\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"queries = [\n",
				"\t\"Is the car trunk opened?\",\n",
				"\t\"What are the main safety procedures to follow before driving the vehicle?\",\n",
				"]\n",
				"\n",
				"user_query = queries[0]\n",
				"\n",
				"user_query_category = await apis_car.categorize_user_query(user_query)\n",
				"rp_print(user_query_category)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"queries_rag = [\n",
				"\t\"What are the main safety procedures to follow before driving the vehicle?\",\n",
				"\t\"What is the correct driving posture recommended in the manual?\",\n",
				"\t\"What are the differences between Apple CarPlay and Android Auto integration?\",\n",
				"\t\"How to turn the audio system on and off?\",\n",
				"\t\"What is the complete procedure for securing child restraint systems?\",\n",
				"\t\"Summarize the content of page 200 in the manual\"\n",
				"]\n",
				"\n",
				"user_query = queries_rag[0]\n",
				"\n",
				"# await components.retriever_car_manual.aretrieve(user_query)\n",
				"\n",
				"pprint(await apis_rag.do_querying(user_query=user_query, mode=\"achat\"))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query = \"Activate the AC mode. Increase front wiper speed. Help me unlock doors.\"\n",
				"user_query = \"Increase front wiper speed\"\n",
				"\n",
				"await apis_car.separate_tasks(user_query=user_query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Timing"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import random\n",
				"\n",
				"user_query = queries[random.randint(0, len(queries) - 1)]\n",
				"\n",
				"user_query_category = await utils.time_it(\n",
				"\tlambda: apis_car.categorize_user_query(user_query), \n",
				"\tname=\"Categorize user query\",\n",
				"\t# n_times=10,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query_RAG = queries[0]\n",
				"\n",
				"result = await utils.time_it(\n",
				"\tlambda: apis_rag.do_querying(user_query=user_query_RAG, mode=\"chat\"), \n",
				"\tname=\"Run flow RAG\",\n",
				"\tn_times=10,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"user_query_control = queries[2]\n",
				"\n",
				"result = await utils.time_it(\n",
				"\tlambda: await apis_car.do_controlling(user_query=user_query_control, mode=\"achat\"), \n",
				"\tname=\"Run flow control\",\n",
				"\t# n_times=10,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

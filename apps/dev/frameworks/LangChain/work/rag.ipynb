{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"source": [
				"# Imports"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import packages\n",
				"\n",
				"from context.utils import typer as t\n",
				"from context.infra.clients import logger\n",
				"\n",
				"from toolkit.utils import utils\n",
				"from toolkit.utils.utils import rp_print\n",
				"from toolkit.utils.llm import main as utils_llm\n",
				"\n",
				"import context.instances as inst\n",
				"import context.consts as const\n",
				"import context.settings.main as settings_main\n",
				"\n",
				"from toolkit.llm.langchain.core import integration, utils as utils_lc\n",
				"from toolkit.llm.langchain.data.indexing import (\n",
				"    documents, document_loaders, text_splitters,\n",
				")\n",
				"from toolkit.llm.langchain.data.persistence import retrievers\n",
				"from toolkit.llm.langchain.execution import (\n",
				"    runnables, graphs, tools, agents, tools as tools_lc\n",
				")\n",
				"from toolkit.llm.langchain.models import (\n",
				"    prompts as prompts_lc, llms, messages,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"vector_store = inst.vector_store_in_memory\n",
				"vector_stores = inst.vector_stores_qdrant\n",
				"\n",
				"COLLS = settings_main.VEC_STR_COLLS\n",
				"\n",
				"llm = inst.llm_main"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"source": [
				"# Dev"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Load Data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [],
			"source": [
				"loader = document_loaders.WebBaseLoader(\n",
				"\tweb_path=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
				"\tbs_kwargs={\n",
				"\t\t\"parse_only\": document_loaders.bs4.SoupStrainer(\n",
				"\t\t\tclass_=(\"post-content\", \"post-title\", \"post-header\")\n",
				"\t\t)\n",
				"\t}\n",
				")\n",
				"docs = loader.load()\n",
				"\n",
				"text_splitter = text_splitters.RecursiveCharacterTextSplitter(\n",
				"\tchunk_size=1000, chunk_overlap=200, add_start_index=True,\n",
				")\n",
				"splits = text_splitter.split_documents(docs)\n",
				"\n",
				"len_splits = len(splits)\n",
				"for i, doc in enumerate(splits):\n",
				"\tif i < len_splits//3:\n",
				"\t\tdoc.metadata[\"section\"] = \"beginning\"\n",
				"\telif i < (len_splits*2)//3:\n",
				"\t\tdoc.metadata[\"section\"] = \"middle\"\n",
				"\telse:\n",
				"\t\tdoc.metadata[\"section\"] = \"end\"\n",
				"\n",
				"document_ids = vector_store.add_documents(documents=splits)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Algo"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Basic"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [],
			"source": [
				"# prompt: runnables.Runnable = integration.hub.pull(\"rlm/rag-prompt\")\n",
				"# prompt = prompts.PromptTemplate.from_template(prompts.prompts[\"RAG\"])\n",
				"\n",
				"vector_store = inst.vector_stores_qdrant[\"vehicle\"]\n",
				"\n",
				"class Search(t.TypedDict):\n",
				"\t\"\"\"Search Query\"\"\"\n",
				"\n",
				"\tquery: t.Annotated[str, ..., \"Search query to run.\"]\n",
				"\tsection: t.Annotated[\n",
				"\t\tt.Literal[\"beginning\", \"middle\", \"end\"],\n",
				"\t\t...,\n",
				"\t\t\"Section to query.\",\n",
				"\t]\n",
				"\n",
				"class State(t.TypedDict):\n",
				"\tquestion: str\n",
				"\tquery: Search\n",
				"\tcontext: t.List[documents.Document]\n",
				"\tanswer: str\n",
				"\n",
				"def node_analyze_query(state: State):\n",
				"\tllm_structured = llms.create_structured_llm(inst.llm_main, Search)\n",
				"\tquery = llm_structured.invoke(state[\"question\"])\n",
				"\treturn {\n",
				"\t\t\"query\": query\n",
				"\t}\n",
				"\n",
				"@tools.tool(response_format=\"content_and_artifact\")\n",
				"def node_retrieve(query: str):\n",
				"\t\"\"\"Retrieve information related to a query.\"\"\"\n",
				"\tretrieved_docs = vector_store.similarity_search(\n",
				"\t\tquery=query, k=2,\n",
				"\t\t# filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
				"  )\n",
				"\tserialized = \"\\n\\n\".join(\n",
				"\t\t(f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
				"\t\tfor doc in retrieved_docs\n",
				"\t)\n",
				"\treturn serialized, retrieved_docs\n",
				"\n",
				"node_tools = graphs.ToolNode([node_retrieve])\n",
				"\n",
				"def node_query_or_respond(state: graphs.MessagesState):\n",
				"\t\"\"\"\n",
				"\tGenerate an AI message that may include a tool call for information \n",
				"\tretrieval or respond accordingly.\n",
				"\t\"\"\"\n",
				"\ttooled_llm = llms.create_tooled_llm(inst.llm_main, [node_retrieve])\n",
				"\tresponse = tooled_llm.invoke(state[\"messages\"])\n",
				"\t# MessagesState appends messages to state instead of overwriting\n",
				"\treturn {\n",
				"\t\t\"messages\": [response],\n",
				"\t}\n",
				"\n",
				"def node_generate(state: graphs.MessagesState):\n",
				"\t\"\"\"Generate a response using the retrieved content.\"\"\"\n",
				"\tmessages_tool = []\n",
				"\tfor message in reversed(state[\"messages\"]):\n",
				"\t\tif message.type == messages.TypeMsg.TOOL:\n",
				"\t\t\tmessages_tool.append(message)\n",
				"\t\telse: break\n",
				"\t\n",
				"\tmessages_tool = messages_tool[::-1]\n",
				"\n",
				"\tdocs_content = utils_lc.extract_content(messages_tool)\n",
				"\n",
				"\tprompt_system_rag: str = prompts.prompts[\"RAG\"][\"system\"]\n",
				"\tprompt_system_rag = prompt_system_rag.replace(\"{context}\", docs_content)\n",
				"\n",
				"\tmessage_system = messages.SystemMessage(prompt_system_rag)\n",
				" \n",
				"\tmessages_conversation = [\n",
				"\t\tmessage\n",
				"\t\tfor message in state[\"messages\"]\n",
				"\t\tif message.type in (messages.TypeMsg.HUMAN, messages.TypeMsg.SYSTEM)\n",
				"\t\tor (message.type == messages.TypeMsg.AI and not message.tool_calls)\n",
				"\t]\n",
				" \n",
				"\tprompt = [message_system] + messages_conversation\n",
				" \n",
				"\tresponse = inst.llm_main.invoke(prompt)\n",
				"\n",
				"\treturn {\n",
				"\t\t\"messages\": [response]\n",
				"\t}\n",
				"\n",
				"# graph_builder = graphs.StateGraph(state_schema=State)\n",
				"graph_builder = graphs.StateGraph(graphs.MessagesState)\n",
				"\n",
				"graph_builder.add_node(node_query_or_respond)\n",
				"graph_builder.add_node(node_tools)\n",
				"graph_builder.add_node(node_generate)\n",
				"\n",
				"graph_builder.set_entry_point(node_query_or_respond.__name__)\n",
				"graph_builder.add_conditional_edges(\n",
				"\tnode_query_or_respond.__name__,\n",
				"\tgraphs.tools_condition,\n",
				"\t{\n",
				"\t\tgraphs.END: graphs.END,\n",
				"\t\tnode_tools.name: node_tools.name,\n",
				"\t}\n",
				")\n",
				"graph_builder.add_edge(node_tools.name, node_generate.__name__)\n",
				"graph_builder.add_edge(node_generate.__name__, graphs.END)\n",
				"\n",
				"memory = graphs.MemorySaver()\n",
				"graph = graph_builder.compile(\n",
				"  # checkpointer=memory\n",
				")\n",
				"\n",
				"config = {\n",
				"\t\"configurable\": {\n",
				"\t\t\"thread_id\": \"abc123\",\n",
				"\t}\n",
				"}\n",
				"\n",
				"# utils.display(utils.Image(graph.get_graph().draw_mermaid_png()))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"questions = [\n",
				"  \"Hello\",\n",
				"  \"What is Task Decomposition?\",\n",
				"\t\"What does the end of the post say about Task Decomposition?\",\n",
				"\t(\n",
				"\t\t\"What is the standard method for Task Decomposition?\\n\\n\"\n",
				"\t\t\"Once you get the answer, look up common extensions of that method.\"\n",
				"\t),\n",
				"]\n",
				"\n",
				"questions_vehicle = [\n",
				"\t\"What are the notes to consider before driving?\",\n",
				"]\n",
				"\n",
				"question = questions_vehicle[0]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = {\n",
				"\t\"messages\": [\n",
				"\t\t{\n",
				"\t\t\t\"role\": \"user\",\n",
				"\t\t\t\"content\": question,\n",
				"\t\t}\n",
				"\t]\n",
				"}\n",
				"async for step in graph.astream(inputs, stream_mode=\"updates\", config=config):\n",
				"  rp_print(step)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Agents"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Basic"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [],
			"source": [
				"agent_executor = agents.create_react_agent(\n",
				"\tmodel=inst.llm_main, tools=[node_retrieve], checkpointer=memory,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"graphs.display_graph(agent_executor)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"source": [
				"# Ref"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"- https://python.langchain.com/docs/tutorials/rag/ ✅\n",
				"- https://python.langchain.com/docs/tutorials/qa_chat_history/ ✅\n",
				"- https://python.langchain.com/docs/how_to/qa_sources/\n",
				"- https://python.langchain.com/docs/concepts/retrieval/\n",
				"- https://python.langchain.com/docs/integrations/vector_stores/qdrant/\n",
				"- Graph\n",
				"  - https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\n",
				"  - https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/\n",
				"  - https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/\n",
				"  - https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/\n",
				"  - https://langchain-ai.github.io/langgraph/tutorials/sql-agent/"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Components**\n",
				"\n",
				"https://python.langchain.com/docs/integrations/components/\n",
				"\n",
				"- Document Loaders\n",
				"  - https://python.langchain.com/docs/how_to/#document-loaders\n",
				"  - https://python.langchain.com/docs/integrations/document_loaders/\n",
				"  - https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html\n",
				"- Text Splitters\n",
				"  - https://python.langchain.com/docs/concepts/text_splitters/\n",
				"  - https://python.langchain.com/docs/how_to/#text-splitters\n",
				"  - https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html\n",
				"- Embeddings\n",
				"  - https://python.langchain.com/docs/concepts/embedding_models/\n",
				"  - https://python.langchain.com/docs/how_to/embed_text/\n",
				"  - https://python.langchain.com/docs/integrations/text_embedding/\n",
				"  - https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.Embeddings.html\n",
				"- Vector Stores\n",
				"  - https://python.langchain.com/docs/concepts/vector_stores/\n",
				"  - https://python.langchain.com/docs/how_to/vector_stores/\n",
				"  - https://python.langchain.com/docs/integrations/vector_stores/\n",
				"  - https://python.langchain.com/api_reference/core/vector_stores/langchain_core.vector_stores.base.vector_store.html\n",
				"- Retrievers\n",
				"  - https://python.langchain.com/docs/concepts/retrievers/\n",
				"  - https://python.langchain.com/docs/how_to/#retrievers\n",
				"- LLMs\n",
				"  - https://python.langchain.com/docs/concepts/chat_models/\n",
				"  - https://python.langchain.com/docs/how_to/#chat-models\n",
				"  - https://python.langchain.com/docs/integrations/chat/\n",
				"  - https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html\n",
				"- Tools\n",
				"  - https://python.langchain.com/docs/concepts/tools/\n",
				"  - https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html\n",
				"  - https://python.langchain.com/docs/how_to/custom_tools/"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Concepts**\n",
				"\n",
				"- https://python.langchain.com/docs/concepts/structured_outputs/\n",
				"- https://python.langchain.com/docs/concepts/chat_history/"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Techniques**\n",
				"\n",
				"- https://python.langchain.com/docs/how_to/#query-analysis\n",
				"- https://python.langchain.com/docs/how_to/message_history/"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "dev",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.15"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}

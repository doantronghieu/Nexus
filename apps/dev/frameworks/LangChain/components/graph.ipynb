{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import packages\n",
    "\n",
    "from context.utils import typer as t\n",
    "from context.utils.handlers import print_hldr\n",
    "from toolkit.utils import utils\n",
    "from toolkit.utils.utils import rp_print\n",
    "\n",
    "from context.infra import clients\n",
    "import context.instances as inst\n",
    "import context.consts as const\n",
    "import context.settings.main as settings_main\n",
    "\n",
    "from toolkit.llm.langchain.core import integration, utils as utils_lc\n",
    "from toolkit.llm.langchain.data.persistence import retrievers\n",
    "from toolkit.llm.langchain.data.indexing import (\n",
    "  documents, document_loaders, text_splitters,\n",
    ")\n",
    "from toolkit.llm.langchain.execution import (\n",
    "  runnables, graphs, tools as tools_lc, agents\n",
    ")\n",
    "from toolkit.llm.langchain.models import (\n",
    "\tprompts as prompts_lc, llms, messages as msgs_lc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = settings_main.CHOSEN[\"framework_llm\"]\n",
    "\n",
    "# vector_store = inst.vector_store_in_memory\n",
    "vector_stores = inst.vector_stores_qdrant\n",
    "\n",
    "COLLS = settings_main.VEC_STR_COLLS\n",
    "\n",
    "llm = inst.llm_main\n",
    "embedding = inst.embedding_main\n",
    "\n",
    "prompts = prompts_lc.prompts\n",
    "# prompt_system_rag = prompt_system_rag.replace(\"{context}\", docs_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODE(t.EnumCustom):\n",
    "\tTOOLS = t.auto()\n",
    "\tAGENT = t.auto()\n",
    "\n",
    "@tools_lc.tool\n",
    "def search(query: str):\n",
    "\t\"\"\"Call to surf the web.\"\"\"\n",
    "\tif \"sf\" in query.lower() or \"san francisco\" in query.lower():\n",
    "\t\t\treturn \"It's 60 degrees and foggy.\"\n",
    "\treturn \"It's 90 degrees and sunny.\"\n",
    "\n",
    "tools = [search] \n",
    "node_tool = graphs.ToolNode(tools)\n",
    "\n",
    "llm = llms.create_tooled_llm(llm, tools)\n",
    "\n",
    "def should_continue(\n",
    "\tstate: graphs.MessagesState\n",
    ") -> t.Literal[NODE.TOOLS, graphs.END]:\n",
    "\tmsgs = state[\"messages\"]\n",
    "\tlast_msg = msgs[-1]\n",
    "\t# If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "\tif last_msg.tool_calls:\n",
    "\t\treturn NODE.TOOLS\n",
    "\t# Otherwise, we stop (reply to the user)\n",
    "\treturn graphs.END\n",
    "\n",
    "def agent(state: graphs.MessagesState):\n",
    "\tmsgs = state[\"messages\"]\n",
    "\tresponse = llm.invoke(msgs)\n",
    "\t# We return a list, because this will get added to the existing list\n",
    "\treturn {\"messages\": [response]}\n",
    "\n",
    "builder = graphs.StateGraph(state_schema=graphs.MessagesState)\n",
    "\n",
    "builder.add_node(NODE.AGENT, agent)\n",
    "builder.add_node(NODE.TOOLS, node_tool)\n",
    "\n",
    "# Set the entrypoint as `agent`; this node is called first.\n",
    "builder.add_edge(graphs.START, NODE.AGENT)\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "\t# Define the start node as `agent`; these are the edges taken after `agent` is called.\n",
    "\tNODE.AGENT,\n",
    "\t# function that will determine which node is called next\n",
    "\tshould_continue,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `tools` to `agent`, so `agent` is called after `tools`.\n",
    "builder.add_edge(NODE.TOOLS, NODE.AGENT)\n",
    "\n",
    "checkpointer = graphs.MemorySaver()\n",
    "\n",
    "# Compile it into a LangChain Runnable, which can be used like any other runnable. \n",
    "# Optionally, pass the memory when compiling the graph.\n",
    "app = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODE(t.EnumCustom):\n",
    "\t# Node can be either an LLM or an agent.\n",
    "\tBOT = t.auto()\n",
    "\tTOOLS = t.auto()\n",
    "\tHUMAN = t.auto()\n",
    "\n",
    "\tMANAGER = t.auto() \n",
    "\tLEAD_ = t.auto()\n",
    "\tMEMBER_ = t.auto()\n",
    "\n",
    "\tAGENT_ = t.auto() # An agent can also be defined as a tool.\n",
    "\n",
    "class FEATURE:\n",
    "  ASK_HUMAN = False\n",
    "\n",
    "class ARCHITECTURE(t.EnumCustom):\n",
    "  NETWORK = t.auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 LangGraph Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEATURE:\n",
    "  ASK_HUMAN = False\n",
    "\n",
    "class NODE(t.EnumCustom):\n",
    "\tBOT = t.auto()\n",
    "\tTOOLS = t.auto()\n",
    "\tHUMAN = t.auto()\n",
    "\n",
    "class State(t.TypedDict):\n",
    "\t# Messages are of type \"list\".\n",
    "\t# The `add_messages` function in the annotation defines how this state key \n",
    "\t# should be updated, appending messages to the list rather than overwriting them.\n",
    "\tmessages: t.Annotated[list, graphs.add_messages]\n",
    "\n",
    "\tif FEATURE.ASK_HUMAN:\n",
    "\t\task_human: bool\n",
    "\n",
    "class ToolRequestAssistance(t.BaseModel):\n",
    "\t\"\"\"\n",
    "\tEscalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\tTo use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    " \t\"\"\"\n",
    "\trequest: str\n",
    "\n",
    "tools = [\n",
    "\ttools_lc.tool_search_tavily,\n",
    "]\n",
    "\n",
    "if FEATURE.ASK_HUMAN: tools.append(ToolRequestAssistance)\n",
    "\n",
    "node_tool = graphs.ToolNode(tools)\n",
    "\n",
    "llm = llms.create_tooled_llm(llm, tools)\n",
    "\n",
    "def node_bot(state: State) -> State:\n",
    "\tresponse: msgs_lc.AIMessage = llm.invoke(state[\"messages\"])\n",
    " \n",
    "\tresult = {\n",
    "\t\t\"messages\": [\n",
    "\t\t\tresponse\n",
    "\t\t],\n",
    "\t}\t\n",
    " \n",
    "\tif FEATURE.ASK_HUMAN:\n",
    "\t\task_human = False\n",
    "\t\n",
    "\t\tif (\n",
    "\t\t\tresponse.tool_calls and \n",
    "\t\t\tresponse.tool_calls[0][\"name\"] == ToolRequestAssistance.__name__\n",
    "\t\t):\n",
    "\t\t\task_human = True,\n",
    "   \n",
    "\t\tresult[\"ask_human\"] = ask_human\n",
    " \n",
    "\treturn result\n",
    "\n",
    "def node_human(state: State) -> State:\n",
    "\tmessages_new = []\n",
    "\n",
    "\tif not isinstance(state[\"messages\"][-1], msgs_lc.ToolMessage):\n",
    "\t\t# Typically, the user will have updated the state during the interrupt.\n",
    "\t\t# If they choose not to, we will include a placeholder ToolMessage to\n",
    "\t\t# let the LLM continue.\n",
    "\t\t\n",
    "\t\tresponse = input(\"Please provide the expert guidance.\")\n",
    "  \n",
    "\t\tmessages_new.append(msgs_lc.respond_tool_calling(\n",
    "\t\t\tresponse=response,\n",
    "\t\t\tmessage_ai=state[\"messages\"][-1],\n",
    "\t\t))\n",
    "\t\n",
    "\treturn {\n",
    "\t\t\"messages\": messages_new,\n",
    "\t\t\"ask_human\": False,\n",
    "\t}\n",
    "\n",
    "def router(state: State):\n",
    "\tif FEATURE.ASK_HUMAN:\n",
    "\t\tif state[\"ask_human\"]:\n",
    "\t\t\treturn NODE.HUMAN\n",
    "\treturn graphs.tools_condition(state)\n",
    "\n",
    "builder = graphs.StateGraph(State)\n",
    "\n",
    "# The first argument is the unique node name, and the second argument is the \n",
    "# function or object called whenever the node is used.\n",
    "builder.add_node(NODE.BOT, node_bot)\n",
    "builder.add_node(NODE.TOOLS, node_tool)\n",
    "builder.add_node(NODE.HUMAN, node_human)\n",
    "\n",
    "builder.add_edge(graphs.START, NODE.BOT)\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "  NODE.BOT, \n",
    "  router,\n",
    "\t{\n",
    "\t\tNODE.HUMAN: NODE.HUMAN,\n",
    "\t\tNODE.TOOLS: NODE.TOOLS,\n",
    "\t\tgraphs.END: graphs.END,\n",
    "\t}\n",
    ")\n",
    "\n",
    "builder.add_edge(NODE.TOOLS, NODE.BOT)\n",
    "builder.add_edge(NODE.HUMAN, NODE.BOT)\n",
    "\n",
    "memory = graphs.MemorySaver()\n",
    "\n",
    "app = builder.compile(\n",
    "  checkpointer=memory,\n",
    "\tinterrupt_before=[\n",
    "\t\tNODE.TOOLS,\n",
    "\t\tNODE.HUMAN,\n",
    "  ],\n",
    "\t# interrupt_after=[NODE.TOOLS],\n",
    ")\n",
    "\n",
    "# graphs.display(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "\t\"Hello\",\n",
    "\t\"Hi there! My name is Will.\",\n",
    "\t\"Remember my name?\",\n",
    "\t\"What do you know about LangGraph?\",\n",
    "\t\"I'm learning LangGraph. Could you do some research on it for me?\",\n",
    "\t\"I need some expert guidance for building this AI agent. Could you request assistance for me?\",\n",
    "\n",
    "]\n",
    "user_input = inputs[5]\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# for event in app.stream(\n",
    "#   input={\"messages\": [(\"user\", user_input)]},\n",
    "# \tconfig=config, stream_mode=\"values\"\n",
    "# ):\n",
    "# \tevent[\"messages\"][-1].pretty_print()\n",
    "\n",
    "while True:\n",
    "\tfor event in app.stream(\n",
    "\t\tinput={\"messages\": [(\"user\", user_input)]},\n",
    "\t\tconfig=config, stream_mode=\"values\"\n",
    "\t):\n",
    "\t\tevent[\"messages\"][-1].pretty_print()\n",
    "\t\n",
    "\tsnapshot = graphs.get_snapshot(app, config)\n",
    "\tmessage_current: t.Union[msgs_lc.BaseMessage, msgs_lc.AIMessage] = snapshot.values[\"messages\"][-1]\n",
    "\n",
    "\tis_tool_calling = msgs_lc.is_tool_calling(message_current)\n",
    "\t\n",
    "\tif is_tool_calling:\n",
    "\t\ttool_name = message_current.tool_calls[0]['name']\n",
    "\t\t\n",
    "\t\tif tool_name == tools_lc.tool_search_tavily.name:\n",
    "\t\t\ttool_query = message_current.tool_calls[0]['args'][\"query\"]\n",
    "\n",
    "\t\t\tif input(f\"Would you like me to begin with this input? `{tool_query}\").lower() in [\"no\"]:\n",
    "\t\t\t\tinput_new = input(\"Please enter your desired input.\")\n",
    "\t\t\t\tapp = graphs.update_last_tool_calling(app, config, input_new=input_new)\n",
    "\n",
    "\t\t\tif input(\"Would you like to continue?\").lower() in [\"yes\", \"ok\"]:\n",
    "\t\t\t\tfor event in app.stream(\n",
    "\t\t\t\t\tinput=None,\n",
    "\t\t\t\t\tconfig=config, stream_mode=\"values\"\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tevent[\"messages\"][-1].pretty_print()\n",
    "\t\t\telse:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif tool_name == ToolRequestAssistance.__name__:\n",
    "\t\t\tresponse_human = (\n",
    "\t\t\t\t\"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "\t\t\t\t\" It's much more reliable and extensible than simple autonomous agents.\"\n",
    "\t\t\t)\n",
    "\t\t\tmessage_tool = msgs_lc.respond_tool_calling(response_human, message_current)\n",
    "\t\t\t\n",
    "\t\t\tapp.update_state(config, {\"messages\": [message_tool]})\n",
    "\n",
    "\t\t\tfor event in app.stream(\n",
    "\t\t\t\tinput=None,\n",
    "\t\t\t\tconfig=config, stream_mode=\"values\"\n",
    "\t\t\t):\n",
    "\t\t\t\tevent[\"messages\"][-1].pretty_print()\n",
    "\t\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODE(t.EnumCustom):\n",
    "\tAGENT_RESEARCH = t.auto()\n",
    "\tAGENT_CHART = t.auto()\n",
    "\n",
    "def get_next_node(message_current: msgs_lc.BaseMessage, goto: str):\n",
    "\tif \"FINAL ANSWER\" in message_current.content:\n",
    "\t\treturn graphs.END\n",
    "\treturn goto\n",
    "\n",
    "agent_research = agents.create_react_agent(\n",
    "\tmodel=llm,\n",
    "\ttools=[tools_lc.tool_search_tavily],\n",
    "\tstate_modifier=prompts[\"agents\"][\"tpl_system_network\"].replace(\n",
    "\t\t\"{suffix}\",\n",
    "\t\t\"You can only do research. You are working with a chart generator colleague.\",\n",
    "\t)\n",
    ")\n",
    "\n",
    "def node_agent_research(\n",
    "\tstate: graphs.MessagesState,\n",
    ") -> graphs.Command[t.Literal[NODE.AGENT_CHART, graphs.END]]:\n",
    "\trp_print(state)\n",
    "\tresult = agent_research.invoke(state)\n",
    "\trp_print(result)\n",
    "\tmessage_current = graphs.get_lastest_message(result)\n",
    "\t\n",
    "\tgoto = get_next_node(message_current, NODE.AGENT_CHART)\n",
    "\n",
    "\tmessage_current = graphs.manipulate_message(\n",
    "\t\tmessage=message_current, type_msg=msgs_lc.TypeMsg.HUMAN,\n",
    "\t\tname=NODE.AGENT_RESEARCH,\n",
    "\t)\n",
    " \n",
    "\t# wrap in a human message, as not all providers allow\n",
    "\t# AI message at the last position of the input messages list\n",
    "\tresult[\"messages\"][-1] = message_current\n",
    "\t\n",
    "\treturn graphs.Command(\n",
    "\t\tupdate={\n",
    "\t\t\t# share internal message history of between agents\n",
    "\t\t\t\"messages\": result[\"messages\"],\n",
    "\t\t},\n",
    "\t\tgoto=goto,\n",
    "\t)\n",
    "\n",
    "agent_chart = agents.create_react_agent(\n",
    "\tmodel=llm,\n",
    "\ttools=[tools_lc.tool_python_repl],\n",
    "\tstate_modifier=prompts[\"agents\"][\"tpl_system_network\"].replace(\n",
    "\t\t\"{suffix}\",\n",
    "\t\t\"You can only generate charts. You are working with a researcher colleague.\",\n",
    "\t)\n",
    ")\n",
    "\n",
    "def node_agent_chart(\n",
    "\tstate: graphs.MessagesState,\n",
    ") -> graphs.Command[t.Literal[NODE.AGENT_RESEARCH, graphs.END]]:\n",
    "\tresult = agent_chart.invoke(state)\n",
    "\tmessage_current = graphs.get_lastest_message(result)\n",
    "\t\n",
    "\tgoto = get_next_node(message_current, NODE.AGENT_RESEARCH)\n",
    "\n",
    "\tmessage_current = graphs.manipulate_message(\n",
    "\t\tmessage=message_current, type_msg=msgs_lc.TypeMsg.HUMAN,\n",
    "\t\tname=NODE.AGENT_CHART,\n",
    "\t)\n",
    " \n",
    "\t# wrap in a human message, as not all providers allow\n",
    "\t# AI message at the last position of the input messages list\n",
    "\tresult[\"messages\"][-1] = message_current\n",
    " \n",
    "\treturn graphs.Command(\n",
    "\t\tupdate={\n",
    "\t\t\t# share internal message history of between agents\n",
    "\t\t\t\"messages\": result[\"messages\"],\n",
    "\t\t},\n",
    "\t\tgoto=goto,\n",
    "\t)\n",
    "\n",
    "builder = graphs.StateGraph(graphs.MessagesState)\n",
    "\n",
    "builder.add_node(NODE.AGENT_RESEARCH, node_agent_research)\n",
    "builder.add_node(NODE.AGENT_CHART, node_agent_chart)\n",
    "\n",
    "builder.add_edge(graphs.START, NODE.AGENT_RESEARCH)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "# graphs.display_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = (\n",
    "\t\"First, get the UK's GDP over the past 5 years, then make a line chart of it. \"\n",
    "\t\"Once you make the chart, finish.\",\n",
    ")\n",
    "\n",
    "for event in app.stream(\n",
    "\tinput={\"messages\": [(\"user\", user_input)]},\n",
    "\t# config=config, stream_mode=\"values\"\n",
    "):\n",
    "\t# event[\"messages\"][-1].pretty_print()\n",
    "\trp_print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervisor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Teams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan-and-Execute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasoning without Observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLMCompiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection & Critique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Reflection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflexion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree of Thoughts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Agent Tree Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Discover Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controllability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create branches for parallel execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create map-reduce branches for parallel execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to control graph recursion limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to combine control flow and state updates with Command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add thread-level persistence to your graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add thread-level persistence to subgraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add cross-thread persistence to your graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use Postgres checkpointer for persistence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use MongoDB checkpointer for persistence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create a custom checkpointer using Redis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to manage conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to delete messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add summary conversation memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add long-term memory (cross-thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use semantic search for long-term memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to wait for user input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to review tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add static breakpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to edit graph state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add dynamic breakpoints with NodeInterrupt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to view and update past graph state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream\n",
    "#### How to stream LLM tokens from specific nodes\n",
    "#### How to stream data from within a tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(t.TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    poem: str\n",
    "\n",
    "async def node_generate(state: State, config: runnables.RunnableConfig) -> State:\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    print_hldr(\"Writing joke ...\")\n",
    "    response_joke = await inst.llm_main.ainvoke(\n",
    "        [msgs_lc.HumanMessage(f\"Write a joke about {topic}\")],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    print_hldr(\"\\n\\nWriting poem ...\")\n",
    "    response_poem = await inst.llm_main.ainvoke(\n",
    "        [msgs_lc.HumanMessage(f\"Write a short poem about {topic}\")],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'joke': response_joke.content,\n",
    "        'poem': response_poem.content,\n",
    "    }\n",
    "\n",
    "\n",
    "@tools_lc.tool\n",
    "async def get_something(\n",
    "    the_input: t.Any, config: runnables.RunnableConfig\n",
    ") -> t.Any:\n",
    "    \"\"\"\n",
    "    Use this tool to get something from the input.\n",
    "    \"\"\"\n",
    "\n",
    "    result: msgs_lc.AnyMessage = await clients.error_handler_silent.execute(\n",
    "        lambda: inst.llm_main.ainvoke(\n",
    "            [msgs_lc.HumanMessage(f\"Get something from {the_input}\")],\n",
    "            config=config,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    result = clients.error_handler_silent.execute(lambda: result.content)\n",
    "\n",
    "    return result\n",
    "\n",
    "builder = graphs.StateGraph(State)\n",
    "\n",
    "builder.add_node(node_generate)\n",
    "\n",
    "builder.add_edge(graphs.START, node_generate.__name__)\n",
    "builder.add_edge(node_generate.__name__, graphs.END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for msg_chunk, metadata in graph.astream(\n",
    "    {\n",
    "        \"topic\": \"ice cream\",\n",
    "    },\n",
    "    stream_mode=\"messages\",\n",
    "):  \n",
    "    msg_chunk: msgs_lc.AnyMessage\n",
    "    metadata: t.Dict\n",
    "    \n",
    "    cond_state = \"joke\" in metadata.get(\"tags\", [])\n",
    "    cond_node = metadata[\"langgraph_node\"] == node_generate.__name__\n",
    "    cond_tool = metadata[\"langgraph_node\"] == \"tools\"\n",
    "\n",
    "    if msg_chunk.content and cond_node:\n",
    "        print_hldr(msg_chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"topic\": \"ice cream\",\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in graph.astream(\n",
    "\tinput={\"topic\": \"ice cream\"},\n",
    "\tstream_mode=\"values\"\n",
    "):\n",
    "\tprint(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream from subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSub(t.TypedDict):\n",
    "    foo: str # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "def node_sub_1(state: StateSub) -> StateSub:\n",
    "    return {\n",
    "        \"bar\": \"bar\",\n",
    "    }\n",
    "\n",
    "def node_sub_2(state: StateSub) -> StateSub:\n",
    "    return {\n",
    "        \"foo\": state[\"foo\"] + state[\"bar\"]\n",
    "    }\n",
    "\n",
    "builder_sub = graphs.StateGraph(StateSub)\n",
    "builder_sub.add_node(node_sub_1)\n",
    "builder_sub.add_node(node_sub_2)\n",
    "builder_sub.add_edge(graphs.START, node_sub_1.__name__)\n",
    "builder_sub.add_edge(node_sub_1.__name__, node_sub_2.__name__)\n",
    "builder_sub.add_edge(node_sub_2.__name__, graphs.END)\n",
    "graph_sub = builder_sub.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateMain(t.TypedDict):\n",
    "    foo: str # note that this key is shared with the child graph state\n",
    "\n",
    "def node_main_1(state: StateMain) -> StateMain:\n",
    "    return {\n",
    "        \"foo\": \"hi!\" + state[\"foo\"]\n",
    "    }\n",
    "\n",
    "builder_main = graphs.StateGraph(StateMain)\n",
    "builder_main.add_node(\"node_1\", node_main_1)\n",
    "builder_main.add_node(\"node_2\", graph_sub)\n",
    "builder_main.add_edge(graphs.START, \"node_1\")\n",
    "builder_main.add_edge(\"node_1\", \"node_2\")\n",
    "builder_main.add_edge(\"node_2\", graphs.END)\n",
    "graph_main = builder_main.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph_main.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"updates\",\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(chunk)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to disable streaming for models that don't support it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to call tools using ToolNode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to handle tool calling errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to pass runtime values to tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to pass config to tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to update graph state from tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to handle large numbers of tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph nodes\n",
    "class NODES_SUB(t.EnumCustom):\n",
    "    STEP1 = t.auto()    \n",
    "    STEP2 = t.auto()    \n",
    "\n",
    "# Subgraph state\n",
    "class State_Sub(t.TypedDict):\n",
    "    input_text: str    \n",
    "    helper_text: str   \n",
    "\n",
    "def node_sub_step1(state: State_Sub) -> dict:\n",
    "    \"\"\"First subgraph step: adds helper text\"\"\"\n",
    "    return {\n",
    "        \"helper_text\": \"HELPER\"\n",
    "    }\n",
    "\n",
    "def node_sub_step2(state: State_Sub) -> dict:\n",
    "    \"\"\"Second subgraph step: combines input with helper\"\"\"\n",
    "    return {\n",
    "        \"input_text\": f\"{state['input_text']}_{state['helper_text']}\"\n",
    "    }\n",
    "\n",
    "# Build subgraph\n",
    "builder_sub = graphs.StateGraph(State_Sub)\n",
    "builder_sub.add_node(NODES_SUB.STEP1, node_sub_step1)\n",
    "builder_sub.add_node(NODES_SUB.STEP2, node_sub_step2)\n",
    "builder_sub.add_edge(graphs.START, NODES_SUB.STEP1)\n",
    "builder_sub.add_edge(NODES_SUB.STEP1, NODES_SUB.STEP2)\n",
    "graph_sub = builder_sub.compile()\n",
    "\n",
    "# Main graph nodes\n",
    "class NODES_MAIN(t.EnumCustom):\n",
    "    STEP1 = t.auto()    \n",
    "    STEP2 = t.auto()    \n",
    "\n",
    "# Main graph state\n",
    "class State_Main(t.TypedDict):\n",
    "    text: str    # Text being processed\n",
    "\n",
    "def node_main_step1(state: State_Main) -> dict:\n",
    "    \"\"\"First main step: formats text\"\"\"\n",
    "    return {\n",
    "        \"text\": f\"Main_{state['text']}\"\n",
    "    }\n",
    "\n",
    "def node_main_step2(state: State_Main) -> dict:\n",
    "    \"\"\"Second main step: uses subgraph to enhance text\"\"\"\n",
    "    # Call subgraph with current text\n",
    "    sub_result = graph_sub.invoke({\"input_text\": state[\"text\"]})\n",
    "    \n",
    "    return {\n",
    "        \"text\": sub_result[\"input_text\"]\n",
    "    }\n",
    "\n",
    "# Build main graph\n",
    "builder_main = graphs.StateGraph(State_Main)\n",
    "builder_main.add_node(NODES_MAIN.STEP1, node_main_step1)\n",
    "builder_main.add_node(NODES_MAIN.STEP2, node_main_step2)\n",
    "builder_main.add_edge(graphs.START, NODES_MAIN.STEP1)\n",
    "builder_main.add_edge(NODES_MAIN.STEP1, NODES_MAIN.STEP2)\n",
    "graph_main = builder_main.compile()\n",
    "\n",
    "# Test the graph\n",
    "for chunk in graph_main.stream({\"text\": \"Test\"}, subgraphs=True):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to view and update state in subgraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to transform inputs and outputs of a subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to implement handoffs between agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, BaseMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "model = inst.llm_main\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Transfer the calculation to the multiplication expert.\"\"\"\n",
    "    return\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Transfer the calculation to the addition expert.\"\"\"\n",
    "    return\n",
    "\n",
    "def has_calculations_remaining(messages: list[BaseMessage]) -> bool:\n",
    "    if not messages:\n",
    "        return True\n",
    "    last_msg = messages[-1]\n",
    "    content = last_msg.content if isinstance(last_msg, AIMessage) else str(last_msg)\n",
    "    return not (any(phrase in content.lower() for phrase in [\"final result is\", \"the result is\", \"= \", \"equals\"]) \n",
    "                and any(char.isdigit() for char in content))\n",
    "\n",
    "def process_tools(response: AIMessage, messages: list) -> tuple[list, bool]:\n",
    "    updated_msgs = messages + [response]\n",
    "    transfer = False\n",
    "    \n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            name, call_id = tool_call[\"name\"], tool_call[\"id\"]\n",
    "            if name in [\"add\", \"multiply\"]:\n",
    "                result = globals()[name].invoke(tool_call)\n",
    "                updated_msgs.append(ToolMessage(content=str(result), tool_call_id=call_id))\n",
    "            elif \"transfer\" in name:\n",
    "                updated_msgs.append(ToolMessage(content=\"Transfer request acknowledged\", tool_call_id=call_id))\n",
    "                transfer = True\n",
    "    \n",
    "    return updated_msgs, transfer\n",
    "\n",
    "def create_expert(expert_type: str):\n",
    "    def expert(state: MessagesState) -> Command[Literal[\"multiplication_expert\", \"addition_expert\", \"__end__\"]]:\n",
    "        messages = state[\"messages\"]\n",
    "        if not has_calculations_remaining(messages):\n",
    "            return Command(goto=\"__end__\", update={\"messages\": messages})\n",
    "\n",
    "        system_msg = SystemMessage(content=(\n",
    "            f\"You are a {expert_type} expert. Follow these rules:\\n\"\n",
    "            f\"1. If you see a {expert_type} problem, solve it using the {expert_type.split()[0]} tool\\n\"\n",
    "            f\"2. If you see a {'multiplication' if 'addition' in expert_type else 'addition'} problem, \"\n",
    "            f\"use transfer_to_{'multiplication' if 'addition' in expert_type else 'addition'}_expert\\n\"\n",
    "            \"3. Once you've done the calculation, state the result clearly\\n\"\n",
    "        ))\n",
    "\n",
    "        tools = [add if \"addition\" in expert_type else multiply,\n",
    "                transfer_to_multiplication_expert if \"addition\" in expert_type else transfer_to_addition_expert]\n",
    "        response = model.bind_tools(tools).invoke([system_msg] + messages)\n",
    "        updated_msgs, should_transfer = process_tools(response, messages)\n",
    "        \n",
    "        return Command(\n",
    "            goto=\"multiplication_expert\" if \"addition\" in expert_type else \"addition_expert\" if should_transfer else \"__end__\",\n",
    "            update={\"messages\": updated_msgs}\n",
    "        )\n",
    "    return expert\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", create_expert(\"addition\"))\n",
    "builder.add_node(\"multiplication_expert\", create_expert(\"multiplication\"))\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()\n",
    "\n",
    "def run_calculation(question: str):\n",
    "    \"\"\"Run a calculation through the expert system.\"\"\"\n",
    "    for chunk in graph.stream({\"messages\": [HumanMessage(content=question)]}):\n",
    "        if isinstance(chunk, tuple):\n",
    "            ns, update = chunk\n",
    "            if not ns:\n",
    "                continue\n",
    "            print(f\"Update from subgraph {ns[-1].split(':')[0]}:\\n\")\n",
    "        else:\n",
    "            for node_name, node_update in chunk.items():\n",
    "                print(f\"Update from node {node_name}:\\n\")\n",
    "                for m in node_update[\"messages\"]:\n",
    "                    if isinstance(m, (HumanMessage, AIMessage, SystemMessage, ToolMessage)):\n",
    "                        m.pretty_print()\n",
    "                    else:\n",
    "                        print(m)\n",
    "                print(\"\\n\")\n",
    "\n",
    "# Example usage\n",
    "run_calculation(\"what's (3 + 5) * 12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Any, Optional\n",
    "from pydantic import create_model, BaseModel\n",
    "\n",
    "def create_transfer_tool(description: str, name: str = None):\n",
    "    # Get the calling frame\n",
    "    frame = inspect.currentframe().f_back\n",
    "    \n",
    "    # If name not provided, try to get the variable name from assignment\n",
    "    if name is None:\n",
    "        # Get the code context from the frame\n",
    "        context = inspect.getframeinfo(frame).code_context\n",
    "        if context:\n",
    "            # Find the variable name from assignment\n",
    "            caller_lines = \"\".join(context)\n",
    "            assignment = caller_lines.split(\"=\")[0].strip()\n",
    "            name = assignment\n",
    "    \n",
    "    # Fallback if we couldn't determine the name\n",
    "    if not name:\n",
    "        name = \"transfer_tool\"\n",
    "        \n",
    "    # Create a simple schema with message field\n",
    "    schema = create_model(\n",
    "        f\"{name}_schema\",\n",
    "        __base__=BaseModel,\n",
    "        query=(str, None)  # Optional message field for any additional info\n",
    "    )\n",
    "    \n",
    "    return tools_lc.StructuredTool(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        func=lambda query=None: None,  # Simple pass-through function\n",
    "        args_schema=schema\n",
    "    )\n",
    "\n",
    "# Usage example - much simpler now\n",
    "tool_transfer_to_agent_mul = create_transfer_tool(\n",
    "    description=\"Ask multiplication agent for help.\"\n",
    ")\n",
    "\n",
    "tool_transfer_to_agent_add = create_transfer_tool(\n",
    "    description=\"Ask addition agent for help.\"\n",
    ")\n",
    "\n",
    "# llm = llms.create_tooled_llm(\n",
    "#     inst.llm_main,\n",
    "#     tools=[tool_transfer_to_agent_mul, tool_transfer_to_agent_add]\n",
    "# )\n",
    "# result = llm.invoke(\"How to add one and two multiply three\")\n",
    "# rp_print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define registry with proper framework typing\n",
    "TOOL_REGISTRY: t.Dict[str, tools_lc.BaseTool] = {}\n",
    "\n",
    "def process_tools(response: msgs_lc.AIMessage, messages: list) -> tuple[list, bool]:\n",
    "    \"\"\"\n",
    "    Process tool calls from the LLM response.\n",
    "    \n",
    "    Args:\n",
    "        response: The AI message containing tool calls\n",
    "        messages: Current message history\n",
    "        \n",
    "    Returns:\n",
    "        tuple[list, bool]: Updated messages and transfer flag\n",
    "    \"\"\"\n",
    "    updated_msgs = messages + [response]\n",
    "    transfer = False\n",
    "    \n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            name = tool_call[\"name\"]\n",
    "            call_id = tool_call[\"id\"]\n",
    "            \n",
    "            if name in TOOL_REGISTRY:\n",
    "                # Use the tool's built-in invoke method\n",
    "                tool = TOOL_REGISTRY[name]\n",
    "                result = tool.invoke(tool_call)\n",
    "                \n",
    "                # Extract just the value from the result\n",
    "                if isinstance(result, str) and 'content=' in result:\n",
    "                    content = result.split(\"'\")[1]\n",
    "                else:\n",
    "                    content = str(result)\n",
    "                \n",
    "                tool_msg = msgs_lc.ToolMessage(\n",
    "                    content=content,\n",
    "                    tool_call_id=call_id\n",
    "                )\n",
    "                updated_msgs.append(tool_msg)\n",
    "            elif \"tool_transfer_to_agent\" in name:\n",
    "                tool_msg = msgs_lc.ToolMessage(\n",
    "                    content=\"Transfer request acknowledged\",\n",
    "                    tool_call_id=call_id\n",
    "                )\n",
    "                updated_msgs.append(tool_msg)\n",
    "                transfer = True\n",
    "    \n",
    "    return updated_msgs, transfer\n",
    "\n",
    "@tools_lc.tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tools_lc.tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Register tools after creation\n",
    "TOOL_REGISTRY['add'] = add\n",
    "TOOL_REGISTRY['multiply'] = multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODE(t.EnumCustom):\n",
    "\tAGENT_ADD = t.auto()\n",
    "\tAGENT_MUL = t.auto()\n",
    "\t\n",
    "# Define template for system prompts\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are an expert agent specializing in {domain}. \n",
    "\n",
    "ROLE AND RESPONSIBILITIES:\n",
    "1. You are responsible for {primary_task}\n",
    "2. Stay focused on your specific expertise area\n",
    "3. Collaborate with other agents when needed\n",
    "\n",
    "TOOL USAGE PRINCIPLES:\n",
    "1. Use your assigned tools effectively and appropriately\n",
    "2. Make all necessary tool calls in a single response\n",
    "3. Validate inputs before using tools\n",
    "4. Process results clearly and accurately\n",
    "\n",
    "COLLABORATION RULES:\n",
    "1. Transfer to other experts when task is outside your expertise\n",
    "2. Always complete your part before transferring\n",
    "3. Provide clear context when transferring\n",
    "4. Never transfer back to an agent that just transferred to you\n",
    "\n",
    "RESPONSE STRUCTURE:\n",
    "1. Analyze the task first\n",
    "2. Use appropriate tools as needed\n",
    "3. Show work clearly and step by step\n",
    "4. Transfer only when necessary\n",
    "\n",
    "{specific_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Specific instructions for each agent type\n",
    "ADD_SPECIFIC = \"\"\"\n",
    "KEY RULES FOR ADDITION EXPERT:\n",
    "1. PATTERN RECOGNITION:\n",
    "\tWhen you see expressions like \"(a + b) * c\", this ALWAYS requires TWO tool calls:\n",
    "\t- First call: add tool for (a + b)\n",
    "\t- Second call: transfer to multiplication expert for the result * c\n",
    "\tYou MUST make BOTH calls in the SAME response.\n",
    "\n",
    "2. REQUIRED TOOL SEQUENCE:\n",
    "\tFor ANY expression involving multiplication after addition:\n",
    "\tStep 1: Use add tool to calculate the addition\n",
    "\tStep 2: IMMEDIATELY use tool_transfer_to_agent_mul in the SAME response\n",
    "\tDO NOT wait for another interaction to transfer.\n",
    "\n",
    "3. EXAMPLE SEQUENCES:\n",
    "\tFor \"(3 + 5) * 12\":\n",
    "\t- CORRECT (do this):\n",
    "\t\t1. add(a=3, b=5)\n",
    "\t\t2. tool_transfer_to_agent_mul(query=\"8 * 12\")\n",
    "\t- INCORRECT (don't do this):\n",
    "\t\t× Only calling add without transfer\n",
    "\t\t× Waiting for next message to transfer\n",
    "\n",
    "4. MANDATORY ACTIONS:\n",
    "\t- NEVER handle addition alone if multiplication follows\n",
    "\t- ALWAYS make both tool calls in one response\n",
    "\t- ALWAYS transfer after completing addition\n",
    "\"\"\"\n",
    "\n",
    "MUL_SPECIFIC = \"\"\"\n",
    "MULTIPLICATION EXPERTISE:\n",
    "1. You handle multiplication operations using the 'multiply' tool\n",
    "2. Transfer to addition expert if addition is needed first\n",
    "3. Complete multiplications when numbers are ready\n",
    "4. Present final results clearly\n",
    "\n",
    "Example workflow:\n",
    "For received \"8 * 12\":\n",
    "1. multiply(a=8, b=12)  # Calculate final result\n",
    "\"\"\"\n",
    "\t\n",
    "def agent_addition(\n",
    "\t\tstate: graphs.MessagesState,\n",
    ") -> graphs.Command[t.Literal[NODE.AGENT_MUL, graphs.END]]:\n",
    "\t\tmodel = llms.create_tooled_llm(inst.llm_main, [tool_transfer_to_agent_mul, add])\n",
    "\t\tprompt_system = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "\t\t\t\tdomain=\"mathematical addition\",\n",
    "\t\t\t\tprimary_task=\"handling addition operations and coordinating with multiplication expert\",\n",
    "\t\t\t\tspecific_instructions=ADD_SPECIFIC\n",
    "\t\t)\n",
    "\t\tmsgs = [msgs_lc.SystemMessage(prompt_system)] + state[\"messages\"]\n",
    "\t\tmsg_ai: msgs_lc.AIMessage = model.invoke(msgs)\n",
    "\n",
    "\t\t# Process tool calls and get updated messages\n",
    "\t\tupdated_msgs, should_transfer = process_tools(msg_ai, state[\"messages\"])\n",
    "\t\t\n",
    "\t\tif should_transfer:\n",
    "\t\t\t\treturn graphs.Command(\n",
    "\t\t\t\t\t\tgoto=NODE.AGENT_MUL,\n",
    "\t\t\t\t\t\tupdate={\"messages\": updated_msgs}\n",
    "\t\t\t\t)\n",
    "\t\t\n",
    "\t\treturn {\"messages\": updated_msgs}\n",
    "\n",
    "def agent_multiplication(\n",
    "\t\tstate: graphs.MessagesState,\n",
    ") -> graphs.Command[t.Literal[NODE.AGENT_ADD, graphs.END]]:\n",
    "\t\tmodel = llms.create_tooled_llm(inst.llm_main, [tool_transfer_to_agent_add, multiply])\n",
    "\t\tprompt_system = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "\t\t\t\tdomain=\"mathematical multiplication\",\n",
    "\t\t\t\tprimary_task=\"handling multiplication operations and coordinating with addition expert\",\n",
    "\t\t\t\tspecific_instructions=MUL_SPECIFIC\n",
    "\t\t)\n",
    "\t\tmsgs = [msgs_lc.SystemMessage(prompt_system)] + state[\"messages\"]\n",
    "\t\tmsg_ai: msgs_lc.AIMessage = model.invoke(msgs)\n",
    "\n",
    "\t\t# Process tool calls and get updated messages\n",
    "\t\tupdated_msgs, should_transfer = process_tools(msg_ai, state[\"messages\"])\n",
    "\t\t\n",
    "\t\tif should_transfer:\n",
    "\t\t\t\treturn graphs.Command(\n",
    "\t\t\t\t\t\tgoto=NODE.AGENT_ADD,\n",
    "\t\t\t\t\t\tupdate={\"messages\": updated_msgs}\n",
    "\t\t\t\t)\n",
    "\t\t\n",
    "\t\treturn {\"messages\": updated_msgs}\n",
    "\n",
    "builder = graphs.StateGraph(graphs.MessagesState)\n",
    "\n",
    "builder.add_node(NODE.AGENT_ADD, agent_addition)\n",
    "builder.add_node(NODE.AGENT_MUL, agent_multiplication)\n",
    "\n",
    "builder.add_edge(graphs.START, NODE.AGENT_ADD)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream(graph_stream):\n",
    "    \"\"\"\n",
    "    Pretty prints the stream from a LangGraph, showing only new messages at each turn.\n",
    "    \n",
    "    Args:\n",
    "        graph_stream: Iterator from graph.stream()\n",
    "    \"\"\"\n",
    "    seen_message_ids = set()\n",
    "    \n",
    "    def print_new_messages(messages):\n",
    "        \"\"\"Helper function to print only unseen messages.\"\"\"\n",
    "        for msg in messages:\n",
    "            # Skip if we've seen this message before\n",
    "            if hasattr(msg, 'id') and msg.id in seen_message_ids:\n",
    "                continue\n",
    "                \n",
    "            # Add to seen messages if it has an ID\n",
    "            if hasattr(msg, 'id'):\n",
    "                seen_message_ids.add(msg.id)\n",
    "            \n",
    "            # Print the message content based on type\n",
    "            if hasattr(msg, 'content') and msg.content:\n",
    "                print(f\"Message: {msg.content}\")\n",
    "            \n",
    "            # Print tool calls if present\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    tool_name = tool_call.get('name', 'unknown_tool')\n",
    "                    tool_args = tool_call.get('args', {})\n",
    "                    print(f\"Tool Call: {tool_name}\")\n",
    "                    print(f\"Arguments: {tool_args}\")\n",
    "            \n",
    "            # Print tool message results\n",
    "            if hasattr(msg, 'tool_call_id'):\n",
    "                print(f\"Tool Result: {msg.content}\")\n",
    "            \n",
    "            if hasattr(msg, 'content') or hasattr(msg, 'tool_calls') or hasattr(msg, 'tool_call_id'):\n",
    "                print(\"-\" * 50)\n",
    "    \n",
    "    for chunk in graph_stream:\n",
    "        if isinstance(chunk, tuple):\n",
    "            # Handle subgraph updates\n",
    "            ns, update = chunk\n",
    "            if not ns:\n",
    "                continue\n",
    "            print(f\"\\n=== Update from subgraph {ns[-1].split(':')[0]} ===\")\n",
    "            if 'messages' in update:\n",
    "                print_new_messages(update['messages'])\n",
    "        else:\n",
    "            # Handle regular node updates\n",
    "            for node_name, node_update in chunk.items():\n",
    "                print(f\"\\n=== Update from {node_name} ===\")\n",
    "                if 'messages' in node_update:\n",
    "                    print_new_messages(node_update['messages'])\n",
    "\n",
    "content = \"what's (3 + 5) * 12\"\n",
    "# content = \"what's 3*3 + 1\"\n",
    "\n",
    "pretty_print_stream(\n",
    "\t\tgraph.stream({\"messages\": [msgs_lc.HumanMessage(content=content)]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for chunk in graph.stream(\n",
    "\t\t{\"messages\": [(\"user\", \"what's ((3 + 5) * 12) + 123\")]},\n",
    "\t\t# stream_mode=\"values\"\n",
    "):\n",
    "\t\t# rp_print(chunk)\n",
    "\t\tresult.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rp_print(result[-1].keys())\n",
    "\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_messages(update):\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        print(f\"Update from node {node_name}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for m in convert_to_messages(node_update[\"messages\"]):\n",
    "            m.pretty_print()\n",
    "        print(\"\\n\")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]},\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build a multi-agent network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing_extensions import Literal\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool\n",
    "def get_travel_recommendations():\n",
    "    \"\"\"Get recommendation for travel destinations\"\"\"\n",
    "    return random.choice([\"aruba\", \"turks and caicos\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\n",
    "    \"\"\"Get hotel recommendations for a given destination.\"\"\"\n",
    "    return {\n",
    "        \"aruba\": [\n",
    "            \"The Ritz-Carlton, Aruba (Palm Beach)\"\n",
    "            \"Bucuti & Tara Beach Resort (Eagle Beach)\"\n",
    "        ],\n",
    "        \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\n",
    "    }[location]\n",
    "    \n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "def make_handoff_tool(*, agent_name: str):\n",
    "    \"\"\"Create a tool that can return handoff via a Command\"\"\"\n",
    "    tool_name = f\"transfer_to_{agent_name}\"\n",
    "\n",
    "    @tool(tool_name)\n",
    "    def handoff_to_agent(\n",
    "        state: Annotated[dict, InjectedState],\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        \"\"\"Ask another agent for help.\"\"\"\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": tool_name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            # navigate to another agent node in the PARENT graph\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            # This is the state update that the agent `agent_name` will see when it is invoked.\n",
    "            # We're passing agent's FULL internal message history AND adding a tool message to make sure\n",
    "            # the resulting chat history is valid.\n",
    "            update={\"messages\": state[\"messages\"] + [tool_message]},\n",
    "        )\n",
    "\n",
    "    return handoff_to_agent\n",
    "  \n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "model = inst.llm_main\n",
    "\n",
    "# Define travel advisor ReAct agent\n",
    "travel_advisor_tools = [\n",
    "    get_travel_recommendations,\n",
    "    make_handoff_tool(agent_name=\"hotel_advisor\"),\n",
    "]\n",
    "travel_advisor = create_react_agent(\n",
    "    model,\n",
    "    travel_advisor_tools,\n",
    "    state_modifier=(\n",
    "        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n",
    "        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n",
    "        \"You MUST include human-readable response before transferring to another agent.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def call_travel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"hotel_advisor\", \"__end__\"]]:\n",
    "    # You can also add additional logic like changing the input to the agent / output from the agent, etc.\n",
    "    # NOTE: we're invoking the ReAct agent with the full history of messages in the state\n",
    "    return travel_advisor.invoke(state)\n",
    "\n",
    "\n",
    "# Define hotel advisor ReAct agent\n",
    "hotel_advisor_tools = [\n",
    "    get_hotel_recommendations,\n",
    "    make_handoff_tool(agent_name=\"travel_advisor\"),\n",
    "]\n",
    "hotel_advisor = create_react_agent(\n",
    "    model,\n",
    "    hotel_advisor_tools,\n",
    "    state_modifier=(\n",
    "        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n",
    "        \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n",
    "        \"You MUST include human-readable response before transferring to another agent.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def call_hotel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"__end__\"]]:\n",
    "    return hotel_advisor.invoke(state)\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"travel_advisor\", call_travel_advisor)\n",
    "builder.add_node(\"hotel_advisor\", call_hotel_advisor)\n",
    "# we'll always start with a general travel advisor\n",
    "builder.add_edge(START, \"travel_advisor\")\n",
    "\n",
    "graph = builder.compile()\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True,\n",
    "):\n",
    "    rp_print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "model = inst.llm_main\n",
    "\n",
    "\n",
    "# Define a helper for each of the agent nodes to call\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_travel_advisor():\n",
    "    \"\"\"Ask travel advisor for help.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to another agent\n",
    "    # (See the paragraph above)\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_hotel_advisor():\n",
    "    \"\"\"Ask hotel advisor for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def travel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n",
    "        \"If you need hotel recommendations, ask 'hotel_advisor' for help. You must not ask back user, if any information not clear, just suggest and respond.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_hotel_advisor]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"hotel_advisor\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def hotel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n",
    "        \"If you need help picking travel destinations, ask 'travel_advisor' for help. You must not ask back user, if any information not clear, just suggest and respond.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_travel_advisor]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"travel_advisor\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"travel_advisor\", travel_advisor)\n",
    "builder.add_node(\"hotel_advisor\", hotel_advisor)\n",
    "# we'll always start with a general travel advisor\n",
    "builder.add_edge(START, \"travel_advisor\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_messages(update):\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        print(f\"Update from node {node_name}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for m in convert_to_messages(node_update[\"messages\"]):\n",
    "            m.pretty_print()\n",
    "        print(\"\\n\")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\")]}\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List, Dict, Any, Optional, TypeVar, Generic\n",
    "from typing_extensions import Literal, TypedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    AIMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Initialize rich console\n",
    "console = Console()\n",
    "\n",
    "# Use the provided model\n",
    "model = inst.llm_main\n",
    "\n",
    "# ============= Core Types =============\n",
    "class AgentRole(str, Enum):\n",
    "    WORKER = \"worker\"\n",
    "    SUPERVISOR = \"supervisor\"\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    agent_id: str\n",
    "    role: AgentRole\n",
    "    system_prompt: str\n",
    "    allowed_tools: List[Any]\n",
    "    allowed_handoffs: List[str]\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"Base state class with context tracking\"\"\"\n",
    "    context: Dict[str, Any]\n",
    "\n",
    "T = TypeVar('T', bound=State)\n",
    "\n",
    "# ============= Calculator Tools =============\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    result = a + b\n",
    "    console.print(Panel(\n",
    "        f\"add({a}, {b}) = {result}\",\n",
    "        title=\"[#4B9EF0]Tool Execution[/#4B9EF0]\",\n",
    "        border_style=\"#4B9EF0\"\n",
    "    ))\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    result = a * b\n",
    "    console.print(Panel(\n",
    "        f\"multiply({a}, {b}) = {result}\",\n",
    "        title=\"[#4B9EF0]Tool Execution[/#4B9EF0]\",\n",
    "        border_style=\"#4B9EF0\"\n",
    "    ))\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Transfer to multiplication expert.\"\"\"\n",
    "    console.print(Panel(\n",
    "        \"Transferring to multiplication expert\",\n",
    "        title=\"[#F0A732]Transfer[/#F0A732]\",\n",
    "        border_style=\"#F0A732\"\n",
    "    ))\n",
    "    return\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Transfer to addition expert.\"\"\"\n",
    "    console.print(Panel(\n",
    "        \"Transferring to addition expert\",\n",
    "        title=\"[#F0A732]Transfer[/#F0A732]\",\n",
    "        border_style=\"#F0A732\"\n",
    "    ))\n",
    "    return\n",
    "\n",
    "# ============= Base Classes =============\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents\"\"\"\n",
    "    def __init__(self, config: AgentConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def process_tools(self, response: AIMessage, state: State) -> tuple[State, Optional[str]]:\n",
    "        \"\"\"Process tool usage and determine next agent\"\"\"\n",
    "        updated_state = state.copy()\n",
    "        updated_state[\"messages\"] = state[\"messages\"] + [response]\n",
    "        next_agent = None\n",
    "\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call[\"name\"]\n",
    "                \n",
    "                # Handle transfers\n",
    "                if tool_name.startswith(\"transfer_to_\"):\n",
    "                    target_agent = tool_name.replace(\"transfer_to_\", \"\")\n",
    "                    if target_agent in self.config.allowed_handoffs:\n",
    "                        next_agent = target_agent\n",
    "                        return updated_state, next_agent\n",
    "\n",
    "                # Handle regular tools\n",
    "                for tool in self.config.allowed_tools:\n",
    "                    if tool.name == tool_name:\n",
    "                        try:\n",
    "                            result = tool.invoke(tool_call)\n",
    "                            tool_msg = ToolMessage(\n",
    "                                content=str(result),\n",
    "                                tool_call_id=tool_call[\"id\"],\n",
    "                                name=tool_name\n",
    "                            )\n",
    "                            updated_state[\"messages\"].append(tool_msg)\n",
    "                        except Exception as e:\n",
    "                            tool_msg = ToolMessage(\n",
    "                                content=f\"Error: {str(e)}\",\n",
    "                                tool_call_id=tool_call[\"id\"]\n",
    "                            )\n",
    "                            updated_state[\"messages\"].append(tool_msg)\n",
    "                            console.print(f\"[#FF6B6B]Tool Error:[/#FF6B6B] {str(e)}\")\n",
    "\n",
    "        return updated_state, next_agent\n",
    "\n",
    "    def __call__(self, state: State) -> Command[Dict[str, Any]]:\n",
    "        \"\"\"Process state and determine next action\"\"\"\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(content=self.config.system_prompt)\n",
    "            ] + state[\"messages\"]\n",
    "\n",
    "            response = model.bind_tools(\n",
    "                self.config.allowed_tools\n",
    "            ).invoke(messages)\n",
    "            \n",
    "            console.print(Panel(\n",
    "                response.content,\n",
    "                title=f\"[#4CAF50]{self.config.agent_id}[/#4CAF50]\",\n",
    "                border_style=\"#4CAF50\"\n",
    "            ))\n",
    "\n",
    "            updated_state, next_agent = self.process_tools(response, state)\n",
    "            \n",
    "            if next_agent:\n",
    "                return Command(goto=next_agent, update=updated_state)\n",
    "            \n",
    "            if \"FINAL ANSWER:\" in response.content:\n",
    "                return Command(goto=\"__end__\", update=updated_state)\n",
    "            \n",
    "            return Command(goto=\"supervisor\", update=updated_state)\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"[#FF6B6B]Agent Error:[/#FF6B6B] {str(e)}\")\n",
    "            error_state = state.copy()\n",
    "            error_state[\"messages\"] = state[\"messages\"] + [\n",
    "                AIMessage(content=f\"Error: {str(e)}\")\n",
    "            ]\n",
    "            return Command(goto=\"__end__\", update=error_state)\n",
    "\n",
    "class BaseTeam(Generic[T]):\n",
    "    \"\"\"Base class for all agent teams\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        team_id: str,\n",
    "        agents: List[BaseAgent],\n",
    "        supervisor_prompt: str,\n",
    "        state_class: type[T] = State\n",
    "    ):\n",
    "        self.team_id = team_id\n",
    "        self.agents = {agent.config.agent_id: agent for agent in agents}\n",
    "        self.supervisor_prompt = supervisor_prompt\n",
    "        self.state_class = state_class\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the agent interaction graph\"\"\"\n",
    "        builder = StateGraph(self.state_class)\n",
    "        for agent_id, agent in self.agents.items():\n",
    "            builder.add_node(agent_id, agent)\n",
    "        builder.add_node(\"supervisor\", self._create_supervisor())\n",
    "        builder.add_edge(START, \"supervisor\")\n",
    "        return builder.compile()\n",
    "\n",
    "    def _create_supervisor(self):\n",
    "        \"\"\"Create supervisor node\"\"\"\n",
    "        def supervisor_node(state: T) -> Command[Dict[str, Any]]:\n",
    "            system_msg = SystemMessage(content=self.supervisor_prompt)\n",
    "            messages = [system_msg] + state[\"messages\"]\n",
    "            \n",
    "            response = model.invoke(messages)\n",
    "            console.print(Panel(\n",
    "                response.content,\n",
    "                title=\"[#E040FB]Supervisor Decision[/#E040FB]\",\n",
    "                border_style=\"#E040FB\"\n",
    "            ))\n",
    "            \n",
    "            next_agent = self._determine_next_agent(response)\n",
    "            if next_agent == \"__end__\":\n",
    "                return Command(goto=\"__end__\", update={\n",
    "                    \"messages\": state[\"messages\"] + [response]\n",
    "                })\n",
    "                \n",
    "            return Command(goto=next_agent, update={\n",
    "                \"messages\": state[\"messages\"] + [response]\n",
    "            })\n",
    "        \n",
    "        return supervisor_node\n",
    "\n",
    "    def _determine_next_agent(self, response: AIMessage) -> str:\n",
    "        \"\"\"Determine next agent - override in subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _determine_next_agent\")\n",
    "\n",
    "    def process(self, input_data: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process input through the team\"\"\"\n",
    "        console.print(Panel(\n",
    "            input_data,\n",
    "            title=f\"[#00BCD4]{self.team_id} Input[/#00BCD4]\",\n",
    "            border_style=\"#00BCD4\"\n",
    "        ))\n",
    "        state = self.state_class(\n",
    "            messages=[HumanMessage(content=input_data)],\n",
    "            context={}\n",
    "        )\n",
    "        return self.graph.invoke(state)\n",
    "\n",
    "# ============= Calculator Implementation =============\n",
    "class CalculatorTeam(BaseTeam[State]):\n",
    "    \"\"\"Team of calculator agents\"\"\"\n",
    "    def _determine_next_agent(self, response: AIMessage) -> str:\n",
    "        \"\"\"Route based on operation needed\"\"\"\n",
    "        content = response.content.lower()\n",
    "        \n",
    "        if \"FINAL ANSWER:\" in response.content:\n",
    "            return \"__end__\"\n",
    "            \n",
    "        if any(phrase in content for phrase in [\n",
    "            \"need to multiply\",\n",
    "            \"multiplication needed\",\n",
    "            \"multiply these\",\n",
    "            \"*\", \"times\"\n",
    "        ]):\n",
    "            return \"multiplication_expert\"\n",
    "            \n",
    "        if any(phrase in content for phrase in [\n",
    "            \"need to add\",\n",
    "            \"addition needed\",\n",
    "            \"add these\",\n",
    "            \"+\"\n",
    "        ]):\n",
    "            return \"addition_expert\"\n",
    "            \n",
    "        return \"__end__\"\n",
    "\n",
    "def create_calculator_team() -> CalculatorTeam:\n",
    "    \"\"\"Create calculator team instance\"\"\"\n",
    "    add_agent = BaseAgent(\n",
    "        config=AgentConfig(\n",
    "            agent_id=\"addition_expert\",\n",
    "            role=AgentRole.WORKER,\n",
    "            system_prompt=(\n",
    "                \"You are an addition expert. Your job is to:\\n\"\n",
    "                \"1. Look for numbers that need to be added\\n\"\n",
    "                \"2. Use the add tool to perform addition\\n\"\n",
    "                \"3. If multiplication is needed, transfer to multiplication_expert\\n\"\n",
    "                \"4. Show all work and intermediate steps\\n\"\n",
    "                \"5. When the final result is ready, say 'FINAL ANSWER: [result]'\\n\\n\"\n",
    "                \"Remember: Only handle addition. Always transfer multiplication tasks.\"\n",
    "            ),\n",
    "            allowed_tools=[add, transfer_to_multiplication_expert],\n",
    "            allowed_handoffs=[\"multiplication_expert\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mult_agent = BaseAgent(\n",
    "        config=AgentConfig(\n",
    "            agent_id=\"multiplication_expert\",\n",
    "            role=AgentRole.WORKER,\n",
    "            system_prompt=(\n",
    "                \"You are a multiplication expert. Your job is to:\\n\"\n",
    "                \"1. Look for numbers that need to be multiplied\\n\"\n",
    "                \"2. Use the multiply tool to perform multiplication\\n\"\n",
    "                \"3. If addition is needed, transfer to addition_expert\\n\"\n",
    "                \"4. Show all work and intermediate steps\\n\"\n",
    "                \"5. When the final result is ready, say 'FINAL ANSWER: [result]'\\n\\n\"\n",
    "                \"Remember: Only handle multiplication. Always transfer addition tasks.\"\n",
    "            ),\n",
    "            allowed_tools=[multiply, transfer_to_addition_expert],\n",
    "            allowed_handoffs=[\"addition_expert\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return CalculatorTeam(\n",
    "        team_id=\"calculator\",\n",
    "        agents=[add_agent, mult_agent],\n",
    "        supervisor_prompt=(\n",
    "            \"You are a math supervisor. Your job is to:\\n\"\n",
    "            \"1. Analyze the expression and identify needed operations\\n\"\n",
    "            \"2. Route addition tasks to addition_expert\\n\"\n",
    "            \"3. Route multiplication tasks to multiplication_expert\\n\"\n",
    "            \"4. For complex expressions, follow order of operations\\n\"\n",
    "            \"5. Only identify the next operation - do NOT calculate\\n\\n\"\n",
    "            \"Remember: Your job is to ROUTE tasks, not solve them.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    calculator = create_calculator_team()\n",
    "    expression = \"What is (17 + 23) * (14 + 16)?\"\n",
    "    \n",
    "    try:\n",
    "        result = calculator.process(expression)\n",
    "        final_msg = result[\"messages\"][-1].content\n",
    "        if \"FINAL ANSWER:\" in final_msg:\n",
    "            answer = final_msg.split(\"FINAL ANSWER:\")[-1].strip()\n",
    "            console.print(Panel(\n",
    "                f\"[#4CAF50]{answer}[/#4CAF50]\",\n",
    "                title=\"[#4CAF50]Final Answer[/#4CAF50]\",\n",
    "                border_style=\"#4CAF50\"\n",
    "            ))\n",
    "        else:\n",
    "            console.print(\"[#FF6B6B]No final answer found in response[/#FF6B6B]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        console.print(f\"[#FF6B6B]Error:[/#FF6B6B] {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add multi-turn conversation in a multi-agent application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use Pydantic model as state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to define input/output schema for your graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to pass private state between nodes inside the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run graph asynchronously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to visualize your graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add runtime configuration to your graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add node retries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to force function calling agent to structure output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to pass custom LangSmith run ID for graph runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to return state before hitting recursion limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuilt ReAct Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create a ReAct agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add memory to a ReAct agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add a custom system prompt to a ReAct agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add human-in-the-loop processes to a ReAct agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create prebuilt ReAct agent from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add semantic search for long-term memory to a ReAct agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set up app for deployment (requirements.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set up app for deployment (pyproject.toml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set up app for deployment (JavaScript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add semantic search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to customize Dockerfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to test locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to rebuild graph at runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use LangGraph Platform to deploy CrewAI, AutoGen, and other frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to deploy to LangGraph cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to deploy to a self-hosted environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interact with the deployment using RemoteGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication & Access Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add custom authentication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to update the security schema of your OpenAPI spec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to configure agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to version assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to copy threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to check status of your threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run an agent in the background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run multiple agents in the same thread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create cron jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create stateless runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream in debug mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stream multiple modes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add a breakpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to wait for user input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to edit graph state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to replay and branch from prior states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to review tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double-texting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use the interrupt option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use the rollback option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use the reject option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use the enqueue option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webhooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to integrate webhooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cron Jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create cron jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to connect to a LangGraph Cloud deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to connect to a local dev server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to connect to a local deployment (Docker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to test your graph in LangGraph Studio (MacOS only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interact with threads in LangGraph Studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to add nodes as dataset examples in LangGraph Studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPH_RECURSION_LIMIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INVALID_CONCURRENT_GRAPH_UPDATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INVALID_GRAPH_NODE_RETURN_VALUE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MULTIPLE_SUBGRAPHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INVALID_CHAT_HISTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://langchain-ai.github.io/langgraph\n",
    "- https://langchain-ai.github.io/langgraph/concepts/\n",
    "\n",
    "- [Conceptual Guide](https://langchain-ai.github.io/langgraph/concepts/)\n",
    "  - LangGraph\n",
    "    - High Level\n",
    "      - [Why LangGraph?](https://langchain-ai.github.io/langgraph/concepts/high_level/)\n",
    "    - Concepts\n",
    "      - [LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low_level/) 1️⃣\n",
    "      - [Common Agentic Patterns](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) 2️⃣\n",
    "      - [Multi-Agent Systems](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#multi-agent-architectures) 1️⃣\n",
    "      - Breakpoints\n",
    "      - Human-in-the-Loop\n",
    "      - Time Travel\n",
    "      - Persistence\n",
    "      - Memory\n",
    "      - Streaming\n",
    "      - FAQ\n",
    "  - LangGraph Platform\n",
    "    - High Level\n",
    "      - Why LangGraph Platform?\n",
    "      - Deployment Options\n",
    "      - Plans\n",
    "      - Template Applications\n",
    "    - Components\n",
    "      - LangGraph Server\n",
    "      - LangGraph Studio\n",
    "      - LangGraph CLI\n",
    "      - Python/JS SDK\n",
    "      - Remote Graph\n",
    "    - LangGraph Server\n",
    "      - Application Structure\n",
    "      - Assistants\n",
    "      - Web-hooks\n",
    "      - Cron Jobs\n",
    "      - Double Texting\n",
    "      - Authentication & Access Control\n",
    "    - Deployment Options\n",
    "      - Self-Hosted Lite\n",
    "      - Cloud SaaS\n",
    "      - Bring Your Own Cloud\n",
    "      - Self-Hosted Enterprise\n",
    "\n",
    "- Tutorials\n",
    "  - Quick Start\n",
    "    - [🚀 LangGraph Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/) ✅\n",
    "  - Chatbots\n",
    "  - RAG\n",
    "  - Agent Architectures\n",
    "    - Multi-Agent Systems¶\n",
    "      - [Network](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/)\n",
    "      - Supervisor\n",
    "      - Hierarchical Teams\n",
    "    - Planning Agents¶\n",
    "      - Plan-and-Execute\n",
    "      - Reasoning without Observation\n",
    "      - LLMCompiler\n",
    "    - Reflection & Critique¶\n",
    "      - Basic Reflection\n",
    "      - Reflexion\n",
    "      - Tree of Thoughts\n",
    "      - Language Agent Tree Search\n",
    "      - Self-Discover Agent\n",
    "  - Evaluation & Analysis\n",
    "  - Experimental\n",
    "  - LangGraph Platform\n",
    "\n",
    "- How-to\n",
    "  - LangGraph\n",
    "    - Controllability\n",
    "      - How to create branches for parallel execution\n",
    "      - How to create map-reduce branches for parallel execution\n",
    "      - How to control graph recursion limit\n",
    "      - How to combine control flow and state updates with Command\n",
    "    - Persistence\n",
    "      - How to add thread-level persistence to your graph\n",
    "      - How to add thread-level persistence to subgraphs\n",
    "      - How to add cross-thread persistence to your graph\n",
    "      - How to use Postgres checkpointer for persistence\n",
    "      - How to use MongoDB checkpointer for persistence\n",
    "      - How to create a custom checkpointer using Redis\n",
    "    - Memory\n",
    "      - How to manage conversation history\n",
    "      - How to delete messages\n",
    "      - How to add summary conversation memory\n",
    "      - How to add long-term memory (cross-thread)\n",
    "      - How to use semantic search for long-term memory\n",
    "    - Human-in-the-loop\n",
    "      - How to wait for user input\n",
    "      - How to review tool calls\n",
    "      - How to add static breakpoints\n",
    "      - How to edit graph state\n",
    "      - How to add dynamic breakpoints with NodeInterrupt\n",
    "    - Time Travel\n",
    "      - How to view and update past graph state\n",
    "    - Streaming\n",
    "    \t- [How to stream](https://langchain-ai.github.io/langgraph/how-tos/streaming/#values) 1️⃣\n",
    "\t\t\t- [How to stream LLM tokens](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/) 1️⃣\n",
    "\t\t\t- [How to stream LLM tokens from specific nodes](https://langchain-ai.github.io/langgraph/how-tos/streaming-specific-nodes/) 1️⃣\n",
    "\t\t\t- [How to stream data from within a tool](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/) 1️⃣\n",
    "\t\t\t- [How to stream from subgraphs](https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/) 1️⃣\n",
    "\t\t\t- How to disable streaming for models that don't support it\n",
    "    - Tool calling\n",
    "      - How to call tools using ToolNode\n",
    "      - How to handle tool calling errors\n",
    "      - How to pass runtime values to tools\n",
    "      - How to pass config to tools\n",
    "      - How to update graph state from tools\n",
    "      - How to handle large numbers of tools\n",
    "    - Subgraphs\n",
    "      - [How to use subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/) 2️⃣\n",
    "      - [How to view and update state in subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/) 🚧\n",
    "      - How to transform inputs and outputs of a subgraph\n",
    "    - Multi-agent\n",
    "      - [How to implement handoffs between agents](https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/) 2️⃣\n",
    "      - [How to build a multi-agent network](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/) 2️⃣\n",
    "      - How to add multi-turn conversation in a multi-agent application\n",
    "    - State Management\n",
    "      - How to use Pydantic model as state\n",
    "      - How to define input/output schema for your graph\n",
    "      - How to pass private state between nodes inside the graph\n",
    "    - Other\n",
    "      - How to run graph asynchronously\n",
    "      - How to visualize your graph\n",
    "      - How to add runtime configuration to your graph\n",
    "      - How to add node retries\n",
    "      - How to force function calling agent to structure output\n",
    "      - How to pass custom LangSmith run ID for graph runs\n",
    "      - How to return state before hitting recursion limit\n",
    "      - How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n",
    "    - Prebuilt ReAct Agent\n",
    "      - How to create a ReAct agent\n",
    "      - How to add memory to a ReAct agent\n",
    "      - How to add a custom system prompt to a ReAct agent\n",
    "      - How to add human-in-the-loop processes to a ReAct agent\n",
    "      - How to create prebuilt ReAct agent from scratch\n",
    "      - How to add semantic search for long-term memory to a ReAct agent\n",
    "\n",
    "  - LangGraph Platform\n",
    "    - Application Structure\n",
    "      - How to set up app for deployment (requirements.txt)\n",
    "      - How to set up app for deployment (pyproject.toml)\n",
    "      - How to set up app for deployment (JavaScript)\n",
    "      - How to add semantic search\n",
    "      - How to customize Dockerfile\n",
    "      - How to test locally\n",
    "      - How to rebuild graph at runtime\n",
    "      - How to use LangGraph Platform to deploy CrewAI, AutoGen, and other frameworks\n",
    "    - Deployment\n",
    "      - How to deploy to LangGraph cloud\n",
    "      - How to deploy to a self-hosted environment\n",
    "      - How to interact with the deployment using RemoteGraph\n",
    "    - Authentication & Access Control\n",
    "      - How to add custom authentication\n",
    "      - How to update the security schema of your OpenAPI spec\n",
    "    - Assistants\n",
    "      - How to configure agents\n",
    "      - How to version assistants\n",
    "    - Threads\n",
    "      - How to copy threads\n",
    "      - How to check status of your threads\n",
    "    - Runs\n",
    "      - How to run an agent in the background\n",
    "      - How to run multiple agents in the same thread\n",
    "      - How to create cron jobs\n",
    "      - How to create stateless runs\n",
    "    - Streaming\n",
    "      - How to stream values\n",
    "      - How to stream updates\n",
    "      - How to stream messages\n",
    "      - How to stream events\n",
    "      - How to stream in debug mode\n",
    "      - How to stream multiple modes\n",
    "    - Human-in-the-loop\n",
    "      - How to add a breakpoint\n",
    "      - How to wait for user input\n",
    "      - How to edit graph state\n",
    "      - How to replay and branch from prior states\n",
    "      - How to review tool cal\n",
    "    - Double-texting\n",
    "      - How to use the interrupt option\n",
    "      - How to use the rollback option\n",
    "      - How to use the reject option\n",
    "      - How to use the enqueue option\n",
    "    - Webhooks\n",
    "      - How to integrate webhooks\n",
    "    - Cron Jobs\n",
    "      - How to create cron jobs\n",
    "    - LangGraph Studio\n",
    "      - How to connect to a LangGraph Cloud deployment\n",
    "      - How to connect to a local dev server\n",
    "      - How to connect to a local deployment (Docker)\n",
    "      - How to test your graph in LangGraph Studio (MacOS only)\n",
    "      - How to interact with threads in LangGraph Studio\n",
    "      - How to add nodes as dataset examples in LangGraph Studio\n",
    "    - Troubleshooting\n",
    "      - GRAPH_RECURSION_LIMIT\n",
    "      - INVALID_CONCURRENT_GRAPH[](https://langchain-ai.github.io/langgraph/how-tos/streaming/#values)\n",
    "      - INVALID_GRAPH_NODE_RETURN_VALUE\n",
    "      - MULTIPLE_SUBGRAPHS\n",
    "      - INVALID_CHAT_HISTORY\n",
    "  \n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

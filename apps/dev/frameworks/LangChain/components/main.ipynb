{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-04 14:47:57.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpackages\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mapps directory: /Users/thung/Documents/Me/Coding/Embedded-AI/apps\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:57.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpackages\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mToolkit path: /Users/thung/Documents/Me/Coding/Embedded-AI/apps/toolkit\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:57.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpackages\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mEnvironment files loaded:\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:57.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpackages\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m  - /Users/thung/Documents/Me/Coding/Embedded-AI/apps/ports.env\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:57.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpackages\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m  - /Users/thung/Documents/Me/Coding/Embedded-AI/apps/.env\u001b[0m\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-02-04 14:47:58.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.db.mongodb\u001b[0m:\u001b[36mconnect\u001b[0m:\u001b[36m391\u001b[0m - \u001b[1mSuccessfully connected to MongoDB database: \u001b[36mapp\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:58.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.db.mongodb\u001b[0m:\u001b[36mget_collection\u001b[0m:\u001b[36m402\u001b[0m - \u001b[1mCreating manager for collection: \u001b[32mvehicle\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:59.289\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mcontext.settings.main\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[33m\u001b[1mRunning in DEVELOPMENT mode\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:59.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.models.llms\u001b[0m:\u001b[36mcreate_llm\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mðŸ”¹ \u001b[38;5;208mLLM\u001b[0m \u001b[35mOpenAI\u001b[0m \u001b[32mgpt-4o-mini\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:59.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.models.llms\u001b[0m:\u001b[36mcreate_llm\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mðŸ”¹ \u001b[38;5;208mLLM\u001b[0m \u001b[35mGoogle\u001b[0m \u001b[32mgemini-2.0-flash-exp\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:59.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.models.embeddings\u001b[0m:\u001b[36mcreate_embedding\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mðŸ”¹ \u001b[38;5;208mEmbedding\u001b[0m \u001b[35mOpenAI\u001b[0m \u001b[32mtext-embedding-3-small\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:47:59.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.data.persistence.vector_stores\u001b[0m:\u001b[36mcreate_vector_store\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mðŸ”¹ \u001b[38;5;208mVectorStore\u001b[0m \u001b[35mInMemory\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:48:01.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.data.persistence.vector_stores\u001b[0m:\u001b[36mcreate_vector_store\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mðŸ”Œ \u001b[38;5;208mVectorStore\u001b[0m \u001b[35mQdrant\u001b[0m Collection \u001b[32mdev\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:48:03.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.data.persistence.vector_stores\u001b[0m:\u001b[36mcreate_vector_store\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mðŸ”Œ \u001b[38;5;208mVectorStore\u001b[0m \u001b[35mQdrant\u001b[0m Collection \u001b[32mvehicle\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-02-04 14:48:04.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolkit.llm.langchain.data.persistence.vector_stores\u001b[0m:\u001b[36mcreate_vector_store\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mðŸ”Œ \u001b[38;5;208mVectorStore\u001b[0m \u001b[35mQdrant\u001b[0m Collection \u001b[32muser_query_category\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import packages\n",
    "\n",
    "from context.utils import typer as t\n",
    "from context.infra.clients import logger\n",
    "\n",
    "from toolkit.utils import utils\n",
    "from toolkit.utils.utils import rp_print\n",
    "from toolkit.utils.llm import main as utils_llm\n",
    "\n",
    "import context.instances as inst\n",
    "import context.consts as const\n",
    "import context.settings.main as settings_main\n",
    "\n",
    "from toolkit.llm.langchain.core import integration, utils as utils_lc\n",
    "from toolkit.llm.langchain.data.indexing import (\n",
    "    documents, document_loaders, text_splitters,\n",
    ")\n",
    "from toolkit.llm.langchain.data.persistence import retrievers\n",
    "from toolkit.llm.langchain.execution import (\n",
    "    runnables, graphs, tools, agents, tools as tools_lc\n",
    ")\n",
    "from toolkit.llm.langchain.models import (\n",
    "    prompts as prompts_lc, llms, messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = inst.vector_store_in_memory\n",
    "vector_stores = inst.vector_stores_qdrant\n",
    "\n",
    "COLLS = settings_main.VEC_STR_COLLS\n",
    "\n",
    "llm = inst.llm_main\n",
    "embedding = inst.embedding_main\n",
    "\n",
    "prompts = prompts_lc.prompts\n",
    "# prompt_system_rag = prompt_system_rag.replace(\"{context}\", docs_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models and prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(t.BaseModel):\n",
    "\t\"\"\"Information about a person.\"\"\"\n",
    "\n",
    "\t# ^ Doc-string for the entity Person.\n",
    "\t# This doc-string is sent to the LLM as the description of the schema Person,\n",
    "\t# and it can help to improve extraction results.\n",
    "\n",
    "\t# Note that:\n",
    "\t# 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "\t# 2. Each field has a `description` -- this description is used by the LLM.\n",
    "\t# Having a good description can help improve extraction results.\n",
    "\tname: t.Optional[str] = t.Field(\n",
    "\tdefault=None, description=\"The name of the person\"\n",
    "\t)\n",
    "\thair_color: t.Optional[str] = t.Field(\n",
    "\t\tdefault=None, description=\"The color of the person's hair if known\"\n",
    "\t)\n",
    "\theight_in_meters: t.Optional[str] = t.Field(\n",
    "\t\tdefault=None, description=\"Height measured in meters\"\n",
    "\t)\n",
    "\n",
    "class Group(t.BaseModel):\n",
    "  # Creates a model so that we can extract multiple entities.\n",
    "\tpeople: t.List[Person]\n",
    "\n",
    "prompt_tpl = prompts_lc.ChatPromptTemplate.from_messages([\n",
    "\t(\n",
    "\t\t\"system\",\n",
    "\t\t\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\",\n",
    "\t),\n",
    "\t(\n",
    "   \t\"human\", \"{user_input}\"\n",
    "  ),\n",
    "])\n",
    "\n",
    "llm_structured = inst.llm_main.with_structured_output(schema=Group)\n",
    "\n",
    "user_input = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_tpl.invoke({\"user_input\": user_input})\n",
    "response = llm_structured.invoke(prompt)\n",
    "\n",
    "rp_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answering with SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answering with Graph Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to guides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install LangChain packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use LangChain with different Pydantic versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### return structured data from a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use a model to call tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug your LLM apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke runnables in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add default invocation args to runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn any function into a runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass through inputs from one chain step to the next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure runnable behavior at runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add message history (memory) to a chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### route between sub-chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dynamic (self-constructing) chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add fallbacks to a runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass runtime secrets to a runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use few shot examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use few shot examples in chat models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partially format prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compose prompts together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use example selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select examples by length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select examples by semantic similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt_tpl = prompts_lc.PromptTemplate(\n",
    "\tinput_variables=[\"input\", \"output\"],\n",
    "\ttemplate=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "\t{\"input\": \"happy\", \"output\": \"sad\"},\n",
    "\t{\"input\": \"tall\", \"output\": \"short\"},\n",
    "\t{\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "\t{\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "\t{\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = prompts_lc.SemanticSimilarityExampleSelector.from_examples(\n",
    "\texamples=examples,\n",
    "\tembeddings=inst.embedding_main,\n",
    "\tvectorstore_cls=inst.vector_store_in_memory,\n",
    "\tk=2,\n",
    ")\n",
    "\n",
    "prompt_tpl = prompts_lc.FewShotPromptTemplate(\n",
    "\texample_selector=example_selector,\n",
    "\texample_prompt=example_prompt_tpl,\n",
    "\tprefix=\"Give the antonym of every input\",\n",
    "\tsuffix=\"Input: {input}\\nOutput:\",\n",
    "\tinput_variables=[\"input\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(prompt_tpl.format(input=\"worried\"))\n",
    "# result = inst.llm_main.invoke(prompt_tpl.invoke({\"input\": \"enthusiastic\"}))\n",
    "# rp_print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select examples by semantic ngram overlap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select examples by maximal marginal relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select examples from LangSmith few-shot datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do function/tool calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get models to return structured output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cache model responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get log probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom chat model class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream a response back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### track token usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### track response metadata across providers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use chat model to call tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream tool calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle rate limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### few shot prompt tool behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bind model-specific formatted tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### force a specific tool call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work with local models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init any model in one line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trim messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge consecutive messages of the same type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cache model responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom LLM class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream a response back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### track token usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work with local models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse text from message objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use output parsers to parse an LLM response into structured format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse JSON output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse XML output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse YAML output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retry when output parsing errors occur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to fix errors in output parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom output parser class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load CSV data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from a directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load HTML data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load JSON data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load Markdown data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load Microsoft Office data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom document loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recursively split text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split by character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split Markdown by headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recursively split JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split text into semantic chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split by tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embed text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cache embedding results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom embeddings class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use a vector store to retrieve data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use a vector store to retrieve data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate multiple queries to retrieve data for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use contextual compression to compress the data retrieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom retriever class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add similarity scores to retriever results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine the results from multiple retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reorder retrieved results to mitigate the \"lost in the middle\" effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate multiple embeddings per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrieve the whole document for a chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate metadata filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a time-weighted retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use hybrid vector and keyword retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reindex data to keep your vectorstore in-sync with the underlying data source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use built-in tools and toolkits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use chat models to call tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass tool outputs to chat models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass run time values to tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add a human-in-the-loop for tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle tool errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### force models to call a tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### disable parallel tool calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### access the RunnableConfig from a tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream events from a tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### return artifacts from a tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert Runnables to tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add ad-hoc tool calling capability to models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass in runtime secrets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass multimodal data directly to models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use multimodal prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use legacy LangChain Agents (AgentExecutor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### migrate from legacy LangChain agents to LangGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass in callbacks at runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attach callbacks to a module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass callbacks into a module constructor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create custom callback handlers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use callbacks in async environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dispatch custom callback events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom chat model class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom LLM class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom embeddings class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom retriever class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom document loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a custom output parser class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create custom callback handlers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a custom tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dispatch custom callback events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save and load LangChain objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A with RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add chat history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### return sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### return citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do per-user retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use reference examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle long text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do extraction without using function calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manage memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manage large chat history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add examples to the prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle cases where no queries are generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle multiple queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle multiple retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deal with high cardinality categorical variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A over SQL + CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use prompting to improve results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do query validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deal with large databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deal with CSV files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A over graph databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add a semantic layer over the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct knowledge graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize text in a single LLM call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize text through parallelization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summarize text through iterative refinement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why LangChain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorials\n",
    "\t- Get started\n",
    "\t\t- Chat models and prompts\n",
    "\t\t- Semantic search\n",
    "\t\t- Classification\n",
    "\t\t- [Extraction](https://python.langchain.com/docs/tutorials/extraction/)\n",
    "  \t\t- [How to use reference examples when doing extraction](https://python.langchain.com/docs/how_to/extraction_examples/) ðŸš§\n",
    "\t- Orchestration\n",
    "\t\t- Chatbots\n",
    "\t\t- Agents\n",
    "\t\t- Retrieval Augmented Generation (RAG)\n",
    "\t\t- Question-Answering with SQL\n",
    "\t\t- Summarization\n",
    "\t\t- Question-Answering with Graph Databases\n",
    "\t- LangSmith\n",
    "- How-to guides\n",
    "\t- Installation\n",
    "\t\t- install LangChain packages\n",
    "\t\t- use LangChain with different Pydantic versions\n",
    "\t- Key features\n",
    "\t\t- [return structured data from a model](https://python.langchain.com/docs/how_to/structured_output/) ðŸš§\n",
    "\t\t- use a model to call tools\n",
    "\t\t- stream runnables\n",
    "\t\t- debug your LLM apps\n",
    "\t- LangChain Expression Language (LCEL)\n",
    "\t\t- chain runnables\n",
    "\t\t- stream runnables\n",
    "\t\t- invoke runnables in parallel\n",
    "\t\t- add default invocation args to runnables\n",
    "\t\t- turn any function into a runnable\n",
    "\t\t- pass through inputs from one chain step to the next\n",
    "\t\t- configure runnable behavior at runtime\n",
    "\t\t- add message history (memory) to a chain\n",
    "\t\t- route between sub-chains\n",
    "\t\t- create a dynamic (self-constructing) chain\n",
    "\t\t- inspect runnables\n",
    "\t\t- add fallbacks to a runnable\n",
    "\t\t- pass runtime secrets to a runnable\n",
    "\t- Components\n",
    "\t\t- Prompt templates\n",
    "\t\t\t- use few shot examples\n",
    "\t\t\t- use few shot examples in chat models\n",
    "\t\t\t- partially format prompt templates\n",
    "\t\t\t- compose prompts together\n",
    "\t\t- Example selectors\n",
    "\t\t\t- [use example selectors](https://python.langchain.com/docs/how_to/example_selectors/)\n",
    "\t\t\t- select examples by length\n",
    "\t\t\t- [select examples by semantic similarity](https://python.langchain.com/docs/how_to/example_selectors_similarity/)\n",
    "\t\t\t- select examples by semantic ngram overlap\n",
    "\t\t\t- select examples by maximal marginal relevance\n",
    "\t\t\t- select examples from LangSmith few-shot datasets\n",
    "\t\t- Chat models\n",
    "\t\t\t- do function/tool calling\n",
    "\t\t\t- [get models to return structured output](https://python.langchain.com/docs/how_to/structured_output/) ðŸš§\n",
    "\t\t\t- cache model responses\n",
    "\t\t\t- get log probabilities\n",
    "\t\t\t- create a custom chat model class\n",
    "\t\t\t- stream a response back\n",
    "\t\t\t- track token usage\n",
    "\t\t\t- track response metadata across providers\n",
    "\t\t\t- use chat model to call tools\n",
    "\t\t\t- stream tool calls\n",
    "\t\t\t- handle rate limits\n",
    "\t\t\t- few shot prompt tool behavior\n",
    "\t\t\t- bind model-specific formatted tools\n",
    "\t\t\t- force a specific tool call\n",
    "\t\t\t- work with local models\n",
    "\t\t\t- init any model in one line\n",
    "\t\t- Messages\n",
    "\t\t\t- trim messages\n",
    "\t\t\t- filter messages\n",
    "\t\t\t- merge consecutive messages of the same type\n",
    "\t\t- LLMs\n",
    "\t\t\t- cache model responses\n",
    "\t\t\t- create a custom LLM class\n",
    "\t\t\t- stream a response back\n",
    "\t\t\t- track token usage\n",
    "\t\t\t- work with local models\n",
    "\t\t- Output parsers\n",
    "\t\t\t- parse text from message objects\n",
    "\t\t\t- [use output parsers to parse an LLM response into structured format](https://python.langchain.com/docs/how_to/output_parser_structured/) ðŸš§\n",
    "\t\t\t- parse JSON output\n",
    "\t\t\t- parse XML output\n",
    "\t\t\t- parse YAML output\n",
    "\t\t\t- retry when output parsing errors occur\n",
    "\t\t\t- try to fix errors in output parsing\n",
    "\t\t\t- write a custom output parser class\n",
    "\t\t- Document loaders\n",
    "\t\t\t- load PDF files\n",
    "\t\t\t- load web pages\n",
    "\t\t\t- load CSV data\n",
    "\t\t\t- load data from a directory\n",
    "\t\t\t- load HTML data\n",
    "\t\t\t- load JSON data\n",
    "\t\t\t- load Markdown data\n",
    "\t\t\t- load Microsoft Office data\n",
    "\t\t\t- write a custom document loader\n",
    "\t\t- Text splitters\n",
    "\t\t\t- recursively split text\n",
    "\t\t\t- split HTML\n",
    "\t\t\t- split by character\n",
    "\t\t\t- split code\n",
    "\t\t\t- split Markdown by headers\n",
    "\t\t\t- recursively split JSON\n",
    "\t\t\t- split text into semantic chunks\n",
    "\t\t\t- split by tokens\n",
    "\t\t- Embedding models\n",
    "\t\t\t- embed text data\n",
    "\t\t\t- cache embedding results\n",
    "\t\t\t- create a custom embeddings class\n",
    "\t\t- Vector stores\n",
    "\t\t\t- use a vector store to retrieve data\n",
    "\t\t- Retrievers\n",
    "\t\t\t- use a vector store to retrieve data\n",
    "\t\t\t- generate multiple queries to retrieve data for\n",
    "\t\t\t- use contextual compression to compress the data retrieved\n",
    "\t\t\t- write a custom retriever class\n",
    "\t\t\t- add similarity scores to retriever results\n",
    "\t\t\t- combine the results from multiple retrievers\n",
    "\t\t\t- reorder retrieved results to mitigate the \"lost in the middle\" effect\n",
    "\t\t\t- generate multiple embeddings per document\n",
    "\t\t\t- retrieve the whole document for a chunk\n",
    "\t\t\t- generate metadata filters\n",
    "\t\t\t- create a time-weighted retriever\n",
    "\t\t\t- use hybrid vector and keyword retrieval\n",
    "\t\t- Indexing\n",
    "\t\t\t- reindex data to keep your vectorstore in-sync with the underlying data source\n",
    "\t\t- Tools\n",
    "\t\t\t- create tools\n",
    "\t\t\t- use built-in tools and toolkits\n",
    "\t\t\t- use chat models to call tools\n",
    "\t\t\t- pass tool outputs to chat models\n",
    "\t\t\t- pass run time values to tools\n",
    "\t\t\t- add a human-in-the-loop for tools\n",
    "\t\t\t- handle tool errors\n",
    "\t\t\t- force models to call a tool\n",
    "\t\t\t- disable parallel tool calling\n",
    "\t\t\t- access the RunnableConfig from a tool\n",
    "\t\t\t- stream events from a tool\n",
    "\t\t\t- return artifacts from a tool\n",
    "\t\t\t- convert Runnables to tools\n",
    "\t\t\t- add ad-hoc tool calling capability to models\n",
    "\t\t\t- pass in runtime secrets\n",
    "\t\t- Multimodal\n",
    "\t\t\t- pass multimodal data directly to models\n",
    "\t\t\t- use multimodal prompts\n",
    "\t\t- Agents\n",
    "\t\t\t- use legacy LangChain Agents (AgentExecutor)\n",
    "\t\t\t- migrate from legacy LangChain agents to LangGraph\n",
    "\t\t- Callbacks\n",
    "\t\t\t- pass in callbacks at runtime\n",
    "\t\t\t- attach callbacks to a module\n",
    "\t\t\t- pass callbacks into a module constructor\n",
    "\t\t\t- create custom callback handlers\n",
    "\t\t\t- use callbacks in async environments\n",
    "\t\t\t- dispatch custom callback events\n",
    "\t\t- Custom\n",
    "\t\t\t- create a custom chat model class\n",
    "\t\t\t- create a custom LLM class\n",
    "\t\t\t- create a custom embeddings class\n",
    "\t\t\t- write a custom retriever class\n",
    "\t\t\t- write a custom document loader\n",
    "\t\t\t- write a custom output parser class\n",
    "\t\t\t- create custom callback handlers\n",
    "\t\t\t- define a custom tool\n",
    "\t\t\t- dispatch custom callback events\n",
    "\t\t- Serialization\n",
    "\t\t\t- save and load LangChain objects\n",
    "\t- Use cases\n",
    "\t\t- Q&A with RAG\n",
    "\t\t\t- add chat history\n",
    "\t\t\t- stream\n",
    "\t\t\t- return sources\n",
    "\t\t\t- return citations\n",
    "\t\t\t- do per-user retrieval\n",
    "\t\t- Extraction\n",
    "\t\t\t- [use reference examples](https://python.langchain.com/docs/how_to/extraction_examples/) ðŸš§\n",
    "\t\t\t- [handle long text](https://python.langchain.com/docs/how_to/extraction_long_text/) ðŸš§\n",
    "\t\t\t- [do extraction without using function calling](https://python.langchain.com/docs/how_to/extraction_parse/) ðŸš§\n",
    "\t\t- Chatbots\n",
    "\t\t\t- manage memory\n",
    "\t\t\t- do retrieval\n",
    "\t\t\t- use tools\n",
    "\t\t\t- manage large chat history\n",
    "\t\t- Query analysis\n",
    "\t\t\t- add examples to the prompt\n",
    "\t\t\t- handle cases where no queries are generated\n",
    "\t\t\t- handle multiple queries\n",
    "\t\t\t- handle multiple retrievers\n",
    "\t\t\t- construct filters\n",
    "\t\t\t- deal with high cardinality categorical variables\n",
    "\t\t- Q&A over SQL + CSV\n",
    "\t\t\t- use prompting to improve results\n",
    "\t\t\t- do query validation\n",
    "\t\t\t- deal with large databases\n",
    "\t\t\t- deal with CSV files\n",
    "\t\t- Q&A over graph databases\n",
    "\t\t\t- add a semantic layer over the database\n",
    "\t\t\t- construct knowledge graphs\n",
    "\t\t- Summarization\n",
    "\t\t\t- summarize text in a single LLM call\n",
    "\t\t\t- summarize text through parallelization\n",
    "\t\t\t- summarize text through iterative refinement\n",
    "\t- LangSmith\n",
    "- Conceptual guide\n",
    "\t- High level\n",
    "\t\t- Why LangChain?\n",
    "\t\t- Architecture\n",
    "\t- Concepts\n",
    "\t\t- Chat models\n",
    "\t\t- Messages\n",
    "\t\t- Chat history\n",
    "\t\t- Tools\n",
    "\t\t- Tool calling\n",
    "\t\t- Structured output\n",
    "\t\t- Memory\n",
    "\t\t- Multimodality\n",
    "\t\t- Runnable interface\n",
    "\t\t- Streaming\n",
    "\t\t- LangChain Expression Language (LCEL)\n",
    "\t\t- Document loaders\n",
    "\t\t- Retrieval\n",
    "\t\t- Text splitters\n",
    "\t\t- Embedding models\n",
    "\t\t- Vector stores\n",
    "\t\t- Retriever\n",
    "\t\t- Retrieval Augmented Generation (RAG)\n",
    "\t\t- Agents\n",
    "\t\t- Prompt templates\n",
    "\t\t- Output parsers\n",
    "\t\t- Few-shot prompting\n",
    "\t\t- Example selectors\n",
    "\t\t- Async programming\n",
    "\t\t- Callbacks\n",
    "\t\t- Tracing\n",
    "\t\t- Evaluation\n",
    "\t\t- Testing\n",
    "\t- Glossary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

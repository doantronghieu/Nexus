Models
The NanoLLM interface provides model loading, quantization, embeddings, and inference.

from nano_llm import NanoLLM

model = NanoLLM.from_pretrained(
   "meta-llama/Llama-3-8b-hf",  # HuggingFace repo/model name, or path to HF model checkpoint
   api='mlc',                   # supported APIs are: mlc, awq, hf
   api_token='hf_abc123def',    # HuggingFace API key for authenticated models ($HUGGINGFACE_TOKEN)
   quantization='q4f16_ft'      # q4f16_ft, q4f16_1, q8f16_0 for MLC, or path to AWQ weights
)

response = model.generate("Once upon a time,", max_new_tokens=128)

for token in response:
   print(token, end='', flush=True)
You can run text completion from the command-line like this:

python3 -m nano_llm.completion --api=mlc \
  --model meta-llama/Llama-3-8b-chat-hf \
  --quantization q4f16_ft \
  --prompt 'Once upon a time,'
See the Chat section for examples of running multi-turn chat and function calling.

Supported Architectures
Llama

Llava

StableLM

Phi-2

Gemma

Mistral

GPT-Neox

These include fine-tuned derivatives that share the same network architecture as above (for example, lmsys/vicuna-7b-v1.5 is a Llama model). Others model types are supported via the various quantization APIs well - check the associated library documentation for details.

Tested Models
Access to Gated Models from HuggingFace Hub

To download models requiring authentication, generate an API key and request access (Llama)

Large Language Models

meta-llama/Meta-Llama-3-8B

meta-llama/Llama-2-7b-chat-hf

meta-llama/Llama-2-13b-chat-hf

meta-llama/Llama-2-70b-chat-hf

Small Language Models (SLM)

stabilityai/stablelm-2-zephyr-1_6b

stabilityai/stablelm-zephyr-3b

NousResearch/Nous-Capybara-3B-V1.9

TinyLlama/TinyLlama-1.1B-Chat-v1.0

princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT

google/gemma-2b-it

microsoft/phi-2

Vision Language Models (VLM)

liuhaotian/llava-v1.5-7b

liuhaotian/llava-v1.5-13b

liuhaotian/llava-v1.6-vicuna-7b

liuhaotian/llava-v1.6-vicuna-13b

NousResearch/Obsidian-3B-V0.5

Efficient-Large-Model/VILA-2.7b

Efficient-Large-Model/VILA-7b

Efficient-Large-Model/VILA-13b

Efficient-Large-Model/VILA1.5-3b

Efficient-Large-Model/Llama-3-VILA1.5-8B

Efficient-Large-Model/VILA1.5-13b

Model API
classNanoLLM(model_path, **kwargs)[source]
Bases: object

LLM interface that model APIs implement, including:

generate() for token generation

tokenize() and detokenize()

embed_text(), embed_tokens(), and embed_image()

The static method from_pretrained() will load the model using the specified API.

staticfrom_pretrained(model, api=None, use_cache=False, **kwargs)[source]
Load a model from the given path or download it from HuggingFace Hub. Various inference and quantization APIs are supported, such as MLC and AWQ. If the API isn’t explicitly specified, it will be inferred from the type of model.

Base class for local LLM APIs. It defines common Huggingface-like interfaces for model loading, text generation, tokenization, embeddings, and streaming. It also supports multimodal vision models like Llava and generating image embeddings with CLIP.

Parameters
:
model (str) – either the path to the model, or HuggingFace model repo/name.

api (str) – the model backend API to use: ‘auto_gptq’, ‘awq’, ‘mlc’, or ‘hf’ if left as None, it will attempt to be automatically determined.

quantization (str) – for AWQ or MLC, either specify the quantization method, or the path to the quantized model (AWQ and MLC API’s only)

vision_model (str) – for VLMs, override the vision embedding model (typically openai/clip-vit-large-patch14-336). Otherwise, it will use the CLIP variant from the config.

Returns
:
A loaded NanoLLM model instance using the determined API.

generate(inputs, streaming=True, **kwargs)[source]
Generate output from input text, tokens, or an embedding. For detailed kwarg descriptions, see transformers.GenerationConfig.

Parameters
:
inputs (str|ndarray) – Text or embedding inputs to the model/

streaming (bool) – If True, an iterator will be returned that returns text chunks. Otherwise, this function will block and return the generated text.

functions (list[callable]) – Dynamic functions or plugins to run inline with token generation for things like function calling, guidance, token healing, ect. These will be passed the text generated by the LLM so far, and any additional text that these return will be added to the chat.

max_new_tokens (int) – The number of tokens to output in addition to the prompt (default: 128)

min_new_tokens (int) – Force the model to generate a set number of output tokens (default: -1)

do_sample (bool) – If True, temperature/top_p will be used. Otherwise, greedy search (default: False)

repetition_penalty – The parameter for repetition penalty. 1.0 means no penalty (default: 1.0)

temperature (float) – Randomness token sampling parameter (default=0.7, only used if do_sample=True)

top_p (float) – If set to float < 1 and do_sample=True, only the smallest set of most probable tokens. with probabilities that add up to top_p or higher are kept for generation (default 0.95)

stop_tokens (list[int]|list[str]) – Stop generation if the bot produces tokens or text from this list (defaults to EOS token ID)

kv_cache (np.ndarray) – Previous kv_cache that the inputs will be appended to. By default, a blank kv_cache will be created for each generation (i.e. a new chat). This generation’s kv_cache will be set in the returned StreamingResponse iterator after the request is complete.

Returns
:
An asynchronous StreamingResponse iterator (when streaming=True) that outputs one decoded token string at a time. Otherwise, this function blocks and a string containing the full reply is returned after it’s been completed.

tokenize(text, add_special_tokens=False, dtype=<class 'numpy.int32'>, return_tensors='np', **kwargs)[source]
Tokenize the given string and return the encoded token ID’s.

Parameters
:
text (str) – the text to tokenize.

add_special_tokens (str) – if BOS/EOS tokens (like <s> or <|endoftext|>) should automatically be added (default False)

dtype (type) – the numpy or torch datatype of the tensor to return.

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

kwargs – additional arguments forwarded to the HuggingFace transformers.AutoTokenizer encode function.

Returns
:
The token ID’s with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) and datatype as indicated by dtype (by default int32)

detokenize(tokens, skip_special_tokens=False, **kwargs)→ str[source]
Detokenize the given token ID’s and return the decoded string.

Parameters
:
tokens (list[int], np.ndarray, torch.Tensor) – the array of token ID’s

skip_special_tokens (bool) – if special tokens (like BOS/EOS) should be supressed from the output or not (default false)

kwargs –

additional arguments forwarded to the HuggingFace transformers.AutoTokenizer decode function.

Returns
:
The string containing the decoded text.

embed_text(text, add_special_tokens=False, use_cache=False, return_tensors='np', return_tokens=False, **kwargs)[source]
Tokenize the string with NanoLLM.tokenize() and return its embedding as computed by NanoLLM.embed_tokens(). Note that if model.has_embed=False, then None will be returned for the embedding and the tokens should be used instead.

Parameters
:
text (str) – the text to tokenize and embed.

add_special_tokens (str) – if BOS/EOS tokens (like <s>, <|endoftext|>) should automatically be added (default False)

use_cache (bool) – if True, the text embedding will be cached and returned without additional computation if the same string was already embedded previously. This is useful for things like the system prompt that are relatively static, but probably shouldn’t be used for dynamic user inputs that are unlikely to be re-used again (leading to unnecessarily increased memory usage). The default is false.

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

return_tokens (bool) – if True, then the tokens will also be returned in addition to the embedding.

kwargs –

additional arguments forwarded to NanoLLM.tokenize() and the HuggingFace transformers.AutoTokenizer

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) with float32 data. If return_tokens=True, then an (embedding, tokens) tuple will be returned instead of only the embeddings. If ``model.has_embed=False`, then the embedding will be None.

embed_tokens(tokens, return_tensors='np', **kwargs)[source]
Compute the token embedding and return its tensor. This will raise an exception if model.has_embed=False.

Parameters
:
tokens (list[int], np.ndarray, torch.Tensor) – the array of token ID’s

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) with float32 data.

embed_image(image, return_tensors='pt', return_dict=False, **kwargs)[source]
Compute the embedding of an image (for multimodel models with a vision encoder like CLIP), and apply any additional projection layers as specified by the model.

Parameters
:
image (pil.Image, np.ndarray, torch.Tensor, jetson.utils.cudaImage, __cuda_array_interface__) – the image

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor (on the GPU)

return_dict (bool) – if true, return a dict including the vision encoder’s hidden_state and embedding

kwargs – additional arguments forwarded to the vision encoder (nano_llm.vision.CLIPImageEmbedding)

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor), or a dict containing the embedding and vision encoder’s hidden_state if return_dict=True.

config_path
The local path to the model config file (config.json)

model_path
The local path to the model checkpoint/weights in HuggingFace format.

config
Dict containing the model configuration (inspect it on the HuggingFace model card)

stats
Dict containing the latest generation performance statistics.

has_vision
True if this is a multimodal vision/language model.

has_embed
True if this model has a separate text embedding layer for embed_text()

tokenizer
HuggingFace transformers.AutoTokenizer instance used for tokenization/detokenization.

Streaming
classStreamingResponse(model, input, **kwargs)[source]
Bases: object

Asynchronous output iterator returned from NanoLLM.generate(). Use it to stream the reply from the LLM as they are decoded token-by-token:

response = model.generate("Once upon a time,")

for token in response:
    print(token, end='', flush=True)
The entire response generated so far is also stored in StreamingResponse.tokens and StreamingResponse.text. To terminate processing prematurely, call StreamingResponse.stop(), which will signal the model to stop from generating additional output tokens.

tokens
accumulated output tokens generated so far (for the whole reply)

text
detokenized output text generated so far (for the whole reply)

delta
the new text added since the iterator was last read

input
the original input query from the user

model
the NanoLLM model instance being used to generate the output

kv_cache
the KVCache used by this request

stopping
set if the user requested early termination

stopped
set when generation has actually stopped

__next__()[source]
Wait until the model generates more output, and return the new text (only the delta)

propertyeos
Returns true if End of Sequence (EOS) and generation has stopped.

stop()[source]
Signal the model to halt output generation before the end of the reply.

add_tokens(tokens, detokenize=True, event=False)[source]
Add an output token, detokenize the reply, and accumulate the delta message. This function is only used by the model APIs when they generate a new token.

KV Cache
classKVCache[source]
Bases: object

Abstract interface for storing & manipulating the KV cache, which encodes all the context and model state in the chat. These are implemented by different LLM backends and are backed by CUDA memory for each layer in the model, which these functions provide some modifications.

It gets returned in the StreamingResponse iterator from NanoLLM.generate() and as an optional argument can be re-used during the next generation to grow the cache instead of having to refill the chat context each request.

For example, KVCache.pop() will drop the most recent N tokens off the end of the cache, while KVCache.remove() while remove a range of tokens from anywhere in the cache.

The ChatHistory object provides a higher-level way of maintaining consistency for removing messages from the chat by keeping track of their token counts and positions in the chat. It also keeps the KV cache between requests, so that only the new tokens need to be added (and the model only processes those).

num_tokens
The current length of the KV cache

__len__()[source]
Return the current length of the cache in terms of tokens or embedding positions.

pop(tokens)[source]
Remove the given number of tokens from the end of the cache.

remove(start, stop, sync=True)[source]
Remove a range of tokens from the cache, from the start index (inclusive) to the stop index (exclusive)

Models
The NanoLLM interface provides model loading, quantization, embeddings, and inference.

from nano_llm import NanoLLM

model = NanoLLM.from_pretrained(
   "meta-llama/Llama-3-8b-hf",  # HuggingFace repo/model name, or path to HF model checkpoint
   api='mlc',                   # supported APIs are: mlc, awq, hf
   api_token='hf_abc123def',    # HuggingFace API key for authenticated models ($HUGGINGFACE_TOKEN)
   quantization='q4f16_ft'      # q4f16_ft, q4f16_1, q8f16_0 for MLC, or path to AWQ weights
)

response = model.generate("Once upon a time,", max_new_tokens=128)

for token in response:
   print(token, end='', flush=True)
You can run text completion from the command-line like this:

python3 -m nano_llm.completion --api=mlc \
  --model meta-llama/Llama-3-8b-chat-hf \
  --quantization q4f16_ft \
  --prompt 'Once upon a time,'
See the Chat section for examples of running multi-turn chat and function calling.

Supported Architectures
Llama

Llava

StableLM

Phi-2

Gemma

Mistral

GPT-Neox

These include fine-tuned derivatives that share the same network architecture as above (for example, lmsys/vicuna-7b-v1.5 is a Llama model). Others model types are supported via the various quantization APIs well - check the associated library documentation for details.

Tested Models
Access to Gated Models from HuggingFace Hub

To download models requiring authentication, generate an API key and request access (Llama)

Large Language Models

meta-llama/Meta-Llama-3-8B

meta-llama/Llama-2-7b-chat-hf

meta-llama/Llama-2-13b-chat-hf

meta-llama/Llama-2-70b-chat-hf

Small Language Models (SLM)

stabilityai/stablelm-2-zephyr-1_6b

stabilityai/stablelm-zephyr-3b

NousResearch/Nous-Capybara-3B-V1.9

TinyLlama/TinyLlama-1.1B-Chat-v1.0

princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT

google/gemma-2b-it

microsoft/phi-2

Vision Language Models (VLM)

liuhaotian/llava-v1.5-7b

liuhaotian/llava-v1.5-13b

liuhaotian/llava-v1.6-vicuna-7b

liuhaotian/llava-v1.6-vicuna-13b

NousResearch/Obsidian-3B-V0.5

Efficient-Large-Model/VILA-2.7b

Efficient-Large-Model/VILA-7b

Efficient-Large-Model/VILA-13b

Efficient-Large-Model/VILA1.5-3b

Efficient-Large-Model/Llama-3-VILA1.5-8B

Efficient-Large-Model/VILA1.5-13b

Model API
classNanoLLM(model_path, **kwargs)[source]
Bases: object

LLM interface that model APIs implement, including:

generate() for token generation

tokenize() and detokenize()

embed_text(), embed_tokens(), and embed_image()

The static method from_pretrained() will load the model using the specified API.

staticfrom_pretrained(model, api=None, use_cache=False, **kwargs)[source]
Load a model from the given path or download it from HuggingFace Hub. Various inference and quantization APIs are supported, such as MLC and AWQ. If the API isn’t explicitly specified, it will be inferred from the type of model.

Base class for local LLM APIs. It defines common Huggingface-like interfaces for model loading, text generation, tokenization, embeddings, and streaming. It also supports multimodal vision models like Llava and generating image embeddings with CLIP.

Parameters
:
model (str) – either the path to the model, or HuggingFace model repo/name.

api (str) – the model backend API to use: ‘auto_gptq’, ‘awq’, ‘mlc’, or ‘hf’ if left as None, it will attempt to be automatically determined.

quantization (str) – for AWQ or MLC, either specify the quantization method, or the path to the quantized model (AWQ and MLC API’s only)

vision_model (str) – for VLMs, override the vision embedding model (typically openai/clip-vit-large-patch14-336). Otherwise, it will use the CLIP variant from the config.

Returns
:
A loaded NanoLLM model instance using the determined API.

generate(inputs, streaming=True, **kwargs)[source]
Generate output from input text, tokens, or an embedding. For detailed kwarg descriptions, see transformers.GenerationConfig.

Parameters
:
inputs (str|ndarray) – Text or embedding inputs to the model/

streaming (bool) – If True, an iterator will be returned that returns text chunks. Otherwise, this function will block and return the generated text.

functions (list[callable]) – Dynamic functions or plugins to run inline with token generation for things like function calling, guidance, token healing, ect. These will be passed the text generated by the LLM so far, and any additional text that these return will be added to the chat.

max_new_tokens (int) – The number of tokens to output in addition to the prompt (default: 128)

min_new_tokens (int) – Force the model to generate a set number of output tokens (default: -1)

do_sample (bool) – If True, temperature/top_p will be used. Otherwise, greedy search (default: False)

repetition_penalty – The parameter for repetition penalty. 1.0 means no penalty (default: 1.0)

temperature (float) – Randomness token sampling parameter (default=0.7, only used if do_sample=True)

top_p (float) – If set to float < 1 and do_sample=True, only the smallest set of most probable tokens. with probabilities that add up to top_p or higher are kept for generation (default 0.95)

stop_tokens (list[int]|list[str]) – Stop generation if the bot produces tokens or text from this list (defaults to EOS token ID)

kv_cache (np.ndarray) – Previous kv_cache that the inputs will be appended to. By default, a blank kv_cache will be created for each generation (i.e. a new chat). This generation’s kv_cache will be set in the returned StreamingResponse iterator after the request is complete.

Returns
:
An asynchronous StreamingResponse iterator (when streaming=True) that outputs one decoded token string at a time. Otherwise, this function blocks and a string containing the full reply is returned after it’s been completed.

tokenize(text, add_special_tokens=False, dtype=<class 'numpy.int32'>, return_tensors='np', **kwargs)[source]
Tokenize the given string and return the encoded token ID’s.

Parameters
:
text (str) – the text to tokenize.

add_special_tokens (str) – if BOS/EOS tokens (like <s> or <|endoftext|>) should automatically be added (default False)

dtype (type) – the numpy or torch datatype of the tensor to return.

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

kwargs – additional arguments forwarded to the HuggingFace transformers.AutoTokenizer encode function.

Returns
:
The token ID’s with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) and datatype as indicated by dtype (by default int32)

detokenize(tokens, skip_special_tokens=False, **kwargs)→ str[source]
Detokenize the given token ID’s and return the decoded string.

Parameters
:
tokens (list[int], np.ndarray, torch.Tensor) – the array of token ID’s

skip_special_tokens (bool) – if special tokens (like BOS/EOS) should be supressed from the output or not (default false)

kwargs –

additional arguments forwarded to the HuggingFace transformers.AutoTokenizer decode function.

Returns
:
The string containing the decoded text.

embed_text(text, add_special_tokens=False, use_cache=False, return_tensors='np', return_tokens=False, **kwargs)[source]
Tokenize the string with NanoLLM.tokenize() and return its embedding as computed by NanoLLM.embed_tokens(). Note that if model.has_embed=False, then None will be returned for the embedding and the tokens should be used instead.

Parameters
:
text (str) – the text to tokenize and embed.

add_special_tokens (str) – if BOS/EOS tokens (like <s>, <|endoftext|>) should automatically be added (default False)

use_cache (bool) – if True, the text embedding will be cached and returned without additional computation if the same string was already embedded previously. This is useful for things like the system prompt that are relatively static, but probably shouldn’t be used for dynamic user inputs that are unlikely to be re-used again (leading to unnecessarily increased memory usage). The default is false.

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

return_tokens (bool) – if True, then the tokens will also be returned in addition to the embedding.

kwargs –

additional arguments forwarded to NanoLLM.tokenize() and the HuggingFace transformers.AutoTokenizer

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) with float32 data. If return_tokens=True, then an (embedding, tokens) tuple will be returned instead of only the embeddings. If ``model.has_embed=False`, then the embedding will be None.

embed_tokens(tokens, return_tensors='np', **kwargs)[source]
Compute the token embedding and return its tensor. This will raise an exception if model.has_embed=False.

Parameters
:
tokens (list[int], np.ndarray, torch.Tensor) – the array of token ID’s

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor) with float32 data.

embed_image(image, return_tensors='pt', return_dict=False, **kwargs)[source]
Compute the embedding of an image (for multimodel models with a vision encoder like CLIP), and apply any additional projection layers as specified by the model.

Parameters
:
image (pil.Image, np.ndarray, torch.Tensor, jetson.utils.cudaImage, __cuda_array_interface__) – the image

return_tensors (str) – 'np' to return a np.ndarray or 'pt' to return a torch.Tensor (on the GPU)

return_dict (bool) – if true, return a dict including the vision encoder’s hidden_state and embedding

kwargs – additional arguments forwarded to the vision encoder (nano_llm.vision.CLIPImageEmbedding)

Returns
:
The embedding with the tensor type as indicated by return_tensors (either ‘np’ for np.ndarray or ‘pt’ for torch.Tensor), or a dict containing the embedding and vision encoder’s hidden_state if return_dict=True.

config_path
The local path to the model config file (config.json)

model_path
The local path to the model checkpoint/weights in HuggingFace format.

config
Dict containing the model configuration (inspect it on the HuggingFace model card)

stats
Dict containing the latest generation performance statistics.

has_vision
True if this is a multimodal vision/language model.

has_embed
True if this model has a separate text embedding layer for embed_text()

tokenizer
HuggingFace transformers.AutoTokenizer instance used for tokenization/detokenization.

Streaming
classStreamingResponse(model, input, **kwargs)[source]
Bases: object

Asynchronous output iterator returned from NanoLLM.generate(). Use it to stream the reply from the LLM as they are decoded token-by-token:

response = model.generate("Once upon a time,")

for token in response:
    print(token, end='', flush=True)
The entire response generated so far is also stored in StreamingResponse.tokens and StreamingResponse.text. To terminate processing prematurely, call StreamingResponse.stop(), which will signal the model to stop from generating additional output tokens.

tokens
accumulated output tokens generated so far (for the whole reply)

text
detokenized output text generated so far (for the whole reply)

delta
the new text added since the iterator was last read

input
the original input query from the user

model
the NanoLLM model instance being used to generate the output

kv_cache
the KVCache used by this request

stopping
set if the user requested early termination

stopped
set when generation has actually stopped

__next__()[source]
Wait until the model generates more output, and return the new text (only the delta)

propertyeos
Returns true if End of Sequence (EOS) and generation has stopped.

stop()[source]
Signal the model to halt output generation before the end of the reply.

add_tokens(tokens, detokenize=True, event=False)[source]
Add an output token, detokenize the reply, and accumulate the delta message. This function is only used by the model APIs when they generate a new token.

KV Cache
classKVCache[source]
Bases: object

Abstract interface for storing & manipulating the KV cache, which encodes all the context and model state in the chat. These are implemented by different LLM backends and are backed by CUDA memory for each layer in the model, which these functions provide some modifications.

It gets returned in the StreamingResponse iterator from NanoLLM.generate() and as an optional argument can be re-used during the next generation to grow the cache instead of having to refill the chat context each request.

For example, KVCache.pop() will drop the most recent N tokens off the end of the cache, while KVCache.remove() while remove a range of tokens from anywhere in the cache.

The ChatHistory object provides a higher-level way of maintaining consistency for removing messages from the chat by keeping track of their token counts and positions in the chat. It also keeps the KV cache between requests, so that only the new tokens need to be added (and the model only processes those).

num_tokens
The current length of the KV cache

__len__()[source]
Return the current length of the cache in terms of tokens or embedding positions.

pop(tokens)[source]
Remove the given number of tokens from the end of the cache.

remove(start, stop, sync=True)[source]
Remove a range of tokens from the cache, from the start index (inclusive) to the stop index (exclusive)

#!/usr/bin/env python3
import os
import re
import time
import json
import shutil
import logging

import torch
import numpy as np

from transformers import AutoTokenizer

from .vision import CLIPVisionModel, MMProjector
from .utils import AttributeDict, convert_tensor, download_model, default_model_api, print_table


class NanoLLM():
    """
    LLM interface that model APIs implement, including:
    
      * :func:`generate` for token generation
      * :func:`tokenize` and :func:`detokenize`
      * :func:`embed_text`, :func:`embed_tokens`, and :func:`embed_image`
      
    The static method :func:`from_pretrained` will load the model using the specified API.
    """
    ModelCache={}
    
    @staticmethod
    def from_pretrained(model, api=None, use_cache=False, **kwargs):
        """
        Load a model from the given path or download it from HuggingFace Hub.
        Various inference and quantization APIs are supported, such as MLC and AWQ.
        If the API isn't explicitly specified, it will be inferred from the type of model.
        
        Base class for local LLM APIs. It defines common Huggingface-like interfaces for
        model loading, text generation, tokenization, embeddings, and streaming.
        It also supports multimodal vision models like Llava and generating image embeddings with CLIP.
    
        Args:
          model (str): either the path to the model, or HuggingFace model repo/name.
          api (str): the model backend API to use:  'auto_gptq', 'awq', 'mlc', or 'hf'
                       if left as None, it will attempt to be automatically determined.

          quantization (str): for AWQ or MLC, either specify the quantization method,
                              or the path to the quantized model (AWQ and MLC API's only)

          vision_model (str): for VLMs, override the vision embedding model 
                              (typically `openai/clip-vit-large-patch14-336 <https://huggingface.co/openai/clip-vit-large-patch14-336>`_).
                              Otherwise, it will use the CLIP variant from the config.
                                
        Returns:
          A loaded `NanoLLM` model instance using the determined API.
        """
        if use_cache:
            model_config = frozenset({'model': model, 'api': api, **kwargs}.items())
            cached_model = NanoLLM.ModelCache.get(model_config)
            if cached_model is not None:
                return cached_model
                
        if os.path.isdir(model) or os.path.isfile(model):
            model_path = model
            model_name = os.path.basename(model_path)
        else:
            model_path = download_model(model, **kwargs)
            model_name = os.path.basename(model)
            
        if not api:
            api = default_model_api(model_path, kwargs.get('quantization'))
        
        api = api.lower()
        
        kwargs['name'] = model_name
        kwargs['api'] = api
        
        logging.info(f"loading {model_path} with {api.upper()}")
        load_begin = time.perf_counter()
        
        # doing this imports here avoid circular import, and makes it so these
        # dependencies are only needed if they are actually used to load a model
        if api == 'auto_gptq':
            from nano_llm.models import AutoGPTQModel
            model = AutoGPTQModel(model_path, **kwargs)
        elif api == 'awq':
            from nano_llm.models import AWQModel
            model = AWQModel(model_path, **kwargs)
        elif api == 'mlc':
            from nano_llm.models import MLCModel
            model = MLCModel(model_path, **kwargs)
        elif api == 'hf':
            from nano_llm.models import HFModel
            model = HFModel(model_path, **kwargs)
        else:
            raise ValueError(f"invalid API: {api}")

        # moved CLIP to after LLM is loaded because of MLC CUDA errors when running in subprocess
        model.init_vision(**kwargs)  
        model.config.load_time = time.perf_counter() - load_begin
        
        print_table(model.config)
        print('')
        
        if use_cache:
            NanoLLM.ModelCache[model_config] = model
            
        return model
     
    def generate(self, inputs, streaming=True, **kwargs):
        """
        Generate output from input text, tokens, or an embedding.
        For detailed kwarg descriptions, see `transformers.GenerationConfig <https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig>`_.
        
        Args:
        
          inputs (str|ndarray): Text or embedding inputs to the model/
          
          streaming (bool): If True, an iterator will be returned that returns text chunks.
                            Otherwise, this function will block and return the generated text.
                              
          functions(list[callable]): Dynamic functions or plugins to run inline with token generation 
                                     for things like function calling, guidance, token healing, ect.
                                     These will be passed the text generated by the LLM so far, and any
                                     additional text that these return will be added to the chat.

          max_new_tokens (int): The number of tokens to output in addition to the prompt (default: 128)
          min_new_tokens (int): Force the model to generate a set number of output tokens (default: -1)
          do_sample (bool): If ``True``, temperature/top_p will be used.  Otherwise, greedy search (default: ``False``)
          repetition_penalty: The parameter for repetition penalty. 1.0 means no penalty (default: 1.0)
          temperature (float): Randomness token sampling parameter (default=0.7, only used if ``do_sample=True``)
          top_p (float): If set to float < 1 and ``do_sample=True``, only the smallest set of most probable tokens.
                           with probabilities that add up to top_p or higher are kept for generation (default 0.95)
          stop_tokens (list[int]|list[str]): Stop generation if the bot produces tokens or text from this list (defaults to EOS token ID)
          kv_cache (np.ndarray): Previous kv_cache that the inputs will be appended to.  By default, a blank kv_cache 
                                will be created for each generation (i.e. a new chat).  This generation's kv_cache
                                will be set in the returned :class:`StreamingResponse` iterator after the request is complete.

        Returns:
          An asynchronous :class:`StreamingResponse` iterator (when ``streaming=True``) that outputs one decoded token string at a time.
          Otherwise, this function blocks and a string containing the full reply is returned after it's been completed.
        """
        raise NotImplementedError("use LLM.from_pretrained() as opposed to instantiating an LLM object directly")

    def tokenize(self, text, add_special_tokens=False, dtype=np.int32, return_tensors='np', **kwargs):
        """
        Tokenize the given string and return the encoded token ID's.
        
        Args:
          text (str): the text to tokenize.
          add_special_tokens (str): if BOS/EOS tokens (like ``<s>`` or ``<|endoftext|>``) should automatically be added (default False)
          dtype (type): the numpy or torch datatype of the tensor to return.
          return_tensors (str): ``'np'`` to return a `np.ndarray` or ``'pt'`` to return a `torch.Tensor`
          kwargs:  additional arguments forwarded to the HuggingFace `transformers.AutoTokenizer <https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer>`_ encode function.
          
        Returns:
          The token ID's with the tensor type as indicated by `return_tensors` (either `'np'` for `np.ndarray`
          or `'pt'` for `torch.Tensor`) and datatype as indicated by `dtype` (by default ``int32``)
        """
        if return_tensors == 'tvm':
            return_tensors = 'np'
  
        tokens = self.tokenizer(
            text, 
            add_special_tokens=add_special_tokens, 
            return_tensors=return_tensors,
            **kwargs
        ).input_ids

        return convert_tensor(tokens, return_tensors=return_tensors, dtype=dtype)

    def detokenize(self, tokens, skip_special_tokens=False, **kwargs) -> str:
        """
        Detokenize the given token ID's and return the decoded string.
        
        Args:
          tokens (list[int], np.ndarray, torch.Tensor): the array of token ID's
          skip_special_tokens (bool): if special tokens (like BOS/EOS) should be supressed from the output or not (default false)
          kwargs:  additional arguments forwarded to the HuggingFace `transformers.AutoTokenizer <https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer>`_ decode function.
          
        Returns:
          The string containing the decoded text.
        """
        return self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens, **kwargs)
        
    def embed_text(self, text, add_special_tokens=False, use_cache=False, return_tensors='np', return_tokens=False, **kwargs):
        """
        Tokenize the string with :meth:`NanoLLM.tokenize` and return its embedding as computed by :meth:`NanoLLM.embed_tokens`.
        Note that if ``model.has_embed=False``, then None will be returned for the embedding and the tokens should be used instead.
        
        Args:
          text (str): the text to tokenize and embed.
          add_special_tokens (str): if BOS/EOS tokens (like ``<s>``, ``<|endoftext|>``) should automatically be added (default False)
          use_cache (bool): if True, the text embedding will be cached and returned without additional computation if
                            the same string was already embedded previously.  This is useful for things like the system prompt
                            that are relatively static, but probably shouldn't be used for dynamic user inputs that are unlikely
                            to be re-used again (leading to unnecessarily increased memory usage).  The default is false.
          return_tensors (str): ``'np'`` to return a `np.ndarray` or ``'pt'`` to return a `torch.Tensor`
          return_tokens (bool): if True, then the tokens will also be returned in addition to the embedding.
          kwargs:  additional arguments forwarded to :meth:`NanoLLM.tokenize` and the HuggingFace `transformers.AutoTokenizer <https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer>`_ 
          
        Returns:
          The embedding with the tensor type as indicated by `return_tensors` (either `'np'` for `np.ndarray`
          or `'pt'` for `torch.Tensor`) with ``float32`` data.  If ``return_tokens=True``, then an (embedding, tokens)
          tuple will be returned instead of only the embeddings. If ``model.has_embed=False`, then the embedding will be None.
        """
        result = None
        
        if use_cache:
            result = self.embed_cache.get(text)
            logging.debug(f'text embedding cache hit `{text}`'.replace('\n', '\\n'))
            
        if result is None:
            tokens = self.tokenize(text, add_special_tokens=add_special_tokens, return_tensors=return_tensors, **kwargs)
            embed = self.embed_tokens(tokens, return_tensors=return_tensors) if self.has_embed else None
            result = (embed, tokens)

        if use_cache:
            self.embed_cache[text] = result
            
        if return_tokens:
            return result
        else:
            return result[0]

    def embed_tokens(self, tokens, return_tensors='np', **kwargs):
        """
        Compute the token embedding and return its tensor.  This will raise an exception if ``model.has_embed=False``.
        
        Args:
          tokens (list[int], np.ndarray, torch.Tensor): the array of token ID's
          return_tensors (str): ``'np'`` to return a `np.ndarray` or ``'pt'`` to return a `torch.Tensor`
          
        Returns:
          The embedding with the tensor type as indicated by `return_tensors` (either `'np'` for `np.ndarray`
          or `'pt'` for `torch.Tensor`) with ``float32`` data.
        """
        raise NotImplementedError("embed_tokens() not implemented for this model")
       
    def embed_image(self, image, return_tensors='pt', return_dict=False, **kwargs):
        """
        Compute the embedding of an image (for multimodel models with a vision encoder like CLIP),
        and apply any additional projection layers as specified by the model.
        
        Args:
          image (pil.Image, np.ndarray, torch.Tensor, jetson.utils.cudaImage, __cuda_array_interface__): the image
          return_tensors (str): ``'np'`` to return a `np.ndarray` or ``'pt'`` to return a `torch.Tensor` (on the GPU)
          return_dict (bool): if true, return a dict including the vision encoder's `hidden_state` and `embedding`
          kwargs: additional arguments forwarded to the vision encoder (`nano_llm.vision.CLIPImageEmbedding`)
        
        Returns:
          The embedding with the tensor type as indicated by `return_tensors` (either `'np'` for `np.ndarray`
          or `'pt'` for `torch.Tensor`), or a dict containing the embedding and vision encoder's `hidden_state`
          if ``return_dict=True``.
        """  
        assert(self.has_vision)

        output = self.vision(image, hidden_state=self.config.mm_vision_select_layer, return_dict=return_dict)
        
        embedding = output.hidden_state if return_dict else output
        embedding = embedding.to(dtype=torch.float16)
        embedding = self.mm_projector(embedding if 'mm_projector_cfg' in self.config else embedding[:, 1:])

        logging.debug(f"image embedding  shape={embedding.shape}  dtype={embedding.dtype}  device={embedding.device}")
        
        if return_dict:
            output.embedding = embedding
            for key in output:
                output[key] = convert_tensor(output[key], return_tensors=return_tensors)
            return output
        else:
            return convert_tensor(embedding, return_tensors=return_tensors)
        
    def __init__(self, model_path, **kwargs):
        #: HuggingFace `transformers.AutoTokenizer <https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer>`_ instance used for tokenization/detokenization.
        self.tokenizer = None

        #: Dict containing the model configuration (inspect it on the HuggingFace model card)
        self.config = AttributeDict()
        
        #: The local path to the model config file (``config.json``)
        self.config_path = os.path.join(model_path, 'config.json')
        
        #: The local path to the model checkpoint/weights in HuggingFace format.
        self.model_path = model_path

        # load the config file
        if os.path.isfile(self.config_path):
            with open(self.config_path) as config_file:
                self.config = AttributeDict(json.load(config_file))
        else:
            logging.warning(f"could not find model config file at {self.config_path}")
            self.config = AttributeDict()

        self.config.name = kwargs.get('name')
        self.config.api = kwargs.get('api')
        
        #: Dict containing the latest generation performance statistics.
        self.stats = AttributeDict()
        
        #: True if this is a multimodal vision/language model.
        self.has_vision = self.config_vision()
        
        #: True if this model has a separate text embedding layer for embed_text()
        self.has_embed = False
        
        # token and embedding caches
        self.embed_cache = {}
        
        # create the tokenizer        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=True, trust_remote_code=True)
        except:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=False, trust_remote_code=True)
            
        
    def patch_config(self, **kwargs):
        # Update the original HF model's config.json with different settings from the provided kwargs.
        # The original will be saved under the same directory to 'config.json.backup'
        backup_path = self.config_path + '.backup'
        
        if not os.path.isfile(backup_path):
            logging.info(f"backing up original model config to {backup_path}")
            shutil.copyfile(self.config_path, backup_path)
            
        logging.info(f"patching model config with {kwargs}")
        
        patched_config = self.config #.copy()
        patched_config.update(kwargs)

        with open(self.config_path, 'w') as config_file:
            json.dump(patched_config, config_file, indent=2)
     
    def config_vision(self, **kwargs):
        # Check the model config for multimodal support (can be in a variety of formats)
        model_type = self.config.model_type.lower()
        has_vision = 'llava' in model_type
        
        # patch the config to change llava to llama so the quant tools handle it
        if has_vision:
            if 'stablelm' in model_type:
                self.patch_config(model_type='stablelm_epoch')
            elif 'phi' in model_type:
                self.patch_config(model_type='phi')
            else:
                self.patch_config(model_type='llama')
        else:
            name_or_path = self.config.get('_name_or_path')
            if name_or_path:
                has_vision = 'llava' in name_or_path.lower()

        for arch in self.config.get('architectures', []):
            if 'llava' in arch.lower() or 'bunny' in arch.lower():
                has_vision = True

        if self.config.model_type == 'bunny-stablelm':
            self.patch_config(model_type='stablelm_epoch')
        elif self.config.model_type == 'bunny-phi':
            self.patch_config(model_type='phi')
            
        # support checkpoints with LLM and vision encoder under separate subdirectories
        if 'vision_tower_cfg' in self.config:
            vision_path = os.path.join(self.model_path, os.path.basename(self.config['vision_tower_cfg']['_name_or_path']))
            if not os.path.isdir(vision_path):
                raise IOError(f"multimodal config was for separate models, but could not find {vision_path}")
            if 'mm_vision_tower' not in self.config:
                self.config['mm_vision_tower'] = vision_path
        
        if 'mm_projector_cfg' in self.config:
            self.config.mm_projector_path = os.path.join(self.model_path, os.path.basename(self.config['mm_projector_cfg']['_name_or_path']))
            if not os.path.isdir(self.config.mm_projector_path):
                raise IOError(f"multimodal config was for separate models, but could not find {self.config.mm_projector_path}")
            if 'mm_projector_type' not in self.config:
                self.config['mm_projector_type'] = self.config['mm_projector_cfg']['mm_projector_type']

        if 'mm_projector_path' not in self.config:
            self.config.mm_projector_path = self.model_path
                          
        if 'llm_cfg' in self.config:
            llm_path = os.path.join(self.model_path, os.path.basename(self.config['llm_cfg']['_name_or_path']))
            if not os.path.isdir(llm_path):
                raise IOError(f"multimodal config was for separate models, but could not find {llm_path}")
            with open(os.path.join(llm_path, 'config.json')) as config_file:
                self.config.update(json.load(config_file))
            self.model_path = llm_path  # redirect downstream LLM APIs to the LLM model
          
        return has_vision
               
    def init_vision(self, vision_model=None, vision_api='auto', **kwargs):
        # Load the vision encoder (CLIP/SigLIP) and mm_projector for multimodal models
        if not self.has_vision:
            return

        # load the image embedding model
        self.vision = CLIPVisionModel.from_pretrained(
            vision_model if vision_model else self.config.mm_vision_tower,
            crop=(kwargs.get('vision_scaling', 'resize') == 'crop'),
            use_tensorrt=(vision_api == 'auto' or vision_api == 'trt'), 
            dtype=torch.float16,
        ) 
        
        # create image embedding projection model
        self.mm_projector = MMProjector.from_pretrained(self, dtype=torch.float16)


#!/usr/bin/env python3
import threading

import torch
import numpy as np

from nano_llm.utils import ends_with_token


class StreamingResponse():
    """
    Asynchronous output iterator returned from :meth:`NanoLLM.generate`.
    Use it to stream the reply from the LLM as they are decoded token-by-token::
    
        response = model.generate("Once upon a time,")
        
        for token in response:
            print(token, end='', flush=True)
    
    The entire response generated so far is also stored in :attr:`StreamingResponse.tokens`
    and :attr:`StreamingResponse.text`. To terminate processing prematurely, call :meth:`StreamingResponse.stop`,
    which will signal the model to stop from generating additional output tokens.
    """
    def __init__(self, model, input, **kwargs):
        super().__init__()
        
        #: accumulated output tokens generated so far (for the whole reply)
        self.tokens = []  
        
        #: detokenized output text generated so far (for the whole reply)
        self.text = ''    
        
        #: the new text added since the iterator was last read
        self.delta = ''
        
        #: the original input query from the user
        self.input = input
        
        #: the :class:`NanoLLM` model instance being used to generate the output
        self.model = model
        
        #: the :class:`KVCache` used by this request
        self.kv_cache = kwargs.get('kv_cache', None)
        
        #: set if the user requested early termination
        self.stopping = False  
        
        #: set when generation has actually stopped
        self.stopped = False   
        
        self.event = threading.Event()
        self.kwargs = kwargs
        
    def __iter__(self):
        return self

    def __next__(self):
        """
        Wait until the model generates more output, and return the new text (only the delta)
        """
        if self.stopped:
            '''
            # early-stop EOS token is now added inside LLM APIs
            stop_tokens = self.kwargs.get('stop_tokens', [self.model.tokenizer.eos_token_id])
            if not ends_with_token(self.tokens, stop_tokens, self.model.tokenizer):
                self.add_tokens(self.model.tokenizer.eos_token_id) # add EOS if necessary
                return self._pop_delta()
            '''
            delta = self._pop_delta()
            
            if delta:
                return delta
            else:
                raise StopIteration
            
        self.event.wait()
        self.event.clear()

        return self._pop_delta()
     
    @property
    def eos(self):
        """
        Returns true if End of Sequence (EOS) and generation has stopped.
        """
        return self.stopped
        
    def stop(self):
        """
        Signal the model to halt output generation before the end of the reply.
        """
        self.stopping = True

    def add_tokens(self, tokens, detokenize=True, event=False):
        """
        Add an output token, detokenize the reply, and accumulate the delta message.
        This function is only used by the model APIs when they generate a new token.
        """
        if isinstance(tokens, (torch.Tensor, np.ndarray)):
            tokens = tokens.squeeze().tolist()
            
        if isinstance(tokens, list):
            self.tokens.extend(tokens)
        elif tokens is not None:
            self.tokens.append(tokens)
            
        if not detokenize:
            return
            
        # detokenize the entire reply on each new output token, because multiple tokens can
        # combine with each other, changing the previous text (like with long words and unicode)
        message = self.model.tokenizer.decode(self.tokens, skip_special_tokens=False, clean_up_tokenization_spaces=False)
        
        self.delta = self.delta + message[len(self.text):]
        self.text = message
        
        if event:
            self.event.set()

    def _pop_delta(self, reset=True):
        """
        Get the tokens that have accumulated since the iterator was last read, and reset it.
        """
        delta = self.delta
        
        if reset:
            self.delta = ''
            
        return delta

#!/usr/bin/env python3
import logging
import datetime

import torch
import numpy as np

from ..utils import ImageExtensions, ImageTypes, print_table


class ChatMessage():
    """
    Create a chat entry consisting of a text message, image, ect as input.  
    
    Args:
    
      role (str): The chat's turn template to apply, typically 'user' or 'bot'.
                  The role should have a corresponding entry in the active ChatTemplate.
       
      text (str): String containing the message's content for text messages.
      
      image (str|image): Either a np.ndarray, torch.Tensor, cudaImage, PIL.Image,
                         or a path to an image file (.jpg, .png, .bmp, ect)

      kwargs: For messages with alternate content types, pass them in via kwargs
              and they will automatically be determined like so::
       
                 message = ChatMessage(role='user', audio='sounds.wav')
                 
              There are additional lower-level kwargs that can be set below.
              
      use_cache (bool): cache the tokens/embeddings for reused prompts (defaults to false)
      tokens (list[int] or np.ndarray): the message contents already having been tokenized
      embedding (np.ndarray): the message contents already having been embedded
      history (ChatHistory): the ChatHistory object this message belongs to
    """    
    def __init__(self, role='user', text=None, image=None, **kwargs):
    
        #: The content or media contained in the message
        self.content = None
        
        #: The type of the message ('text', 'image', 'audio', ect)
        self.type = None
        
        #: The user role or character ('user', 'assistant', 'system', ect)
        self.role = role
        
        #: The version of this message with the role template applied
        self.template = None
        
        #: The tokenized version of the message
        self.tokens = kwargs.get('tokens', None)
        
        #: The embedding of the message
        self.embedding = kwargs.get('embedding', None)
        
        #: The ChatHistory object this message belongs to
        self.history = kwargs.get('history', None)
        
        #: Set to true if the tokens/embeddings should be cached for reused prompts
        self.use_cache = kwargs.get('use_cache', False)
        
        #: Set to true if the message is already in the chat embedding
        self.cached = kwargs.get('cached', self.tokens or self.embedding)
        
        #: The index of this message in the chat history
        self.index = None
        
        #: The previous message in the chat history
        self.prev = None
        
        #: The next message in the chat history
        self.next = None
        
        # Determine the message type
        if text is not None:
            self.content = text
            self.type = 'text'
        elif image is not None:
            self.content = image
            self.type = 'image'
        else:
            for key, value in kwargs.items():
                content_type = self.content_type(value)
                if content_type:
                    self.type = content_type
                    self.content = value
                    break
                    
            if self.type is None:
                raise ValueError(f"couldn't find valid message content in {kwargs}, please specify its type")

        # Apply variable substitutions
        #self.apply_substitutions(kwargs.get('substitutions'))
        
    @property
    def num_tokens(self):
        """
        Return the number of tokens used by this message.
        embed() needs to have been called for this to be valid.
        """
        if self.tokens is not None:
            if isinstance(self.tokens, (np.ndarray, torch.Tensor)):
                return self.tokens.shape[1]
            elif isinstance(self.tokens, list):
                return len(self.tokens)
            else:
                raise TypeError(f"ChatMessage had tokens with invalid type ({type(self.tokens)})")
        elif self.embedding is not None:
            return self.embedding.shape[1]
        else:
            return 0

    @property
    def start_token(self):
        """
        The token offset or position in the chat history at which this message begins.
        """
        offset = 0
        
        for i in range(0, self.index):
            offset += self.history[i].num_tokens
            
        return offset
        
    @staticmethod
    def content_type(content):
        """
        Try to automatically determine the message content type.
        """
        if isinstance(content, str):
            if content.endswith(ImageExtensions):
                return 'image'
            else:
                return "text" 
        elif isinstance(content, ImageTypes):
            return 'image'
        else:
            return None
    
    def is_type(self, type):
        """
        Return true if the message is of the given type (like 'text', 'image', ect)
        """
        return (self.type == type)
    
    '''
    def apply_substitutions(self, substitutions=None):
        """
        Apply variable substitutions to the message content, like "Today's date is ${DATE}".
        This is separate from the templating that occurs with the special tokens & separators.
        """
        if self.type != 'text' or self.cached or substitutions is False:
            return
            
        if isinstance(substitutions, dict):
            for key, value in substitutions.items():
                self.content = self.content.replace(key, value)
            return
            
        if "${DATE}" in self.content:
            self.content = self.content.replace("${DATE}", datetime.date.today().strftime("%Y-%m-%d"))
            
        if "${TIME}" in self.content:
            self.content = self.content.replace("${TIME}", datetime.datetime.now().strftime("%-I:%M %p"))
           
        if "${TOOLS}" in self.content:
            from nano_llm import BotFunctions
            self.content = self.content.replace("${TOOLS}", BotFunctions.generate_docs(spec=self.history.template.tool_spec))
          
        if "${LOCATION}" in self.content:
            from nano_llm.plugins.bot_functions.location import LOCATION
            self.content = self.content.replace("${LOCATION}", LOCATION())
    '''
              
    def embed(self, return_tensors='np', **kwargs):
        """
        Apply message templates, tokenization, and generate the embedding.
        """
        if self.embedding is not None:
            return self.embedding
            
        if self.tokens is not None and not self.history.model.has_embed:
            if isinstance(self.tokens, list):
                self.tokens = np.expand_dims(np.asarray(self.tokens, dtype=np.int32), axis=0)
            return self.tokens
          
        if self.history is None:
            raise RuntimeError("this message needs to be added to a ChatHistory before embed() is called")
             
        # lookup the role template to apply
        first_msg = 1 if 'system' in self.history.template else 0
        role = 'first' if 'first' in self.history.template and self.index == first_msg else self.role

        if role not in self.history.template:
            raise RuntimeError(f"chat template {self.history.template.get('name', '')} didn't have a role defined for '{entry.role}' (had keys: {self.history.template.keys()})")
         
        # extract template prefix/postfix
        template = self.history.template[role]
        split_template = template.split('${MESSAGE}')
        
        if len(split_template) == 1:  # there was no ${MESSAGE}
            split_template.append('')

        if self.prev and self.prev.role == self.role:
            split_template[0] = ''
            
        if self.next and self.next.role == self.role:
            split_template[1] = ''
         
        # embed based on media type
        if self.type == 'text':
            self._embed_text(self.history.model, split_template, return_tensors=return_tensors, **kwargs)
        elif self.type == 'image':
            self._embed_image(self.history.model, split_template, return_tensors=return_tensors, **kwargs)
            
        # mark as cached
        self.cached = True
        
        if self.embedding is not None:
            return self.embedding
            
        if self.tokens is not None:
            return self.tokens

    def _embed_text(self, model, template, return_tensors='np', **kwargs):
        """
        Generate the token embeddings for a text message.
        """
        self.template = template[0] + self.content + template[1]
        
        if self.tokens is not None:
            if model.has_embed:
                self.embedding = model.embed_tokens(self.tokens, return_tensors=return_tensors, **kwargs)
        else:     
            self.embedding, self.tokens = model.embed_text(
                self.template, use_cache=self.use_cache,
                return_tensors=return_tensors, return_tokens=True,
                **kwargs
            )
    
    def _embed_image(self, model, template, return_tensors='np', **kwargs):
        """
        Generate the encoded vision embeddings for an image.
        """
        if not model.has_vision:
            raise RuntimeError(f"attempted to embed an image in the chat, but '{model.config.name}' was not a multimodal vision model")

        # add the template prefix                
        embeddings = [] 

        if template[0]:
            embeddings.append(model.embed_text(template[0], use_cache=True, return_tensors=return_tensors))
            
        # encode the image
        image_outputs = model.embed_image(self.content, return_tensors=return_tensors, return_dict=True)
        self.history.image_embedding = image_outputs.image_embeds # save the unprojected embeddings for RAG
        embeddings.append(image_outputs.embedding)
        
        # add the template trailer
        template[1] = '\n' + template[1]
        
        if template[1]:
            embeddings.append(model.embed_text(template[1], use_cache=True, return_tensors=return_tensors))
                
        # concatenate all embeddings
        self.embedding = np.concatenate(embeddings, axis=1)
        
        if self.history.print_stats:
            print_table(model.vision.stats)
            
        #logging.debug(f"chat embed image  shape={self.embedding.shape}  dtype={self.embedding.dtype}  template={template}")
        

#!/usr/bin/env python3
import os
import re
import json
import logging
import termcolor
import numpy as np

from .message import ChatMessage
from .stream import StreamingResponse
from .templates import ChatTemplate, ChatTemplates, StopTokens
from ..utils import AttributeDict, escape_html, code_tags
                        
class ChatHistory():
    """
    Multimodal chat history that can contain a mix of media including text/images.
    
    ChatHistory objects can be indexed like a list to access its messages,
    where each :class:`ChatMessage` can have a different type of content::
    
       chat_history[n]  # will return the n-th chat entry

    Each type of media has an associated embedding function (e.g. LLM's typically 
    do text token embedding internally, and images use CLIP + projection layers). 
    From these, it assembles the embedding for the entire chat as input to the LLM.
    
    It uses templating to add the required special tokens as defined by different
    model architectures.  In normal 2-turn chat, there are 'user' and 'bot' roles
    defined, but arbitrary roles can be added, each with their own template.
    
    The system prompt can also be configured through the chat template
    and by setting the :attr:`ChatHistory.system_prompt` property.
    """
    def __init__(self, model, chat_template=None, system_prompt=None, **kwargs):
        """
        Parameters:
           
           model (NanoLLM):  The model instance used for embeddings
           
           chat_template (str|dict):  Either a chat template dict, or the name of the 
                                      chat template to use like ``llama-2``, ``vicuna-v1``
                                      If None, will attempt to determine model type.
                                  
           system_prompt (str):  Set the default system prompt used at the beginning of chats.
                                 If ``None``, will use system prompt from the template by default.
           
           tools (bool|str):  If True, tool calling will be enabled for models that have the
                              ``tool_call`` and ``tool_response`` roles in their chat templates.
                              When enabled, the function descriptors will automatically be generated
                              from their pydoc strings, and appended to the system prompt.
                                   
           print_stats (bool):  If True, generation performance will be printed to the terminal after EOS.
                                This also gets enabled by default if ``--debug`` or ``--verbose`` is used.
        """
        self.model = model 
        self.messages = None
        
        #: The :class:`KVCache` from :meth:`NanoLLM.generate()` used to store the model state.
        self.kv_cache = None
        
        # look-up or load the chat template
        if not chat_template or chat_template == 'auto':
            self.template = ChatTemplate(model)
            if self.template is None:
                raise RuntimeError(f"Couldn't automatically determine model type from {model.config.name}, please set the --chat-template argument")
            logging.info(f"using chat template '{self.template.name}' for model {model.config.name}")
        elif isinstance(chat_template, str):
            if os.path.isfile(chat_template):
                with open(chat_template) as template_file:
                    self.template = AttributeDict(json.load(template_file))
            else:
                self.template = AttributeDict(ChatTemplates[chat_template])
        elif isinstance(chat_template, dict):
            self.template = AttributeDict(template)
        else:
            raise TypeError(f"chat_template should be a str or dict (was {type(chat_template)})")

        # parse the stop tokens    
        if 'stop' in self.template:
            if not isinstance(self.template.stop, list):
                self.template.stop = [self.template.stop]
                
            for i, stop in enumerate(self.template.stop):
                if isinstance(stop, str):
                    self.template.stop[i] = self.model.tokenizer(stop, add_special_tokens=False, return_tensors='np').input_ids.squeeze().tolist()
        else:
            self.template.stop = [self.model.tokenizer.eos_token_id]
         
        #self.template.stop = [x for x in self.template.stop if x >= 0]  # filter out ignored stop tokens
        logging.info(f"model '{self.model.config.name}', chat template '{self.template.name}' stop tokens:  {self.model.tokenizer.batch_decode(self.template.stop)} -> {self.template.stop}")      

        # setup the default system prompt
        if system_prompt:
            self.template['system_prompt'] = system_prompt

        # try to determine the function-calling style
        if 'tool_spec' not in self.template:
            if 'tool_call' in self.template:
                self.template.tool_spec = 'openai'
            else:
                self.template.tool_spec = kwargs.get('tool_spec')

        self.print_stats = kwargs.get('print_stats', kwargs.get('debug', False))
        
        self.web_regex = [
            (re.compile(r'`(.*?)`'), r'<code>\1</code>'),  # code blocks
            (re.compile(r'\*(.*?)\*'), r'*<i>\1</i>*'),    # emotives inside asterisks
        ]
        
        from nano_llm import BotFunctions
        self.BotFunctions = BotFunctions
        
        self.reset()

    @property
    def num_tokens(self):
        """
        Return the number of tokens used by the chat so far.
        :meth:`embed_chat()` needs to have been called for this to be upated,
        because otherwise the input wouldn't have been tokenized yet.
        """
        position = 0
        for msg in self.messages:
            position += msg.num_tokens
        return position
        
    def __len__(self):
        """
        Returns the number of messages in the chat history
        """
        return len(self.messages)
        
    def __getitem__(self, key):
        """
        Return the n-th chat message with the subscript indexing operator
        """
        return self.messages[key]
        
    def __delitem__(self, key):
        """
        Remove one or more messages from the chat history::
        
           del chat_history[-2]   # remove the second-to-last entry
           del chat_history[-2:]  # pop the last 2 entries
           del chat_history[1:]   # remove all entries but the first
           
        This will also update the KV cache and alter the bot memory.
        """
        if isinstance(key, int):
            start = key
            stop = key + 1
        elif isinstance(key, ChatMessage):
            start = self.messages.index(key)
            stop = start + 1
        elif isinstance(key, slice):
            start = key.start
            stop = key.stop
        else:
            raise TypeError(f"The `del chat_history[*]` operator expects an int, ChatMessage, or slice (was '{type(key)}')")
        
        if start is None:
            start = 0
            
        if stop is None:
            stop = len(self.messages)
      
        self.remove(start, stop)
     
    def append(self, role='user', msg=None, **kwargs):
        """
        Add a chat entry consisting of a text message, image, ect.
        See the :class:`ChatMessage` class for description of arguments.
        This can also accept an existing :class:`ChatMessage` set to ``msg``.
        """
        if isinstance(msg, ChatMessage):
            self.messages.append(msg)
        elif isinstance(msg, StreamingResponse):
            self.messages.append(ChatMessage(role, text=msg.text, tokens=msg.tokens, history=self, **kwargs))
            self.kv_cache = msg.kv_cache
        else:
            self.messages.append(ChatMessage(role, msg=msg, history=self, **kwargs))
            
        self.reindex()
        return self.messages[-1]

    def pop(self, count):
        """
        Remove the last N messages from the chat and KV cache.
        """
        num_tokens = 0
        
        for n in range(0, count):
            num_tokens += self.messages[len(self.messages)-n-1].num_tokens
            
        if self.kv_cache:
            self.kv_cache.pop(num_tokens)
            
        del self.messages[-count:]
        self.reindex()

    def remove(self, start, stop=None):
        """
        Remove the chat entries from the start (inclusive) to stop (exclusive) indexes.
        If stop is not specified, then only the single entry at the start index will be removed::
        
          chat_history.remove(0)    # remove the first chat entry
          chat_history.remove(0,2)  # remove the first and second chat entries
          chat_history.remove(-1)   # remove the last chat entry
          chat_history.remove(-2,0) # remove the last two entries
          
        This will also update the KV cache and alter the bot's memory (potentially destructively)
        """
        num_messages = len(self.messages)
        
        if stop is None:
            stop = start + 1
             
        if start < 0:
            start += num_messages
            
        if stop <= 0:
            stop += num_messages

        if stop > num_messages:
            raise ValueError(f"remove index {stop} exceeded the number of messages ({num_messages})")
            
        if stop == num_messages:
            return self.pop(num_messages - start)
       
        if self.kv_cache:
            self.kv_cache.remove(self.messages[start].start_token, self.messages[stop].start_token)
            
        del self.messages[start:stop]       
        self.reindex()
       
    def reset(self, system_prompt=True, use_cache=True, wrap_tokens=None):
        """
        Reset the chat history, and optionally add the system prompt to the new chat.
        If ``use_cache=True``, then the system prompt tokens/embedding will be cached.
        If `wrap_tokens` is set, then the most recent N tokens from the chat will be kept.
        """
        if wrap_tokens:
            wrap_entry = self.find_wrap_entry(wrap_tokens)
            if wrap_entry:
                logging.warning(f"Wrapping chat to keep the most recent {len(self.messages)-wrap_entry} messages")
                self.messages = self.messages[wrap_entry:]
            else:
                logging.warning(f"Chat history overflow couldn't find previous chat entry to wrap to (clearing chat)")
                self.messages = []
        else:
            self.messages = []

        self.kv_cache = None
        self.image_embedding = None
        
        if isinstance(system_prompt, str):
            self.add_system_prompt(system_prompt=system_prompt, use_cache=use_cache)
        elif system_prompt:
            self.add_system_prompt(use_cache=use_cache)

    def turn(self, role='user'):
        """
        Returns true if it's the given role's turn in the chat, otherwise false.
        """
        n = len(self.messages)
        prev_role = self.messages[n-1].role if n > 0 else None
        
        if role == 'system':
            return (n == 0)
        elif role == 'user':
            if n == 0:
                return ('system' not in self.template)
            else:
                return (prev_role != 'tool_response')
        elif role == 'bot':
            return (prev_role == 'user' or prev_role == 'tool_response')
        else:
            logging.warning(f"unrecognized role in ChatHistory.turn() (role={role})")
            
        return True
        
    def to_list(self, messages=None, html=False):
        """
        Serialize the history to a list of dicts, where each dict is a chat entry
        with the non-critical keys removed (suitable for web transport, ect)
        """
        if messages is None:
            messages = self.messages
        
        if messages and isinstance(messages[0], ChatMessage):    
            messages = [{'role' : msg.role, msg.type : msg.content} for msg in messages]
        
        if html:
            messages = self.to_html(messages)
            
        return messages

    def add_system_prompt(self, system_prompt=None, use_cache=True):
        """
        Add the system prompt message to the chat, containing :attr:`ChatHistory.system_prompt`
        appended by the tool function descriptions if tools are enabled.  If the ``system`` role
        is not defined by the model's chat template, then this function does nothing.
        
        Arguments:
        
            use_cache (bool):  If true, then the system prompt tokens/embeddedings will be cached.
                               This is the default because the system prompt typically may not change.
        Returns:
        
            The :class:`ChatMessage` that was added to the chat with the ``system`` role.
        """
        if 'system' not in self.template:
            return None

        if system_prompt is not None:
            self.template.system_prompt = system_prompt
        
        return self.append(role='system', text=self.template.system_prompt, use_cache=use_cache)
            
    @property
    def system_prompt(self):
        """
        Get the system prompt, the typically hidden instruction at the beginning
        of the chat like "You are a curious and helpful AI assistant, ..."
        """
        return self.template.get('system_prompt', '')
        
    @system_prompt.setter
    def system_prompt(self, instruction):
        """
        Set the system prompt instruction string and reset the chat history.
        TODO make it so this doesn't reset the chat history, but uncaches it.
        """
        if instruction is None:
            return
            
        if self.template['system_prompt'] == instruction:
            return

        self.reset(system_prompt=instruction)

    def embed_chat(self, use_cache=True, max_tokens=None, wrap_tokens=None, **kwargs):
        """
        Assemble the embedding of either the latest or entire chat.
        
        If ``use_cache=True`` (the default), and only the new embeddings will be returned.
        If ``use_cache=False``, then the entire chat history will be returned.
        
        This function returns an ``(embedding, position)`` tuple, where the embedding array
        contains the new embeddings (or tokens) from the chat, and position is the current
        overall position in the history (up to the model's context window length)
        
        If the number of tokens in the chat history exceeds the length given in ``max_tokens`` argument
        (which is typically the model's context window, minus the max generation length),
        then the chat history will drop all but the latest ``wrap_tokens``, starting with a user prompt.
        If `max_tokens` is provided but `wrap_tokens` is not, then the overflow tokens will be truncated.
        """
        embeddings = []
        position = 0
      
        for n, msg in enumerate(self.messages):
            if use_cache:
                if msg.cached:
                    position += msg.num_tokens
                else:
                    embeddings.append(msg.embed())
                    use_cache = False  # all entries after this need to be included
            else:
                embeddings.append(msg.embed())
              
            #if not use_cache and logging.getLogger().isEnabledFor(logging.DEBUG):
            #    logging.debug(f"chat msg {n}  role={msg.role}  type={msg.type}  tokens={msg.num_tokens}  `{msg.template if msg.template else msg.content if isinstance(msg.content, str) else ''}`".replace('\n', '\\n'))

        entries = len(embeddings)
        embeddings = np.concatenate(embeddings, axis=1) #, position

        '''
        if max_tokens and position + embeddings.shape[1] > max_tokens:
            if wrap_tokens:
                self.reset(wrap_tokens=wrap_tokens)
                embeddings, position = self.embed_chat(use_cache=False, max_tokens=max_tokens, wrap_tokens=wrap_tokens, **kwargs)
                logging.warning(f"Chat overflow, max history lenth {max_tokens} tokens exceeded (keeping the most recent {embeddings.shape[1]} tokens)")
            else:
                logging.warning(f"Truncating chat history overflow to {max_tokens} tokens")
                return embeddings[:,:max_tokens,:], position
        '''
            
        logging.debug(f"chat embed  entries={entries}  shape={embeddings.shape}  position={position}")
        return embeddings, position  

    def reindex(self):
        """
        Update the linked lists in the messages that refer to each other.
        This gets called after messages are added, removed, or their order changed.
        You wouldn't typically need to call this yourself.
        """
        for i, msg in enumerate(self.messages):
            msg.index = i
            msg.history = self
            
            if i == 0:
                msg.prev = None
            elif i > 0:
                msg.prev = self.messages[i-1]
                msg.prev.next = msg
                
            if i >= len(self.messages) - 1:
                msg.next = None
           
    def find_wrap_entry(self, wrap_tokens):
        """
        Find the oldest entry from which the chat doesn't exceed the number of wrap_tokens,
        and that the entry should be a user query.  This is used to keep those more recent
        chat entries when the history overflows past the max context window of the model.
        """
        position = 0
        for n in range(len(self.messages)-1, -1, -1):
            msg = self.messages[n]
            position += msg.num_tokens
            if position >= wrap_tokens:
                for i in range(n+1, len(self.messages)):
                    if self.messages[i].role == 'user':
                        return i
     
    def to_html(self, messages=None):
        """
        Sanitize message contents to HTML representation, apply code formatting, ect.       
        """
        messages = self.to_list(messages, html=False)
        
        def web_text(text):
            for stop_token in StopTokens:
                text = text.replace(stop_token, '')
               
            text = text.strip()
            text = text.strip('\n')
             
            if text.find('<tool_call>') == 0:
                text = text.replace('\n', '')

            text = text.replace('<s>', '')
            text = escape_html(text)
            
            for regex, replace in self.web_regex:
                text = regex.sub(replace, text)

            return code_tags(text)
          
        def web_image(image):
            from nano_llm.web import WebServer
            
            if not isinstance(image, str):
                if not hasattr(image, 'filename'):
                    return None
                image = image.filename
                
            if WebServer.Instance:
                return os.path.join(self.server.mounts.get(os.path.dirname(image), ''), os.path.basename(image))
            else:
                return image
                
        for entry in messages:
            if 'text' in entry:
                entry['text'] = web_text(entry['text'])
            if 'image' in entry:
                entry['image'] = web_image(entry['image'])
                if not entry['image']:
                    del entry['image']
                
        return messages
     
    def run_tools(self, message, tools={}, append=True):
        """
        Invoke any function calls in the output text and return the results.
        """  
        if not tools:
            return None
              
        if isinstance(message, ChatMessage):
            text = message.content if message.is_type('text') else None
        elif isinstance(message, dict):
            text = message.get('text')
        elif isinstance(message, str):
            text = message
        else:
            raise ValueError("expected a message dict or string (was {type(message)})")
            
        if not text:
            return None

        tool_response = self.BotFunctions.run(text, template=self.template, functions=tools)

        if not tool_response:
            return None
            
        if append:
            self.append('tool_response', tool_response)
            
        return tool_response

#!/usr/bin/env python3

class KVCache():
    """
    Abstract interface for storing & manipulating the KV cache,
    which encodes all the context and model state in the chat.
    These are implemented by different LLM backends and are
    backed by CUDA memory for each layer in the model, which
    these functions provide some modifications.  
    
    It gets returned in the :class:`StreamingResponse` iterator
    from :meth:`NanoLLM.generate()` and as an optional argument
    can be re-used during the next generation to grow the cache
    instead of having to refill the chat context each request.
    
    For example, :meth:`KVCache.pop` will drop the most recent
    N tokens off the end of the cache, while :meth:`KVCache.remove`
    while remove a range of tokens from anywhere in the cache.
    
    The :class:`ChatHistory` object provides a higher-level way
    of maintaining consistency for removing messages from the chat
    by keeping track of their token counts and positions in the chat.
    It also keeps the KV cache between requests, so that only the
    new tokens need to be added (and the model only processes those).
    """
    def __init__(self):
        super().__init__()
        
        #: The current length of the KV cache
        self.num_tokens = 0
  
    def __len__(self):
        """
        Return the current length of the cache in terms of tokens or embedding positions.
        """
        return self.num_tokens
    
    def pop(self, tokens):
        """
        Remove the given number of tokens from the end of the cache.
        """
        raise NotImplementedError(f"{type(self)} did not implement pop()")

    def remove(self, start, stop, sync=True):
        """
        Remove a range of tokens from the cache, from the start index (inclusive) to the stop index (exclusive)
        """
        raise NotImplementedError(f"{type(self)} did not implement pop()")
               
   
#!/usr/bin/env python3
from ..utils import AttributeDict


# TODO: revisit bot trailing templates, and if \n are necessary (they were for open_llama)
#       add proper generation template instead of pre-pending it to the user template
ChatTemplates = {
    # https://huggingface.co/blog/llama2#how-to-prompt-llama-2
    'llama-2': {
        'system_prompt': "Answer the questions.",
        'system': '<s>[INST] <<SYS>>\n${MESSAGE}\n<</SYS>>\n\n',
        'first': '${MESSAGE} [/INST]',
        'user': '<s>[INST] ${MESSAGE} [/INST]',
        'bot': ' ${MESSAGE}'  # llama-2 output already ends in </s>
    },
    
    'llama-3': {
        'system_prompt': "You are a helpful and friendly AI assistant.",
        'system': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n${MESSAGE}<|eot_id|>',
        'user': '<|start_header_id|>user<|end_header_id|>\n\n${MESSAGE}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n',
        'bot': '${MESSAGE}<|eot_id|>',
        'stop': ['<|end_of_text|>', '<|eot_id|>'],
    },
    
    # https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0
    'tiny-llama': {
        'system_prompt': "You are a friendly chatbot who always gives helpful answers to the user's questions.",
        'system': "<|system|>\n${MESSAGE}</s>\n",
        'user': "<|user|>\n${MESSAGE}</s>\n<|assistant|>\n",
        'bot': "${MESSAGE}",  # model output already ends in </s>
    },
    
    # https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT
    'sheared-llama': {
        'system_prompt': "You are a helpful assistant. Write a response that appropriately completes the request.",
        'system': "${MESSAGE}\n\n",
        'user': "### Input:\n${MESSAGE}\n\n### Response:",
        'bot': "${MESSAGE}",
    },
    
    # https://huggingface.co/openlm-research/open_llama_3b_v2
    'open-llama': {
        'user': "Q: ${MESSAGE}\nA:",
        'bot': "${MESSAGE}\n",
        'stop': ["</s>", '\n', 'Q:'],  # open_llama only really outputs 1 line, and spits gibberish after
    },
    
    # https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template
    'vicuna-v0': {
        'system_prompt': "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.",
        'system': '${MESSAGE}\n\n',
        'user': '### Human: ${MESSAGE}\n### Assistant: ',
        'bot': '${MESSAGE}\n',
    },
    
    'vicuna-v1': {
        'system_prompt': "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.",
        'system': '${MESSAGE}\n\n',
        'user': 'USER: ${MESSAGE}\nASSISTANT: ',
        'bot': '${MESSAGE}\n', # TODO: does output already end in </s> ?
    },
    
    # https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml
    'chat-ml': {
        'system_prompt': "You are a helpful AI assistant.",
        'system': "<|im_start|>system\n${MESSAGE}<|im_end|>\n",
        'user': "<|im_start|>user\n${MESSAGE}<|im_end|>\n<|im_start|>assistant\n",
        'bot': "${MESSAGE}\n",  # <|im_end|> is after $MESSAGE, but is already included in bot output
    },

    # https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B
    'chat-ml-tools': {
        'system_prompt': "You are a helpful AI assistant.",
        'system': "<|im_start|>system\n${MESSAGE}<|im_end|>\n",
        'user': "<|im_start|>user\n${MESSAGE}<|im_end|>\n<|im_start|>assistant\n",
        'bot': "${MESSAGE}\n",  # <|im_end|> is after $MESSAGE, but is already included in bot output
        'tool_call': "<tool_call>(.*?)</tool_call>",
        'tool_response': "<|im_start|>tool\n<tool_response>\n${MESSAGE}\n</tool_response>\n<|im_end|>\n<|im_start|>assistant\n",
        'stop': ['<|im_end|>', '</tool_call>'],
    },

    # https://github.com/NousResearch/Obsidian/blob/e09c51d88d74657f442a898e3c4607a5b961f0b3/llava/llava/conversation.py#L385
    'nous-obsidian': {
        'system_prompt': "You are a helpful AI assistant.",
        'system': "<|im_start|>system\n${MESSAGE}\n###\n",
        'user': "<|im_start|>user\n${MESSAGE}\n###\n<|im_start|>assistant\n",
        'bot': "${MESSAGE}\n",  # ### is after $MESSAGE, but is already included in bot output
        'stop': ["###", '<|im_end|>'],
    },
    
    # https://ollama.com/library/stablelm-zephyr:latest
    'stablelm-zephyr': {
        'system_prompt': "You are a helpful AI assistant.",
        'system': "<|system|>\n${MESSAGE}<|endoftext|>\n",
        'user': "<|user|>\n${MESSAGE}<|endoftext|>\n<|assistant|>\n",
        'bot': "${MESSAGE}\n",  # <|endoftext|> is after $MESSAGE, but is already included in bot output
    },
    
    # https://huggingface.co/microsoft/phi-2
    'phi-2-chat': {
        'user': "Alice: ${MESSAGE}\nBob: ",
        'bot': "${MESSAGE}\n",
    },
    
    'phi-2-instruct': {
        'user': "Instruct: ${MESSAGE}\nOutput: ",
        'bot': "${MESSAGE}\n",
    },
    
    # https://huggingface.co/google/gemma-2b-it
    'gemma': {
        'first': "<bos><start_of_turn>user\n${MESSAGE}<end_of_turn>\n<start_of_turn>model\n",
        'user': "<end_of_turn>\n<start_of_turn>user\n${MESSAGE}<end_of_turn>\n<start_of_turn>model\n",
        'bot': "${MESSAGE}",
    },
    
    'bunny': {
        'user': 'USER: ${MESSAGE}\n',
        'bot': 'ASSISTANT: ${MESSAGE}\n', # TODO: does output already end in </s> ?
    },
}

ChatTemplates['llava-v0'] = ChatTemplates['vicuna-v0']
ChatTemplates['llava-v1'] = ChatTemplates['vicuna-v1']

ChatTemplates['llava-llama-2'] = ChatTemplates['llama-2'].copy()
ChatTemplates['llava-llama-2'].update({
    'system_prompt': "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language."
})

for key in ChatTemplates:
    ChatTemplates[key] = AttributeDict(name=key, **ChatTemplates[key])

StopTokens = ['</s>', '<|endoftext|>', '<|im_end|>', '<eos>', '<|end_of_text|>', '<|eot_id|>']

def remove_special_tokens(text):
    """
    Remove special tokens (BOS/EOS) from the string.
    """
    text = text.replace('<s>', '')
    
    for stop_token in StopTokens:
        text = text.replace(stop_token, '')
        
    return text

def ChatTemplate(model):
    """
    Attempt to automatically determine the chat template from the model name/type.
    Either returns one of the ChatTemplate dicts from above, or None if undetermined.
    """
    if not isinstance(model, str):
        model = model.config.name.lower()

    if 'stablelm' in model and 'zephyr' in model:
        chat_template = 'stablelm-zephyr'
    elif 'obsidian-3b' in model:
        chat_template = 'nous-obsidian'
    elif 'phi' in model:
        chat_template = 'phi-2-instruct'
    elif 'gemma' in model:
        chat_template = 'gemma'
    elif 'tinyllama' in model:
        chat_template = 'tiny-llama'
    elif 'sheared-llama' in model:
        chat_template = 'sheared-llama'
    elif 'open_llama' in model:
        chat_template = 'open-llama'
    elif 'vila' in model:
        chat_template = 'vicuna-v1'
    elif 'llama-2' in model:
        if 'llava' in model:
            chat_template = 'llava-llama-2'
        else:
            chat_template = 'llama-2'
    elif 'llama-3' in model:
        chat_template = 'llama-3'
    elif 'vicuna' in model:
        if 'v1' in model:
            chat_template = 'vicuna-v1'
        else:
            chat_template = 'vicuna-v0'
    elif 'llava' in model:
        if 'v1' in model:
            chat_template = 'llava-v1'
        else:
            chat_template = 'llava-v0'
    else:
        return None
        
    return AttributeDict(ChatTemplates[chat_template])  # return a copy in case user edits it
    
#!/usr/bin/env python3
from nanodb.utils import *

from .args import *
from .audio import *
from .inspection import *
from .keyboard import *
from .model import *
from .prompts import *
from .text import *
from .request import WebRequest


def ends_with_token(input, tokens, tokenizer=None):
    """
    Check to see if the list of input tokens ends with any of the list of stop tokens.
    This is typically used to check if the model produces a stop token like </s> or <eos>
    """
    if not isinstance(input, list):
        input = [input]
        
    if not isinstance(tokens, list):
        tokens = [tokens]
     
    if len(input) == 0 or len(tokens) == 0:
        return False
        
    for stop_token in tokens:
        if isinstance(stop_token, list):
            if len(stop_token) == 1:
                if input[-1] == stop_token[0]:
                    return True
            elif len(input) >= len(stop_token):
                if tokenizer:
                    input_text = tokenizer.decode(input, skip_special_tokens=False, clean_up_tokenization_spaces=False)
                    stop_text = tokenizer.decode(stop_token, skip_special_tokens=False, clean_up_tokenization_spaces=False)
                    #print('input_text', input_text, 'stop_text', f"'{stop_text}'")
                    if input_text.endswith(stop_text):
                        #print('STOPPING TEXT')
                        return True
                else:
                    if input[-len(stop_token):] == stop_token:
                        return True
        elif input[-1] == stop_token:
            return True
            
    return False
    
   
def filter_keys(dictionary, keep=None, remove=None):
    """
    Remove keys from a dict by either a list of keys to keep or remove.
    """
    if isinstance(dictionary, list):
        for x in dictionary:
            filter_keys(x, keep=keep, remove=remove)
        return dictionary
        
    for key in list(dictionary.keys()):
        if (keep and key not in keep) or (remove and key in remove):
            del dictionary[key]
 
 
def update_default(value, default=None, cast=None):
    """
    If the value is None, return the default instead.
    """
    if isinstance(value, str):
        value = value if value.strip() else default
    else:
        value = default if value is None else value
        
    if cast is not None:
        value = cast(value)
        
    return value
    
     

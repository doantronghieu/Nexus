Define your success criteria
Building a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?

Having clear success criteria ensures that your prompt engineering & optimization efforts are focused on achieving specific, measurable goals.

​
Building strong criteria
Good success criteria are:

Specific: Clearly define what you want to achieve. Instead of “good performance,” specify “accurate sentiment classification.”

Measurable: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied along with quantitative measures.

Even “hazy” topics such as ethics and safety can be quantified:
Safety criteria
Bad	Safe outputs
Good	Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter.

Example metrics and measurement methods

Quantitative metrics:

Task-specific: F1 score, BLEU score, perplexity
Generic: Accuracy, precision, recall
Operational: Response time (ms), uptime (%)
Quantitative methods:

A/B testing: Compare performance against a baseline model or earlier version.
User feedback: Implicit measures like task completion rates.
Edge case analysis: Percentage of edge cases handled without errors.
Qualitative scales:

Likert scales: “Rate coherence from 1 (nonsensical) to 5 (perfectly logical)”
Expert rubrics: Linguists rating translation quality on defined criteria
Achievable: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.

Relevant: Align your criteria with your application’s purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.


Example task fidelity criteria for sentiment analysis

Criteria
Bad	The model should classify sentiments well
Good	Our sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable).
*More on held-out test sets in the next section

​
Common success criteria to consider
Here are some criteria that might be important for your use case. This list is non-exhaustive.


Task fidelity

How well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.


Consistency

How similar does the model’s responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?


Relevance and coherence

How well does the model directly address the user’s questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?


Tone and style

How well does the model’s output style match expectations? How appropriate is its language for the target audience?


Privacy preservation

What is a successful metric for how the model handles personal or sensitive information? Can it follow instructions not to use or share certain details?


Context utilization

How effectively does the model use provided context? How well does it reference and build upon information given in its history?


Latency

What is the acceptable response time for the model? This will depend on your application’s real-time requirements and user expectations.


Price

What is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.

Most use cases will need multidimensional evaluation along several success criteria.


Example multidimensional criteria for sentiment analysis

Criteria
Bad	The model should classify sentiments well
Good	On a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:
- an F1 score of at least 0.85
- 99.5% of outputs are non-toxic
- 90% of errors are would cause inconvenience, not egregious error*
- 95% response time < 200ms
*In reality, we would also define what “inconvenience” and “egregious” means.


---

Prompt engineering overview
​
Before prompt engineering
This guide assumes that you have:

A clear definition of the success criteria for your use case
Some ways to empirically test against those criteria
A first draft prompt you want to improve
If not, we highly suggest you spend time establishing that first. Check out Define your success criteria and Create strong empirical evaluations for tips and guidance.

Prompt generator
Don’t have a first draft prompt? Try the prompt generator in the Anthropic Console!

​
When to prompt engineer
This guide focuses on success criteria that are controllable through prompt engineering. Not every success criteria or failing eval is best solved by prompt engineering. For example, latency and cost can be sometimes more easily improved by selecting a different model.


Prompting vs. finetuning

Prompt engineering is far faster than other methods of model behavior control, such as finetuning, and can often yield leaps in performance in far less time. Here are some reasons to consider prompt engineering over finetuning:

Resource efficiency: Fine-tuning requires high-end GPUs and large memory, while prompt engineering only needs text input, making it much more resource-friendly.
Cost-effectiveness: For cloud-based AI services, fine-tuning incurs significant costs. Prompt engineering uses the base model, which is typically cheaper.
Maintaining model updates: When providers update models, fine-tuned versions might need retraining. Prompts usually work across versions without changes.
Time-saving: Fine-tuning can take hours or even days. In contrast, prompt engineering provides nearly instantaneous results, allowing for quick problem-solving.
Minimal data needs: Fine-tuning needs substantial task-specific, labeled data, which can be scarce or expensive. Prompt engineering works with few-shot or even zero-shot learning.
Flexibility & rapid iteration: Quickly try various approaches, tweak prompts, and see immediate results. This rapid experimentation is difficult with fine-tuning.
Domain adaptation: Easily adapt models to new domains by providing domain-specific context in prompts, without retraining.
Comprehension improvements: Prompt engineering is far more effective than finetuning at helping models better understand and utilize external content such as retrieved documents
Preserves general knowledge: Fine-tuning risks catastrophic forgetting, where the model loses general knowledge. Prompt engineering maintains the model’s broad capabilities.
Transparency: Prompts are human-readable, showing exactly what information the model receives. This transparency aids in understanding and debugging.
​
How to prompt engineer
The prompt engineering pages in this section have been organized from most broadly effective techniques to more specialized techniques. When troubleshooting performance, we suggest you try these techniques in order, although the actual impact of each technique will depend on your use case.

Prompt generator
Be clear and direct
Use examples (multishot)
Let Claude think (chain of thought)
Use XML tags
Give Claude a role (system prompts)
Prefill Claude’s response
Chain complex prompts
Long context tips

---

Use prompt templates and variables
When deploying an LLM-based application with Claude, your API calls will typically consist of two types of content:

Fixed content Static instructions or context that remain constant across multiple interactions
Variable content: Dynamic elements that change with each request or conversation, such as:
User inputs
Retrieved content for Retrieval-Augmented Generation (RAG)
Conversation context such as user account history
System-generated data such as tool use results fed in from other independent calls to Claude
A prompt template combines these fixed and variable parts, using placeholders for the dynamic content. In the Anthropic Console, these placeholders are denoted with {{double brackets}}, making them easily identifiable and allowing for quick testing of different values.

​
When to use prompt templates and variables
You should always use prompt templates and variables when you expect any part of your prompt to be repeated in another call to Claude (only via the API or the Anthropic Console. claude.ai currently does not support prompt templates or variables).

Prompt templates offer several benefits:

Consistency: Ensure a consistent structure for your prompts across multiple interactions
Efficiency: Easily swap out variable content without rewriting the entire prompt
Testability: Quickly test different inputs and edge cases by changing only the variable portion
Scalability: Simplify prompt management as your application grows in complexity
Version control: Easily track changes to your prompt structure over time by keeping tabs only on the core part of your prompt, separate from dynamic inputs
The Anthropic Console heavily uses prompt templates and variables in order to support features and tooling for all the above, such as with the:

Prompt generator: Decides what variables your prompt needs and includes them in the template it outputs
Prompt improver: Takes your existing template, including all variables, and maintains them in the improved template it outputs
Evaluation tool: Allows you to easily test, scale, and track versions of your prompts by separating the variable and fixed portions of your prompt template
#Example prompt template

Let’s consider a simple application that translates English text to Spanish. The translated text would be variable since you would expect this text to change between users or calls to Claude. This translated text could be dynamically retrieved from databases or the user’s input.

Thus, for your translation app, you might use this simple prompt template:


Translate this text from English to Spanish: {{text}}

---

Be clear, direct, and detailed
When interacting with Claude, think of it as a brilliant but very new employee (with amnesia) who needs explicit instructions. Like any new employee, Claude does not have context on your norms, styles, guidelines, or preferred ways of working. The more precisely you explain what you want, the better Claude’s response will be.

The golden rule of clear prompting
Show your prompt to a colleague, ideally someone who has minimal context on the task, and ask them to follow the instructions. If they’re confused, Claude will likely be too.
​
How to be clear, contextual, and specific
Give Claude contextual information: Just like you might be able to better perform on a task if you knew more context, Claude will perform better if it has more contextual information. Some examples of contextual information:
What the task results will be used for
What audience the output is meant for
What workflow the task is a part of, and where this task belongs in that workflow
The end goal of the task, or what a successful task completion looks like
Be specific about what you want Claude to do: For example, if you want Claude to output only code and nothing else, say so.
Provide instructions as sequential steps: Use numbered lists or bullet points to better ensure that Claude carries out the task the exact way you want it to.
​
Examples

Example: Anonymizing customer feedback

Notice that Claude still makes mistakes in the unclear prompting example, such as leaving in a customer’s name.

Role	Unclear Prompt	Clear Prompt
User	Please remove all personally identifiable information from these customer feedback messages: {{FEEDBACK_DATA}}	Your task is to anonymize customer feedback for our quarterly review.

Instructions:
1. Replace all customer names with “CUSTOMER_[ID]” (e.g., “Jane Doe” → “CUSTOMER_001”).
2. Replace email addresses with “EMAIL_[ID]@example.com”.
3. Redact phone numbers as “PHONE_[ID]“.
4. If a message mentions a specific product (e.g., “AcmeCloud”), leave it intact.
5. If no PII is found, copy the message verbatim.
6. Output only the processed messages, separated by ”---”.

Data to process: {{FEEDBACK_DATA}}
Assistant (Claude’s response)	Here are the anonymized customer feedback messages:
1. I’ve been using your software for years, and it’s been great. But lately, AcmeCloud has been crashing a lot. Fix it!
2. XXX, your customer support rep, was incredibly helpful. He guided me through the whole process. My email is XXX if you want to send a survey.
3. I’m John Smith, and I’m not happy. I’ve been on hold for 2 hours trying to reach someone at XXX. This is unacceptable!	CUSTOMER_001: I’ve been using your software for years, and it’s been great. But lately, AcmeCloud has been crashing a lot. Fix it!
---
CUSTOMER_002: CUSTOMER_SERVICE_001, your customer support rep, was incredibly helpful. He guided me through the whole process. My email is EMAIL_002@example.com if you want to send a survey.
---
CUSTOMER_003: I’m CUSTOMER_003, and I’m not happy. I’ve been on hold for 2 hours trying to reach someone at PHONE_001. This is unacceptable!

Example: Crafting a marketing email campaign

Notice that Claude makes up details to fill in the gaps where it lacks context with the vague prompt.

Role	Vague Prompt	Specific Prompt
User	Write a marketing email for our new AcmeCloud features.	Your task is to craft a targeted marketing email for our Q3 AcmeCloud feature release.

Instructions:
1. Write for this target audience: Mid-size tech companies (100-500 employees) upgrading from on-prem to cloud.
2. Highlight 3 key new features: advanced data encryption, cross-platform sync, and real-time collaboration.
3. Tone: Professional yet approachable. Emphasize security, efficiency, and teamwork.
4. Include a clear CTA: Free 30-day trial with priority onboarding.
5. Subject line: Under 50 chars, mention “security” and “collaboration”.
6. Personalization: Use {{COMPANY_NAME}} and {{CONTACT_NAME}} variables.

Structure:
1. Subject line
2. Email body (150-200 words)
3. CTA button text
Assistant (Claude’s response)	Here’s a draft marketing email for our new features:

Subject: Boost Your Business with AcmeCloud’s Latest Features

Dear Valued Customer,

We’re excited to introduce three powerful features to AcmeCloud that will revolutionize your workflow:

1. Real-Time Collaboration: Work on documents simultaneously with your team. No more version control issues!
2. AI-Powered Insights: Make data-driven decisions with AI that analyzes trends and automates tasks.
3. Advanced Security Suite: End-to-end encryption, multi-factor auth, and threat detection keep your data safe.

Limited Offer: Upgrade now for a 20% first-year discount. Plus, refer a friend for an extra 10% off!

Best,
The AcmeCloud Team	Subject line: AcmeCloud: Enterprise security meets real-time teamwork

Email body:
Dear {{CONTACT_NAME}},

At {{COMPANY_NAME}}, you’re scaling fast and eyeing the cloud. But with growth comes the challenge of keeping data secure while fostering collaboration. That’s where AcmeCloud’s Q3 release comes in, tailor-made for tech innovators like you.

1. Fort Knox for Your Files: Military-grade encryption now safeguards your data, both in motion and at rest. Your intellectual property stays yours, period.
2. Sync Without Borders: Windows, Mac, or Linux—AcmeCloud speaks all languages. Unified version control means no more “final_v2_REALLY_FINAL.doc”.
3. Collaborate at Light Speed: Ditch the attachment ping-pong. Now your devs, designers, and data scientists co-create in real-time, slashing time-to-market.

Don’t just take our word for it. Experience the AcmeCloud difference with our 30-day trial. And because we know time is your most precious asset, we’re throwing in priority onboarding.

Elevate Your Team Now - Free 30-Day Trial

Example: Incident response

Notice that Claude outputs superfluous text and different formating with the vague prompt.

Role	Vague Prompt	Detailed Prompt
User	Analyze this AcmeCloud outage report and summarize the key points.

{{REPORT}}	Analyze this AcmeCloud outage report. Skip the preamble. Keep your response terse and write only the bare bones necessary information. List only:
1) Cause
2) Duration
3) Impacted services
4) Number of affected users
5) Estimated revenue loss.

Here’s the report: {{REPORT}}
Assistant (Claude’s response)	Here’s a summary of the key points from the AcmeCloud outage report:

On May 15th, a misconfigured load balancer update caused significant traffic routing issues, resulting in a service outage from 09:15 to 13:52 UTC. The 4-hour 37-minute disruption impacted our core file syncing service and admin dashboard, rendering them inaccessible. Additionally, the mobile app experienced partial functionality loss. Approximately 68% of our user base, totaling 1.3 million users, were affected. Based on our Service Level Agreements (SLAs) and average revenue per user, we estimate a financial impact of $420,000 in service credits and potential customer churn.	1) Cause: Misconfigured load balancer update
2) Duration: 4h 37m (09:15-13:52 UTC, May 15)
3) Impacted: Core sync, admin dashboard (down); mobile app (partial)
4) Affected users: 1.3M (68% of base)
5) Est. revenue loss: $420,000

---

Use examples (multishot prompting) to guide Claude's behavior
Examples are your secret weapon shortcut for getting Claude to generate exactly what you need. By providing a few well-crafted examples in your prompt, you can dramatically improve the accuracy, consistency, and quality of Claude’s outputs. This technique, known as few-shot or multishot prompting, is particularly effective for tasks that require structured outputs or adherence to specific formats.

Power up your prompts: Include 3-5 diverse, relevant examples to show Claude exactly what you want. More examples = better performance, especially for complex tasks.
​
Why use examples?
Accuracy: Examples reduce misinterpretation of instructions.
Consistency: Examples enforce uniform structure and style.
Performance: Well-chosen examples boost Claude’s ability to handle complex tasks.
​
Crafting effective examples
For maximum effectiveness, make sure that your examples are:

Relevant: Your examples mirror your actual use case.
Diverse: Your examples cover edge cases and potential challenges, and vary enough that Claude doesn’t inadvertently pick up on unintended patterns.
Clear: Your examples are wrapped in <example> tags (if multiple, nested within <examples> tags) for structure.
Ask Claude to evaluate your examples for relevance, diversity, or clarity. Or have Claude generate more examples based on your initial set.

Example: Analyzing customer feedback

Notice that in the “no examples” version, Claude does not list multiple options for each category, despite its written analysis indicating that there are multiple categories for certain feedback. It also includes a long explanation when we may not want it to.

Role	No Examples	With Examples
User	Analyze this customer feedback and categorize the issues. Use these categories: UI/UX, Performance, Feature Request, Integration, Pricing, and Other. Also rate the sentiment (Positive/Neutral/Negative) and priority (High/Medium/Low).

Here is the feedback: {{FEEDBACK}}	Our CS team is overwhelmed with unstructured feedback. Your task is to analyze feedback and categorize issues for our product and engineering teams. Use these categories: UI/UX, Performance, Feature Request, Integration, Pricing, and Other. Also rate the sentiment (Positive/Neutral/Negative) and priority (High/Medium/Low). Here is an example:

<example>
Input: The new dashboard is a mess! It takes forever to load, and I can’t find the export button. Fix this ASAP!
Category: UI/UX, Performance
Sentiment: Negative
Priority: High</example>

Now, analyze this feedback: {{FEEDBACK}}
Assistant (Claude’s response)	Here’s the analysis of the customer feedback:

Issue: UI/UX
Sentiment: Negative
Priority: High
The customer is frustrated with the new dashboard design, which they find cluttered and slow to load. They specifically call out the missing “export” button, which is likely a critical feature. This issue needs to be addressed as a high priority.

Issue: Integration
Sentiment: Positive
Priority: Medium
The customer is pleased with the existing Salesforce integration but would like to see support for Hubspot as well. This is a feature request that could be considered for a future product update.

Issue: Pricing
Sentiment: Negative
Priority: High
The customer finds the pricing to be unreasonable, especially for the basic features. They are threatening to switch to a competitor, so this is a high-priority issue that needs to be reviewed.

Issue: Performance
Sentiment Negative
Priority: High
The customer is experiencing critical issues with the mobile app, where large file uploads consistently result in crashes. This is a significant problem that impacts the customer’s ability to use the product effectively and should be addressed as a high priority.

Issue: UI/UX
Sentiment: Positive
Priority: Low
The customer is overall satisfied with the product, specifically calling out the intuitive UI and excellent customer support. This is a positive piece of feedback that does not require any immediate action.	Here’s the analysis of the customer feedback:

1. The new dashboard is a mess! It takes forever to load, and I can’t find the export button. Fix this ASAP!
Category: UI/UX, Performance
Sentiment: Negative
Priority: High

2. Love the Salesforce integration! But it’d be great if you could add Hubspot too.
Category: Integration, Feature Request
Sentiment: Positive
Priority: Medium

3. Your pricing is ridiculous! $99/mo for basic features? I’m switching to Competitor X.
Category: Pricing
Sentiment: Negative
Priority: High

4. The mobile app keeps crashing when I try to upload large files (>50MB). This is critical for our field agents.
Category: Performance
Sentiment: Negative
Priority: High

5. Overall, I’m happy with the product. The UI is intuitive, and the customer support is top-notch. Keep it up!
Category: UI/UX, Other (Customer Support)
Sentiment: Positive
Priority: Low

---

Let Claude think (chain of thought prompting) to increase performance
When faced with complex tasks like research, analysis, or problem-solving, giving Claude space to think can dramatically improve its performance. This technique, known as chain of thought (CoT) prompting, encourages Claude to break down problems step-by-step, leading to more accurate and nuanced outputs.

​
Before implementing CoT
​
Why let Claude think?
Accuracy: Stepping through problems reduces errors, especially in math, logic, analysis, or generally complex tasks.
Coherence: Structured thinking leads to more cohesive, well-organized responses.
Debugging: Seeing Claude’s thought process helps you pinpoint where prompts may be unclear.
​
Why not let Claude think?
Increased output length may impact latency.
Not all tasks require in-depth thinking. Use CoT judiciously to ensure the right balance of performance and latency.
Use CoT for tasks that a human would need to think through, like complex math, multi-step analysis, writing complex documents, or decisions with many factors.
​
How to prompt for thinking
The chain of thought techniques below are ordered from least to most complex. Less complex methods take up less space in the context window, but are also generally less powerful.

CoT tip: Always have Claude output its thinking. Without outputting its thought process, no thinking occurs!
Basic prompt: Include “Think step-by-step” in your prompt.
Lacks guidance on how to think (which is especially not ideal if a task is very specific to your app, use case, or organization)

Example: Writing donor emails (basic CoT)

Role	Content
User	Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think step-by-step before you write the email.
Guided prompt: Outline specific steps for Claude to follow in its thinking process.
Lacks structuring to make it easy to strip out and separate the answer from the thinking.

Example: Writing donor emails (guided CoT)

Role	Content
User	Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think before you write the email. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email using your analysis.
Structured prompt: Use XML tags like <thinking> and <answer> to separate reasoning from the final answer.

Example: Writing donor emails (structured guided CoT)

Role	Content
User	Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think before you write the email in <thinking> tags. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email in <email> tags, using your analysis.
​
Examples

Example: Financial analysis without thinking

Without step-by-step thinking, Claude gives a recommendation that seems reasonable but lacks depth. It doesn’t quantify the potential outcomes or consider historical market performance, which are crucial for making an informed decision.

Role	Content
User	You’re a financial advisor. A client wants to invest $10,000. They can choose between two options: A) A stock that historically returns 12% annually but is volatile, or B) A bond that guarantees 6% annually. The client needs the money in 5 years for a down payment on a house. Which option do you recommend?
Assistant	Based on the information provided, I would recommend option B, the bond that guarantees a 6% annual return. Here’s why:

1. Certainty: The bond offers a guaranteed 6% annual return, which provides certainty in financial planning. This is crucial when the client has a specific financial goal (down payment) with a fixed timeline (5 years).
2. Risk aversion: Since the client needs this money for an important life event (buying a house), they likely have a low risk tolerance. The volatility of the stock market could jeopardize their plans if there’s a downturn close to when they need the funds.
3. Time horizon: With only 5 years, there’s less time to recover from potential stock market downturns. Longer investment horizons (10+ years) are generally better suited for weathering stock market volatility.

While the stock’s historical 12% return is attractive, past performance doesn’t guarantee future results. The guaranteed 6% from the bond ensures the client will have the funds they need when it’s time to make the down payment.

Example: Financial analysis with thinking

With step-by-step thinking, the advisor provides a much richer analysis. It calculates exact figures for both scenarios, considers historical market volatility, and deeply analyzes the client’s risk tolerance. This thorough reasoning leads to a more confident and justifiable recommendation.

Role	Content
User	You’re a financial advisor. A client wants to invest $10,000. They can choose between two options: A) A stock that historically returns 12% annually but is volatile, or B) A bond that guarantees 6% annually. The client needs the money in 5 years for a down payment on a house. Which option do you recommend? Think step-by-step.
Assistant	To determine the best investment option for our client, I’ll consider several factors step-by-step:

<thinking>
1. Understand the client’s goal and timeline:
- Goal: Down payment for a house
- Timeline: 5 years
- Implication: This is a short-term, high-priority financial goal. The client likely cannot afford significant risk or volatility.

2. Analyze Option A (Stock):
- Pro: Higher historical return (12% annually)
- Con: Volatile (implying potential for significant gains or losses)
- Calculate potential 5-year return:
- Best case (12% annually): $10,000 * (1.12)^5 ≈ $17,623.42
- Worst case (market crash): Could lose a substantial portion, e.g., 50% crash = $5,000

3. Analyze Option B (Bond):
- Pro: Guaranteed return (6% annually)
- Con: Lower return compared to historical stock performance
- Calculate 5-year return:
- Guaranteed: $10,000 * (1.06)^5 ≈ $13,382.25

4. Consider risk tolerance:
- Goal is a house down payment (essential, not discretionary)
- Short timeline (5 years) limits ability to recover from market downturns
- Conclusion: Client likely has low risk tolerance

5. Historical perspective:
- Stock’s 12% is historical, not guaranteed
- In 5-year periods, stock market can vary wildly:
- 2003-2007 (pre-financial crisis): S&P 500 up ~82%
- 2007-2011 (during crisis): S&P 500 down ~2%
- Conclusion: Past performance doesn’t guarantee future results, especially short-term

6. Final analysis:
- Option A (Stock): Higher potential return but unacceptable risk of not meeting the down payment goal
- Option B (Bond): Guaranteed to meet ~80% of best-case stock scenario, 100% certainty of having funds
</thinking>

<answer>
I recommend Option B, the bond with a guaranteed 6% annual return. While the stock’s historical 12% return is tempting, the bond ensures you’ll have $13,382.25 in 5 years for your house down payment. Given the importance and short timeline of your goal, the stock’s volatility poses an unacceptable risk. The bond provides certainty, which is invaluable for such a crucial financial milestone.
</answer>

---

Use XML tags to structure your prompts
When your prompts involve multiple components like context, instructions, and examples, XML tags can be a game-changer. They help Claude parse your prompts more accurately, leading to higher-quality outputs.

XML tip: Use tags like <instructions>, <example>, and <formatting> to clearly separate different parts of your prompt. This prevents Claude from mixing up instructions with examples or context.
​
Why use XML tags?
Clarity: Clearly separate different parts of your prompt and ensure your prompt is well structured.
Accuracy: Reduce errors caused by Claude misinterpreting parts of your prompt.
Flexibility: Easily find, add, remove, or modify parts of your prompt without rewriting everything.
Parseability: Having Claude use XML tags in its output makes it easier to extract specific parts of its response by post-processing.
There are no canonical “best” XML tags that Claude has been trained with in particular, although we recommend that your tag names make sense with the information they surround.
​
Tagging best practices
Be consistent: Use the same tag names throughout your prompts, and refer to those tag names when talking about the content (e.g, Using the contract in <contract> tags...).
Nest tags: You should nest tags <outer><inner></inner></outer> for hierarchical content.
Power user tip: Combine XML tags with other techniques like multishot prompting (<examples>) or chain of thought (<thinking>, <answer>). This creates super-structured, high-performance prompts.
​
Examples

Example: Generating financial reports

Without XML tags, Claude misunderstands the task and generates a report that doesn’t match the required structure or tone. After substitution, there is also a chance that Claude misunderstands where one section (like the the Q1 report example) stops and another begins.

Role	No XML Tags	With XML Tags
User	You’re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors. Include sections on Revenue Growth, Profit Margins, and Cash Flow, like with this example from last year: {{Q1_REPORT}}. Use data points from this spreadsheet: {{SPREADSHEET_DATA}}. The report should be extremely concise, to the point, professional, and in list format. It should and highlight both strengths and areas for improvement.	You’re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors.

AcmeCorp is a B2B SaaS company. Our investors value transparency and actionable insights.

Use this data for your report:<data>{{SPREADSHEET_DATA}}</data>

<instructions>
1. Include sections: Revenue Growth, Profit Margins, Cash Flow.
2. Highlight strengths and areas for improvement.
</instructions>

Make your tone concise and professional. Follow this structure:
<formatting_example>{{Q1_REPORT}}</formatting_example>
Assistant	Q2 Financial Report for AcmeCorp Investors

Dear Valued Investors,

I am pleased to present our Q2 financial report, showcasing AcmeCorp’s continued growth and resilience in the B2B SaaS market.

1. Revenue Growth:
Our Q2 revenue reached $15.2M, a 22% YoY increase. This growth is primarily driven by a 30% expansion in our enterprise client base. However, our SMB segment saw a slight decline of 5%, which we aim to address through targeted marketing initiatives in Q3.

2. Profit Margins:
Gross margin improved to 72% (up 3% from Q1) due to infrastructure optimizations. EBITDA margin, however, dipped to 18% (down 2%) as we invested heavily in R&D for our upcoming AI-driven features.

3. Cash Flow:
Operating cash flow remains strong at $4.1M. Our cash reserves stand at $28M, providing a 15-month runway at current burn rate. We did see a 20% increase in customer acquisition costs, which we’re addressing through more efficient sales strategies.

In summary, AcmeCorp’s core financials remain robust. We’re balancing growth investments with prudent cash management. Our focus for Q3 will be revitalizing SMB growth and optimizing sales efficiency.

Thank you for your continued trust.

Sincerely,
CFO, AcmeCorp	Q2 Financial Report for AcmeCorp Investors</output>

Revenue Growth:
- Q2 revenue: $15.2M (22% YoY ↑)
- Enterprise clients: 30% ↑
- SMB segment: 5% ↓
Action: Targeted Q3 marketing

Profit Margins:
- Gross margin: 72% (3% ↑ from Q1)
Reason: Infrastructure optimizations
- EBITDA margin: 18% (2% ↓)
Reason: R&D investment in AI features

Cash Flow:
- Operating cash flow: $4.1M (strong)
- Cash reserves: $28M (15-month runway)
- Customer acquisition cost: 20% ↑
Action: Optimize sales strategies

Outlook:
- Core financials: Robust
- Q3 focus: Revitalize SMB, optimize sales

Management remains committed to balancing growth investments with financial discipline.

Example: Legal contract analysis

Without XML tags, Claude’s analysis is disorganized and misses key points. With tags, it provides a structured, thorough analysis that a legal team can act on.

Role	No XML Tags	With XML Tags
User	Analyze this software licensing agreement for potential risks and liabilities: {{CONTRACT}}. Focus on indemnification, limitation of liability, and IP ownership clauses. Also, note any unusual or concerning terms. Here’s our standard contract for reference: {{STANDARD_CONTRACT}}. Give a summary of findings and recommendations for our legal team.	Analyze this software licensing agreement for legal risks and liabilities.

We’re a multinational enterprise considering this agreement for our core data infrastructure.

<agreement>
{{CONTRACT}}</agreement>

This is our standard contract for reference:
<standard_contract>{{STANDARD_CONTRACT}}</standard_contract>

<instructions>
1. Analyze these clauses:
- Indemnification
- Limitation of liability
- IP ownership

2. Note unusual or concerning terms.

3. Compare to our standard contract.

4. Summarize findings in <findings> tags.

5. List actionable recommendations in <recommendations> tags.</instructions>
Assistant	The provided software licensing agreement presents several concerning aspects. The indemnification clause is overly broad, potentially exposing us to significant liabilities. The limitation of liability section caps damages at a mere $500, which is inadequate for our enterprise-level usage. Regarding IP, the agreement grants the vendor joint ownership of any modifications or integrations we create.

In summary, this agreement is not in our best interest. It exposes us to undue risk, limits our recourse in case of failures, and compromises our intellectual property rights. I recommend renegotiating these terms or seeking alternative vendors.	<findings>
1. Indemnification (Clause 8):
- Issue: Overly broad. We indemnify vendor against “any and all claims” arising from our use.
- Risk: Could hold us liable even for vendor’s negligence or product defects.
- Deviation: Our standard limits indemnity to claims arising from our violation of agreement.

2. Limitation of Liability (Clause 9):
- Issue: Caps all damages at $500, including direct damages.
- Risk: Grossly inadequate for enterprise use. A critical failure could cost millions.
- Deviation: Our standard allows for direct damages up to 12 months of fees paid.

3. IP Ownership (Clause 12):
- Issue: Grants vendor joint ownership of our modifications/integrations.
- Risk: Vendor could use or sell our proprietary algorithms or data insights.
- Deviation: Our standard retains full ownership of our work.

4. Unusual Terms:
- Clause 5.3: Vendor can use our usage data for “any purpose.”
- Clause 7.2: Vendor can terminate for “convenience” with only 7 days’ notice.
- Clause 11: No warranties of any kind, even for basic merchantability.
</findings>

<recommendations>
1. Reject this agreement. Risks far outweigh benefits for an enterprise.
2. Counter-propose:
- Indemnification: Limit to claims arising from our violation of agreement.
- Liability: Cap at 12 months’ fees for direct damages; negotiate for indirect.
- IP: Full ownership of our modifications/integrations.
- Data Use: Restrict to service improvement only.
- Termination: Minimum 90 days’ notice; only for cause.
- Warranties: Demand basic merchantability and fitness for purpose.
3. If vendor refuses, explore alternative providers or in-house solutions.
4. Engage legal counsel for contract negotiation given high stakes.
</recommendations>

---

Giving Claude a role with a system prompt
When using Claude, you can dramatically improve its performance by using the system parameter to give it a role. This technique, known as role prompting, is the most powerful way to use system prompts with Claude.

The right role can turn Claude from a general assistant into your virtual domain expert!

System prompt tips: Use the system parameter to set Claude’s role. Put everything else, like task-specific instructions, in the user turn instead.
​
Why use role prompting?
Enhanced accuracy: In complex scenarios like legal analysis or financial modeling, role prompting can significantly boost Claude’s performance.
Tailored tone: Whether you need a CFO’s brevity or a copywriter’s flair, role prompting adjusts Claude’s communication style.
Improved focus: By setting the role context, Claude stays more within the bounds of your task’s specific requirements.
​
How to give Claude a role
Use the system parameter in the Messages API to set Claude’s role:


import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=2048,
    system="You are a seasoned data scientist at a Fortune 500 company.", # <-- role prompt
    messages=[
        {"role": "user", "content": "Analyze this dataset for anomalies: <dataset>{{DATASET}}</dataset>"}
    ]
)

print(response.content)
Role prompting tip: Experiment with roles! A data scientist might see different insights than a marketing strategist for the same data. A data scientist specializing in customer insight analysis for Fortune 500 companies might yield different results still!
​
Examples
​
Example 1: Legal contract analysis
Without a role, Claude might miss critical issues:


Legal contract analysis without role prompting

Role	Content
User	Analyze this software licensing agreement for potential risks:
<contract>
{{CONTRACT}}
</contract>

Focus on indemnification, liability, and IP ownership.
Assistant	Here’s a summary of the key points in the software licensing agreement:

1. Indemnification (Section 8):
- We indemnify the vendor against claims arising from our use.

2. Limitation of Liability (Section 9):
- Damages capped at $500.

3. IP Ownership (Section 12):
- We retain ownership of our modifications.

Overall, the agreement seems standard. The indemnification and liability clauses are typical, and we maintain our IP rights.
With a role, Claude catches critical issues that could cost millions:


Legal contract analysis with role prompting

Role	Content
User	You are the General Counsel of a Fortune 500 tech company. We’re considering this software licensing agreement for our core data infrastructure:
<contract>
{{CONTRACT}}
</contract>

Analyze it for potential risks, focusing on indemnification, liability, and IP ownership. Give your professional opinion.
Assistant	I’ve reviewed the software licensing agreement, and I have serious concerns that could expose our company to significant risks:

1. Indemnification (Section 8):
- Critical Issue: We indemnify the vendor against “any and all claims” arising from our use.
- Risk: This could hold us liable even for the vendor’s negligence or product defects.
- Recommendation: Reject. Limit our indemnity to claims arising from our violation of the agreement.

2. Limitation of Liability (Section 9):
- Critical Issue: Damages capped at $500, including direct damages.
- Risk: Grossly inadequate. A critical failure in our data infrastructure could cost us millions in lost revenue and reputational damage.
- Recommendation: Reject. Negotiate for cap at 12 months of fees for direct damages, and carve-outs for data breaches.

3. IP Ownership (Section 12):
- Critical Issue: Vendor gains joint ownership of our modifications and integrations.
- Risk: They could use or sell our proprietary algorithms or data insights to competitors.
- Recommendation: Reject. We must retain full ownership of our work.

Professional Opinion: Do not sign this agreement. It is grossly unfavorable and exposes us to unacceptable levels of risk. If the vendor refuses to renegotiate these terms, I strongly advise exploring alternative providers or accelerating our in-house solution. The potential costs of proceeding with this agreement far outweigh any short-term gains.
​
Example 2: Financial analysis
Without a role, Claude’s analysis lacks depth:


Financial analysis without role prompting

Role	Content
User	Analyze this dataset of our Q2 financials:
<data>
{{FINANCIALS}}
</data>

Highlight key trends and recommend actions.
Assistant	Here’s a summary of your Q2 financials:

1. Revenue: $15.2M, up 22% YoY. Enterprise client base grew 30%.
2. Gross Margin: 72%, up 3% from Q1.
3. EBITDA Margin: 18%, down 2% due to R&D investments.
4. Cash Flow: $4.1M operating, $28M in reserves.

Overall, financials look strong with revenue growth and healthy margins. Increased R&D spending is impacting profitability but could pay off long-term. Maintain current course.
With a role, Claude delivers actionable insights:


Financial analysis with role prompting

Role	Content
User	You are the CFO of a high-growth B2B SaaS company. We’re in a board meeting discussing our Q2 financials:
<data>
{{FINANCIALS}}
</data>

Analyze key trends, flag concerns, and recommend strategic actions. Our investors want aggressive growth but are wary of our burn rate.
Assistant	Board members, here’s my analysis of our Q2 financials:

1. Revenue ($15.2M, +22% YoY):
- Insight: Enterprise growth (+30%) is outpacing overall growth.
- Flag: SMB segment declined 5%.
- Action: Reallocate 20% of SMB marketing budget to enterprise sales. Task product to simplify SMB onboarding.

2. Margins (Gross: 72%, +3% QoQ; EBITDA: 18%, -2% QoQ):
- Insight: Infrastructure optimizations boosted gross margins.
- Flag: R&D spend on AI features dented EBITDA.
- Action: No change. Our AI roadmap is critical for enterprise stickiness.

3. Cash ($4.1M OCF, $28M reserves):
- Insight: 15-month runway at current burn.
- Flag: CAC up 20%, eroding LTV/CAC.
- Action: Freeze non-essential hires. Implement sales funnel analytics to cut CAC by 15% in Q3.

Strategic Stance: We’re threading the needle between growth and burn. Enterprise focus is our growth engine—invest there. But watch SMB churn; it could bleed into enterprise. Our AI bet is sound, but we must convert it to ARR by Q4, or we cut R&D by 30%.

In sum: Double down on enterprise, streamline SMB, optimize sales, and monetize AI. Questions?

---

Prefill Claude's response for greater output control
When using Claude, you have the unique ability to guide its responses by prefilling the Assistant message. This powerful technique allows you to direct Claude’s actions, skip preambles, enforce specific formats like JSON or XML, and even help Claude maintain character consistency in role-play scenarios.

In some cases where Claude is not performing as expected, a few prefilled sentences can vastly improve Claude’s performance. A little prefilling goes a long way!

​
How to prefill Claude’s response
To prefill, include the desired initial text in the Assistant message (Claude’s response will continue from where the Assistant message leaves off):


import anthropic

client = anthropic.Anthropic()
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "What is your favorite color?"},
        {"role": "assistant", "content": "As an AI assistant, I don't have a favorite color, But if I had to pick, it would be green because"}  # Prefill here
    ]
)
​
Examples
​
Example 1: Controlling output formatting and skipping the preamble
Power user tip: Prefilling { forces Claude to skip the preamble and directly output the JSON object. This is cleaner, more concise, and easier for programs to parse without additional processing.

Example: Structured data extraction without prefilling


Example: Structured data extraction with prefilling

​
Example 2: Maintaining character in roleplay scenarios
Role-play tip: Prefilling a bracketed [ROLE_NAME] can remind Claude stay in character, even for longer and more complex conversations. This is especially powerful when combined with role prompting in the system parameter.

Example: Maintaining character without role prompting

Role	Content
User (50 conversation turns in with “Sherlock Holmes” Claude)	What do you deduce about the owner of this shoe? <image>muddy_shoe.jpg</image>
Assistant	As an AI assistant, I would be happy to analyze the image of a muddy shoe as if you had provided it. Here’s my deduction…

Example: Maintaining character with role prompting

Role	Content
User (50 conversation turns in with “Sherlock Holmes” Claude)	What do you deduce about the owner of this shoe?
<image>muddy_shoe.jpg</image>
Assistant (prefill)	[Sherlock Holmes]
Assistant (Claude’s response)	Ah, what have we here? A most curious specimen, Watson! Let us examine this shoe with a discerning eye. The mud, you see, is not your common London clay, but rather the distinctive reddish hue of the iron-rich soil found only in the environs of Hampstead Heath. And yet, the wear on the sole suggests a gait more suited to the cobblestones of Whitechapel than the gentle slopes of the Heath.

Now, observe the laces! They are tied with a particular knot favored by those in the medical profession—doubtless a habit acquired during years of tying surgical gowns. But the scuff marks on the toe box tell a different tale: they are consistent with the kicking motion one might employ in the martial arts, specifically Baritsu.

We may conclude, then, that our absent shoe owner is a doctor, possibly a surgeon, who resides in Whitechapel but frequents Hampstead Heath—perhaps for a secret Baritsu practice, away from prying eyes. A most intriguing case, Watson! We must make haste; the game is afoot!

---

Chain complex prompts for stronger performance
When working with complex tasks, Claude can sometimes drop the ball if you try to handle everything in a single prompt. Chain of thought (CoT) prompting is great, but what if your task has multiple distinct steps that each require in-depth thought?

Enter prompt chaining: breaking down complex tasks into smaller, manageable subtasks.

​
Why chain prompts?
Accuracy: Each subtask gets Claude’s full attention, reducing errors.
Clarity: Simpler subtasks mean clearer instructions and outputs.
Traceability: Easily pinpoint and fix issues in your prompt chain.
​
When to chain prompts
Use prompt chaining for multi-step tasks like research synthesis, document analysis, or iterative content creation. When a task involves multiple transformations, citations, or instructions, chaining prevents Claude from dropping or mishandling steps.

Remember: Each link in the chain gets Claude’s full attention!

Debugging tip: If Claude misses a step or performs poorly, isolate that step in its own prompt. This lets you fine-tune problematic steps without redoing the entire task.
​
How to chain prompts
Identify subtasks: Break your task into distinct, sequential steps.
Structure with XML for clear handoffs: Use XML tags to pass outputs between prompts.
Have a single-task goal: Each subtask should have a single, clear objective.
Iterate: Refine subtasks based on Claude’s performance.
​
Example chained workflows:
Multi-step analysis: See the legal and business examples below.
Content creation pipelines: Research → Outline → Draft → Edit → Format.
Data processing: Extract → Transform → Analyze → Visualize.
Decision-making: Gather info → List options → Analyze each → Recommend.
Verification loops: Generate content → Review → Refine → Re-review.
Optimization tip: For tasks with independent subtasks (like analyzing multiple docs), create separate prompts and run them in parallel for speed.
​
Advanced: Self-correction chains
You can chain prompts to have Claude review its own work! This catches errors and refines outputs, especially for high-stakes tasks.


Example: Self-correcting research summary

Prompt 1
Role	Content
User	Summarize this medical research paper.

<paper>{{RESEARCH_PAPER}}</paper>

Focus on methodology, findings, and clinical implications.
Assistant
{{SUMMARY}}	Methodology: Double-blind RCT with 500 Type 2 diabetes patients. Group A (n=250) received novel GLP-1/GIP dual agonist; Group B (n=250) received standard GLP-1 agonist. 24-week trial, primary endpoint: HbA1c reduction.

Findings:
- Group A: 1.8% mean HbA1c reduction
- Group B: 1.1% mean HbA1c reduction
- 68% of Group A vs. 52% of Group B achieved HbA1c <7%
- Group A: 5.2kg mean weight loss
- Group B: 3.1kg mean weight loss

Safety: Similar adverse event profiles, no serious events attributed to treatment.

Clinical Implications: Dual agonist shows superior glycemic control and weight loss compared to standard therapy. Could be a game-changer, especially for obese diabetics.
Prompt 2
Role	Content
User	Your task is to provide feedback on a research paper summary. Here is a summary of a medical research paper:
<summary>
{{SUMMARY}}
</summary>

Here is the research paper:
<paper>
{{RESEARCH_PAPER}}
</paper>

Review this summary for accuracy, clarity, and completeness on a graded A-F scale.
Assistant
{{FEEDBACK}}	1. Accuracy: A
- Correctly reported key stats: HbA1c reductions, weight loss.
- Accurately noted safety profile and lack of serious adverse events.
- Assertion of “game-changer” is justified by significantly better outcomes.

2. Clarity: A-
- Concise and well-structured with clear sections.
- Good use of comparative language (e.g., “up 3% from Q1”) for context.
- Minor issue: Acronyms like “RCT,” “GLP-1,” and “GIP” not defined. Consider spelling out first use.

3. Completeness: B+
- Covers all requested areas: methodology, findings, clinical implications.
- Missing: Baseline characteristics of groups (were they well-matched?).
- Missing: Any secondary endpoints (e.g., blood pressure, lipids).
- Consider: Long-term implications or need for further research.

Overall, the summary is accurate and clear, providing key insights for a quick grasp of the study. To elevate to A+, define acronyms, mention group baseline stats, note any secondary outcomes, and briefly touch on the need for long-term studies.
Prompt 3
Role	Content
User	Your task is to improve a paper summary given feedback. Here is the first draft of a medical research paper:
<summary>
{{SUMMARY}}
</summary>

Here is the research paper:
<paper>
{{RESEARCH_PAPER}}
</paper>

Here is the feedback:
<feedback>
{{FEEDBACK}}
</feedback>

Update the summary based on the feedback.
Assistant	Methodology: Double-blind randomized controlled trial (RCT) with 500 Type 2 diabetes patients. Group A (n=250) received a novel glucagon-like peptide-1/glucose-dependent insulinotropic polypeptide (GLP-1/GIP) dual agonist; Group B (n=250) received a standard GLP-1 agonist. The 24-week trial’s primary endpoint was HbA1c reduction. Groups were well-matched for age, BMI, and baseline HbA1c.

Findings:
- Group A: 1.8% mean HbA1c reduction
- Group B: 1.1% mean HbA1c reduction
- 68% of Group A vs. 52% of Group B achieved HbA1c <7%
- Group A: 5.2kg mean weight loss
- Group B: 3.1kg mean weight loss

Secondary endpoints showed improvements in blood pressure and lipid profiles for both groups, with Group A showing slightly better results.

Safety: Similar adverse event profiles, with no serious events attributed to either treatment.

Clinical Implications: The dual GLP-1/GIP agonist demonstrates superior glycemic control and weight loss compared to standard GLP-1 therapy. These results suggest it could be a significant advancement, particularly for obese diabetics. However, longer-term studies are needed to confirm sustained efficacy and safety, as well as to investigate potential benefits on cardiovascular outcomes.
​
Examples

Example: Analyzing a legal contract (without chaining)

Note that Claude misses the instruction to provide proposed changes in its email draft.

Role	Content
User	You’re our Chief Legal Officer. Review this SaaS contract for risks, focusing on data privacy, SLAs, and liability caps.

<contract>
{{CONTRACT}}
</contract>

Then draft an email to the vendor with your concerns and proposed changes.
Assistant	After reviewing the SaaS contract, I’ve identified several concerns:

1. Data Privacy (Section 7):
- Issue: Vendor has broad rights to use client data for “service improvement.”
- Risk: Could lead to unauthorized data sharing or monetization.

2. SLAs (Section 5):
- Issue: 99.5% uptime guarantee is below industry standard.
- Risk: Excessive downtime could impact our operations.

3. Liability (Section 9):
- Issue: Cap set at 3 months of fees.
- Risk: Inadequate for potential data breaches or service failures.

Draft email:

Dear {{VENDOR}},

After reviewing the SaaS agreement, we have concerns:

1. Data Privacy: We require that our data only be used for direct service provision, not broad “service improvement.” Please amend Section 7.

2. SLAs: 99.5% uptime is insufficient. We need 99.99% with clearly defined penalties. Please revise Section 5.

3. Liability: The current cap of 3 months’ fees is inadequate. We propose a cap of 12 months’ fees, with no cap for data breaches or willful misconduct.

We value our potential partnership and believe these changes will set us up for success. Please send a revised agreement.

Best regards,
{{CLO_NAME}}

Example: Analyzing a legal contract (with chaining)

Prompt 1
Role	Content
User	You’re our Chief Legal Officer. Review this SaaS contract for risks, focusing on data privacy, SLAs, and liability caps.

<contract>
{{CONTRACT}}
</contract>

Output your findings in <risks> tags.
Assistant
{{CONCERNS}}	1. Data Privacy (Section 7):
- Issue: Vendor has broad rights to use client data for “service improvement.”
- Risk: Could lead to unauthorized data sharing or monetization of client data.
- Impact: Potential breach of client trust, regulatory fines (GDPR, CCPA).

2. SLAs (Section 5):
- Issue: 99.5% uptime guarantee is below industry standard (typically 99.99%).
- Risk: Up to 43.8 hours of downtime per year vs. 52.6 minutes at 99.99%.
- Impact: Operational disruptions, revenue loss, client dissatisfaction.

3. Liability (Section 9):
- Issue: Liability cap set at 3 months of fees.
- Risk: Inadequate for potential data breaches, IP theft, or prolonged service failures.
- Impact: Insufficient compensation for damages that could far exceed the cap, like class-action lawsuits or regulatory fines.
Prompt 2
Role	Content
User	Draft an email to a SaaS product vendor outlining the following concerns and proposing changes. Here are the concerns:
<concerns>
{{CONCERNS}}
</concerns>

Assistant
{{EMAIL}}	Dear {{VENDOR}},

After a thorough review of the proposed SaaS agreement, our legal team has identified several high-priority concerns that must be addressed before we can proceed:

1. Data Privacy (Section 7):
- Current: Broad rights to use client data for “service improvement.”
- Concern: Potential unauthorized data sharing or monetization.
- Proposed Change: Limit data use to direct service provision. Any use for improvement, analytics, or marketing must be opt-in, revocable, and anonymized.

2. SLAs (Section 5):
- Current: 99.5% uptime guarantee.
- Concern: Up to 43.8 hours of downtime per year, risking operational disruptions.
- Proposed Change: 99.99% uptime with tiered service credits. E.g., 10% credit for <99.99%, 25% for <99.9%, 50% for <99.5%.

3. Liability (Section 9):
- Current: Cap at 3 months of fees.
- Concern: Grossly inadequate for data breaches, IP theft, or prolonged outages.
- Proposed Change: 12 months’ fees for general liability. Uncapped for data breaches, willful misconduct, IP infringement.

These changes are non-negotiable for us. They align with industry standards and our commitment to data security, operational excellence, and fiduciary responsibility. We value the potential of our partnership, and addressing these will set a strong foundation.

Please have your legal team review and incorporate these changes. We’re happy to discuss further to ensure mutual understanding and agreement.

Best regards,
{{CLO_NAME}}
Prompt 3
Role	Content
User	Your task is to review an email and provide feedback. Here is the email:
<email>
{{EMAIL}}
</email>

Give feedback on tone, clarity, and professionalism.
Assistant	1. Tone:
- Professional and assertive without being confrontational.
- Use of “non-negotiable” is strong but justified given the criticality of the issues.
- Collaborative language (“we value,” “happy to discuss”) balances the firmness, maintaining a constructive tone.

2. Clarity:
- Excellent structuring: Current state, concern, and proposed change for each issue.
- Technical terms (e.g., “EBITDA,” “tiered service credits”) are appropriate for a B2B SaaS context.
- Action items are clear: vendor’s legal team to review and incorporate changes.

3. Professionalism:
- Focuses on mutual benefit and industry standards, not just our demands.
- Mentions broader responsibilities (data security, operational excellence, fiduciary duty), elevating the discussion.
- Offers further discussion, showing commitment to understanding and resolving issues.

Overall, the email strikes the right balance of assertiveness and collaboration. It clearly articulates our concerns, proposes specific solutions, and maintains a professional, partnership-oriented tone. No revisions needed; proceed with sending.

Example: Multitenancy strategy review

Prompt 1
Role	Content
User	As a senior solutions architect, review and analyze this multitenancy strategy for our new enterprise SaaS product.

<strategy>
{{STRATEGY}}
</strategy>

Focus on scalability, security, and cost-effectiveness.
Assistant
{{ANALYSIS}}	1. Scalability:
+ Database sharding by tenant reduces contention, supports high growth.
+ Microservices allow independent scaling of components.
- Shared app servers could bottleneck under extreme load.
- No mention of auto-scaling or load balancing for app tier.

2. Security:
+ Tenant data isolation via sharding mitigates data leaks.
+ JWT with short expiry and refresh tokens is industry best practice.
- No mention of encryption (at rest, in transit) or key management.
- IAM strategy lacks detail on roles, permissions, or RBAC.

3. Cost-effectiveness:
+ Shared app servers reduce idle resource costs.
+ Serverless functions for infrequent tasks optimize costs.
- Per-tenant DB could increase costs at scale vs. optimized multi-tenant DB.
- No discussion of reserved instances or spot instances for cost saving.
Prompt 2
Role	Content
User	Draft a strategy review document for engineering leadership based on this analysis of a multitenancy strategy.

<strategy>
{{STRATEGY}}
</strategy>

<analysis>
{{ANALYSIS}}
</analysis>

Include an executive summary, detailed analysis, and recommendations.
Assistant
{{STRATEGY_DOC}}	# Multitenancy Strategy Review: SaaS Enterprise Product

## Executive Summary
The proposed strategy demonstrates a solid foundation in scalability and security principles but has gaps in implementation details. It excels in data isolation and component-level scaling but lacks robust app-tier scaling and advanced security measures. Cost optimizations are present but may falter at high scale. With targeted enhancements, this strategy can effectively support our enterprise SaaS ambitions.

## Detailed Analysis

### 1. Scalability: B+

Strengths:
- Database sharding by tenant is excellent. It minimizes query latency, supports high data growth, and facilitates seamless per-tenant scaling.
- Microservices architecture allows scaling individual components based on demand, crucial for handling diverse enterprise workloads.

Weaknesses:
- Shared application servers risk becoming a bottleneck. As tenant count and traffic grow, these could degrade performance across all tenants.
- Absence of auto-scaling and load balancing for the app tier is a critical oversight. These are essential for handling traffic spikes without over-provisioning.

### 2. Security: B-

Strengths:
- Sharded databases provide strong tenant data isolation, mitigating risks of data leaks or unauthorized access between tenants.
- JWT with short expiry and refresh tokens align with OWASP best practices, reducing token vulnerability windows.

Weaknesses:
- No mention of data encryption at rest or in transit (TLS 1.3). This is non-negotiable for enterprise data.
- Key management strategy is missing. How are encryption keys, JWT secrets, and DB credentials managed and rotated?
- IAM lacks depth. We need RBAC with well-defined roles, attribute-based access control (ABAC) for fine-grained permissions, and integration with enterprise identity providers (SAML, OIDC).

### 3. Cost-effectiveness: B

Strengths:
- Shared app servers reduce idle capacity costs, good for steady-state traffic.
- Serverless for infrequent tasks (e.g., reporting) is smart; pay only when these resources are used.

Weaknesses:
- Per-tenant databases may not be cost-effective at scale compared to a well-designed multi-tenant database with robust data partitioning.
- No mention of cost-saving measures like reserved instances for predictable workloads or spot instances for fault-tolerant tasks.
- Lack of app-tier auto-scaling could lead to over-provisioning, increasing costs.

## Recommendations

1. App Tier Resilience: Implement auto-scaling with Kubernetes or cloud-native services (EKS, GKE). Deploy across availability zones with a CDN for static assets.

2. Security Hardening:
a. Encrypt data at rest (AES-256) and in transit (TLS 1.3).
b. Use a managed KMS (AWS KMS, GCP KMS) for key management.
c. Enhance IAM with RBAC, ABAC, and SAML/OIDC for enterprise SSO.

3. Cost Optimization:
a. Evaluate columnar multi-tenant DB (e.g., Snowflake) vs. sharding at scale.
b. Use reserved instances for app servers, spot instances for non-critical workloads.
c. Implement proactive monitoring and right-sizing to avoid overprovisioning.

4. Proof of Concept (PoC): Before full implementation, conduct a PoC with a high-scale simulation:
a. Test sharded DB vs. optimized multi-tenant DB for cost and performance.
b. Stress-test app tier with simulated enterprise-level traffic spikes.
c. Conduct third-party security audit, focusing on data isolation and access controls.

With these enhancements, our multi-tenancy strategy will be enterprise-grade: scalable, secure, and cost-effective. The PoC will validate our design choices and instill confidence in our enterprise clients.
Prompt 3
Role	Content
User	Grade this strategy review document for clarity, actionability, and alignment with enterprise priorities.

<priorities>
{{PRIORITIES}}
</priorities>

<strategy_doc>
{{STRATEGY_DOC}}
</strategy_doc>
Assistant	1. Clarity: A
+ Clear structure with Executive Summary, Detailed Analysis, and Recommendations.
+ Analysis uses a grading system (B+, B-) for quick grasp of strengths/weaknesses.
+ Technical terms (RBAC, ABAC, TLS) are explained or contextualized, aiding non-technical leadership.
+ Visual aids (bullet points, indentation) enhance readability.

2. Actionability: A-
+ Recommendations are specific and technical (e.g., “Implement auto-scaling with Kubernetes”).
+ PoC plan with clear objectives provides a roadmap.
+ Missing: Timelines, resource requirements, or team assignments for each recommendation.
+ Consider: Add a “Next Steps” section with immediate actions, roles, and a high-level timeline.

3. Enterprise Alignment: A+
+ Directly addresses enterprise priorities:
- Scalability: Discusses handling enterprise-level traffic spikes.
- Security: Focuses on data isolation, encryption, and enterprise IAM.
- Cost: Strategies align with enterprise FinOps (reserved instances, right-sizing).
+ PoC with “high-scale simulation” and security audit shows commitment to enterprise-grade quality.
+ Language resonates with enterprise concerns: “unacceptable risk,” “enterprise-grade,” “instill confidence.”

Overall, the document excels in clarity and enterprise alignment. The actionability is strong but could be elevated to A+ with a “Next Steps” section. The technical depth, coupled with clear explanations, makes this highly effective for both technical and non-technical leadership. Great work!

---

Long context prompting tips
Claude’s extended context window (200K tokens for Claude 3 models) enables handling complex, data-rich tasks. This guide will help you leverage this power effectively.

​
Essential tips for long context prompts
Put longform data at the top: Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above your query, instructions, and examples. This can significantly improve Claude’s performance across all models.

Queries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.
Structure document content and metadata with XML tags: When using multiple documents, wrap each document in <document> tags with <document_content> and <source> (and other metadata) subtags for clarity.


Example multi-document structure


<documents>
  <document index="1">
    <source>annual_report_2023.pdf</source>
    <document_content>
      {{ANNUAL_REPORT}}
    </document_content>
  </document>
  <document index="2">
    <source>competitor_analysis_q2.xlsx</source>
    <document_content>
      {{COMPETITOR_ANALYSIS}}
    </document_content>
  </document>
</documents>

Analyze the annual report and competitor analysis. Identify strategic advantages and recommend Q3 focus areas.
Ground responses in quotes: For long document tasks, ask Claude to quote relevant parts of the documents first before carrying out its task. This helps Claude cut through the “noise” of the rest of the document’s contents.


Example quote extraction


You are an AI physician's assistant. Your task is to help doctors diagnose possible patient illnesses.

<documents>
  <document index="1">
    <source>patient_symptoms.txt</source>
    <document_content>
      {{PATIENT_SYMPTOMS}}
    </document_content>
  </document>
  <document index="2">
    <source>patient_records.txt</source>
    <document_content>
      {{PATIENT_RECORDS}}
    </document_content>
  </document>
  <document index="3">
    <source>patient01_appt_history.txt</source>
    <document_content>
      {{PATIENT01_APPOINTMENT_HISTORY}}
    </document_content>
  </document>
</documents>

Find quotes from the patient records and appointment history that are relevant to diagnosing the patient's reported symptoms. Place these in <quotes> tags. Then, based on these quotes, list all information that would help the doctor diagnose the patient's symptoms. Place your diagnostic information in <info> tags.

---

 let's recap some of our most important prompt engineering techniques.

1. Be clear and direct
What it is:
Without a doubt, the most important prompting technique is also the simplest: write explicit, detailed instructions that leave no room for ambiguity. This means specifying desired output formats, lengths, and styles, and not assuming Claude has context about your use case. It's often easier said than done, but it's worth getting right.

Why it matters:
Clarity and precision are paramount. Ambiguous or vague prompts can lead to misunderstandings, wasted time, and potentially costly errors. By being clear and direct, you ensure that Claude understands your exact requirements, reducing the need for back-and-forth clarifications and increasing overall prompt effectiveness.

Bad example: Analyzing customer feedback
Imagine you're a product manager who wants to categorize and summarize customer feedback for a new software release.

Role	Content
User	Here's some customer feedback. Can you tell me what people think?
{{CUSTOMER_FEEDBACK}}
This prompt is vague and lacks specific instructions. Claude might provide a general summary, but it may not include the structured insights you need for decision-making.

Good example: Analyzing customer feedback
Role	Content
User	I need you to analyze this customer feedback for our recent software release:
<feedback>{{CUSTOMER_FEEDBACK}}</feedback>

Please provide a detailed report with the following sections:

1. Summary (50-100 words): Concise overview of the general sentiment and main themes.
2. Feature Analysis:
- List top 3 most praised features (bullet points)
- List top 3 most criticized features (bullet points)
3. User Experience Issues:
- List top 3 reported usability problems (bullet points)
- For each issue, suggest a potential fix (in parentheses)
4. Sentiment Breakdown:
- Positive: X%
- Neutral: Y%
- Negative: Z%
5. Actionable Insights (3-5 bullet points): Key takeaways and recommended actions based on the feedback.

Use XML tags to structure your response for easy parsing:
<summary></summary>
<feature_analysis></feature_analysis>
<ux_issues></ux_issues>
<sentiment></sentiment>
<insights></insights>
This prompt is clear and direct because it:

Specifies the exact nature of the input (customer feedback for a software release)
Outlines the desired output format (summary, bullet points, numbered lists)
Sets explicit word limits (50-100 words for the summary)
Provides a detailed structure for the response
The prompt generator can only help so much here. Before writing a prompt, it's critical to plan out your exact requirements. The Prompt Generator can suggest potential prompt instructions, but it's best to start by formulating your requirements before turning to the Prompt Generator for help.

Key takeaways
When to use this technique
Always, but especially for complex or critical tasks
When you need very specific outputs
When dealing with multi-step processes
Problems it solves:
Misinterpretation of instructions
Vague or irrelevant responses
Incomplete task execution
2. Structure prompts with XML
What it is:
Use XML tags (like <tag></tag>) to wrap and delineate different parts of your prompt, such as instructions, input data, or examples. This technique helps organize complex prompts with multiple components.

Why it matters:
It's important to note that writing effective Claude prompts does not require the use of XML tags. In general, complex prompts are challenging because they usually blend instructions and external data you inject into a single, unstructured text string. When these elements are combined in a single prompt, it becomes challenging for the model to differentiate between your instructions and the input data, leading to confusion.

XML tags offer a solution to this problem by providing a way to separate data from instructions within prompts. We like to use XML tags because they are short and informative, but you could come up with your own unique system of structuring a prompt. What matters is that you use some sort of syntax to separate the parts of a complex prompt. Throughout this course we'll use XML tags, as it's the most "Claude-y" approach.

Bad example: Product defect analysis
You're a quality assurance manager at an electronics manufacturer. After a surge in customer complaints, you need to analyze defect reports for your new smartwatch:

Role	Content
User	Here's a summary of defect reports for the SmartTime 3000 smartwatch: Manufacturing quality issues - 30% of units. Battery life only 12 hours vs advertised 48 hours. Health tracking data inaccurate by 25%. Software bugs causing app crashes.

Here's our current inventory: 50,000 units in the warehouse, 100,000 in transit from suppliers.

Retail price is 299. Production cost is 120 per unit.

Analyze the defects, their impact on our brand, and recommend actions.
This prompt is problematic because:

The data, instructions, and expected output format are all mixed together.
Claude might misinterpret parts of the input data as instructions or miss key data points.
Without a specified structure for the output, Claude's response might be difficult to parse or integrate into management reports.
Good example: Product defect analysis
Now, let's structure the prompt using XML tags:

Role	Content
User	I need you to analyze the quality issues with our SmartTime 3000 smartwatch and recommend actions. Here's the data:

<defect_report>
- Manufacturing quality issues: 30% of units affected
- Battery life: 12 hours (advertised: 48 hours)
- Health tracking data: 25% inaccuracy
- Software: Multiple app crashes reported
</defect_report>

<inventory>
- Warehouse stock: 50,000 units
- In transit: 100,000 units
</inventory>

<financials>
- Retail price: 299
- Production cost: 120 per unit
</financials>

Please provide a detailed report with the following sections:

1. <defect_analysis> Analyze each defect's severity and potential impact on user experience and brand reputation. </defect_analysis>
2. <financial_impact> Calculate potential losses due to returns, warranty claims, and lost sales. Consider both immediate impact and long-term brand damage. </financial_impact>
3. <action_plan> Recommend prioritized actions to address these issues. Include timelines, cost estimates, and expected outcomes. </action_plan>
This rewritten prompt is significantly improved:

Structured data: Each piece of information is wrapped in descriptive XML tags (<defect_report>, <inventory>, <financials>). This makes it crystal clear to Claude what type of information it's dealing with.
Clear response structure: The <defect_analysis>, <financial_impact>, and <action_plan> tags guide Claude to structure its response in a way that's easy for you and your team to review and act upon.
Key takeaways
When to use this technique
For complex prompts with multiple sections
When you need to clearly separate instructions from data
To organize different types of information within a prompt
Problems it solves:
Confusion between instructions and input data
Inconsistent handling of different prompt components
Difficulty in parsing or interpreting complex prompts
3. Use examples: The power of learning by demonstration
What it is:
Provide Claude with examples of the desired output format, style, or content. These examples serve as a template for Claude to follow, helping it understand exactly what sort of input to expect and what its generated outputs should look like. Examples can definitely lead to longer prompts, but they are almost always worth including in any production-quality prompt.

Why it matters:
Examples act as concrete templates, making it easier for Claude to understand and replicate the desired output. This is especially crucial in tasks that require consistent formatting, specific jargon, or adherence to industry standards. By providing examples, you reduce the likelihood of misunderstandings and ensure that Claude's output aligns with your specific needs. It's often much more efficient to just show Claude an example or two of your desired outputs rather than trying to encapsulate all the nuance with text descriptions.

Bad example: Creating a product announcement email
Imagine you're a marketing director who needs to create a series of product announcement emails for a tech company. Without examples, your prompt might look like this:

Role	Content
User	Please write a product announcement email for our new AI-powered CRM software, 'AcmeAI'. Include its key features, benefits, and a call to action. The email should be professional, engaging, and highlight how this product can transform customer relationships. Make sure to mention its AI capabilities, pricing, and availability.
While Claude will likely produce a decent email, it might not perfectly match your company's style, tone, or formatting preferences. It may also miss key elements you typically include in such emails.

Good example: Creating a product announcement email
Now, let's provide examples to guide Claude:

Role	Content
User	Please write a product announcement email for our tech company's latest innovation. Follow the style and structure of these examples:

<examples>
<example>
Subject: Introducing AcmeDataPulse: Real-time Analytics Reimagined

Dear Valued Partner,

We are thrilled to announce the launch of AcmeDataPulse, our groundbreaking real-time analytics platform designed to transform the way businesses harness data.

[Key Features]
- Live Data Streaming: Process and analyze data in real-time, reducing decision latency by up to 80%.
- AI-Driven Insights: Our proprietary machine learning algorithms uncover hidden patterns, giving you a competitive edge.
- Scalable Infrastructure: Whether it's gigabytes or petabytes, AcmeDataPulse grows with your data.

[Benefits]
- Faster Decision-Making: Turn data into actionable insights within seconds.
- Cost Efficiency: Our pay-as-you-go model means you only pay for what you use.
- Seamless Integration: REST APIs and pre-built connectors for your existing stack.

AcmeDataPulse is now available, starting at 499/month. Schedule a demo today to see how we can supercharge your data strategy.

Best regards,
The Acme Team
</example>

<example>
Subject: Elevate Your eCommerce with AcmeSmartCart Pro

Hello eCommerce Leaders,

We're excited to introduce AcmeSmartCart Pro, the next-gen shopping cart solution that's set to revolutionize online retail.

[Key Features]
- AI-powered Recommendations: Boost cross-sells by 30% with our advanced recommendation engine.
- One-Click Checkout: Reduce cart abandonment by 25% with our streamlined process.
- Multi-currency Support: Tap into global markets with automatic currency conversion.

[Benefits]
- Increased Conversions: Frictionless checkout means more sales.
- Global Reach: Sell to customers worldwide without currency hassles.
- Future-Proof: Regular AI updates keep you ahead of the curve.

Get AcmeSmartCart Pro today starting at $299/month. First 30 days are on us. Ready to upgrade? Contact sales@acme.com.

Cheers,
The eComCo Innovations Team
</example>
</examples>

Please draft a product announcement email for our new AI-powered CRM software, 'AcmeAI'. Focus on its key features, benefits, and a call to action.
In this example, the provided examples demonstrate a clear structure and tone for product announcement emails according to the the standards of other tech companies. They highlight key features and benefits, use quantifiable metrics to showcase value, include pricing information, and end with a clear call to action. Claude could use these examples to generate a similar email for AcmeAI, perfectly matching the style, structure, and content type expected in such announcements.

By providing these examples, we've ensured that Claude:

Uses a subject line that catches the reader's attention and summarizes the announcement.
Introduces the product with an emphasis on its innovative nature.
Lists key features with technical details and quantifiable benefits.
Highlights overarching benefits that appeal to business goals like customer satisfaction and sales growth.
Provides pricing information and a call-to-action.
Uses a professional yet engaging tone throughout.
This structured approach makes the generated email ready to use with minimal edits, saving time and ensuring consistency across all product announcements.

To get the most out of using examples in your prompts, consider the following guidelines on how to provide the most effective examples:

Relevance: Ensure that your examples closely resemble the types of inputs and outputs you expect Claude to handle. The more similar the examples are to your actual use case, the better Claude will perform.
Diversity: Include a variety of examples that cover different scenarios, edge cases, and potential challenges. This helps Claude generalize better and handle a wider range of inputs.
Quantity: While there’s no hard rule for the optimal number of examples, aim to provide at least 3-5 examples to start to give Claude a solid foundation. You can always add more targeted examples if Claude’s performance isn’t meeting your expectations. Remember that even a single example is better than zero examples.
Key Takeaways
When to use this technique
To demonstrate desired output format or style
When explaining complex or nuanced tasks
To improve consistency across various inputs
Problems it solves
Inconsistency in responses
Misunderstanding of desired output format
Difficulty with unfamiliar or complex task structures
4. Let Claude think: Enhancing analysis and problem-solving
What it is:
"Let Claude think" or "chain of thought prompting" is a technique where you explicitly instruct Claude to break down complex problems or questions into a series of logical steps, articulate its reasoning at each step, and then use that reasoning to provide a final answer or solution. It's akin to asking a colleague to "think out loud" while solving a problem.

Why it matters:
When writing high-stakes prompts, the process of arriving at a result is often as important as the decision itself. When Claude shows its work, we can get:

Increased accuracy: By breaking down complex problems into steps, Claude is less likely to make logical leaps or assumptions that could lead to errors.
Enhanced decision-making: We as human developers can review Claude's reasoning process, understand how it arrived at a conclusion, and make more informed troubleshooting or prompt improvement decisions.
Risk mitigation: In fields like finance, law, or healthcare, understanding the logic behind a recommendation is crucial for assessing potential risks and liabilities.
Accountability and transparency: In case of audits or reviews, you have a clear record of how decisions were made, which is vital for governance and compliance.
Bad example: Market entry strategy
Imagine you're a CEO considering expanding your successful U.S.-based software company into the Asian market. Without prompting for a step-by-step analysis, your query might look like this:

Role	Content
User	Our software company, AcmeFlow, has seen great success in the U.S. with our project management platform. We're considering expanding into Asia. Is this a good move? Provide a report for the board.
This prompt is too vague. Claude might give a general response that lacks the depth and specificity needed for a critical business decision like international expansion.

Good example: Market entry strategy
Now, let's use the "Let Claude think" technique to guide a more thorough analysis. There are two main ways of achieving this:

Explicitly tell Claude to "think step by step" in your prompt (be sure to include some extra guidance, instructions, or considerations to help Claude asses how it should think). Claude will then break down its thought process before providing a final answer.
Provide Claude with a place to put its thoughts before generating an answer. Consider using XML tags like <thinking> and <answer>. You can instruct Claude to place its thought process inside <thinking> tags and its ultimate answer within <answer> tags. Claude will then complete its thought process within the tags and provide its final answer.
Note: In a real implementation, we would want to give Claude tools to retrieve the latest info regarding Asian markets, financial news, etc.

Let's try updating our "bad" prompt from above to use this strategy:

Role	Content
User	Our software company, AcmeFlow, has seen great success with our project management platform in the U.S. We're considering expanding into Asia. I need a comprehensive analysis to present to the board.

Here's our company data:
<company_data>
{{ACMEFLOW_DATA}}
</company_data>

Here's market research on the Asian tech sector:
<market_research>
{{ASIA_TECH_MARKET}}
</market_research>

Please analyze whether expanding into Asia is a good move for AcmeFlow. Think through your analysis before you answer, considering all factors around market potential, competition, regulatory challenges, and financial projections that might be relevant to creating a holistic and complete strategic analysis. For each step, provide your thought process within <thinking></thinking> tags. After your analysis, provide a clear recommendation for the board in <recommendation></recommendation> tags.

Remember, the board needs a well-reasoned argument to make an informed decision, so be thorough in your analysis.
Here is an example output generated by the above prompt:

<thinking>
Step 1: Analyze market potential
- AcmeFlow's U.S. revenue of 150M shows strong product-market fit in a mature market.
- Asia's tech sector is growing rapidly, with a 12% YoY growth and a 500B market size. This indicates significant potential for expansion.
- High smartphone penetration (80%) and increasing digital transformation suggest a strong demand for tech solutions like AcmeFlow's project management platform.

Step 2: Assess competition
- Only two major local competitors in Asia with a combined 30% market share. This leaves 70% of the market open, a substantial opportunity for AcmeFlow.
- Competitors lack key features like real-time collaboration and AI-driven insights, which are AcmeFlow's USPs. This differentiation could be a significant advantage.

Step 3: Evaluate regulatory challenges
- Data privacy laws in Asia are diverse and evolving. In China and Singapore, laws are stringent (similar to GDPR), while in India and Indonesia, they're still developing.
- AcmeFlow's existing compliance with GDPR and SOC 2 is advantageous, but we'll need to invest in region-specific compliance to mitigate risks. Estimated cost: 300,000-500,000.

Step 4: Project financials
- Initial investment for market entry: 2.5M (includes localization, marketing, legal fees)
- Projected first-year revenue (conservative): 2% of market share = 10B * 0.02 = 200M
- Estimated growth rate: 20% YoY for first 3 years, based on our U.S. growth and Asia's market trend
- 5-year projected revenue: 200M (Y1) * 1.2^4 ≈ 415M (Y5)
- Break-even analysis: Assuming 30% profit margin, we'll break even within the first year.
</thinking>

<recommendation>
Based on the analysis, I strongly recommend that AcmeFlow expands into the Asian market. The potential rewards far outweigh the risks.

Key points:
1. Massive market opportunity: 500B market with rapid 12% YoY growth.
2. Competitive advantage: Our real-time and AI features outshine local competitors.
3. High ROI: Initial 2.5M investment could yield 200M in first year, with strong YoY growth.
4. Manageable risks: Compliance costs are significant but manageable given the revenue potential.

The market is ripe, our product is superior, and the financials are compelling. Early entry will allow us to establish a strong foothold before the market becomes more saturated. I recommend proceeding with a phased rollout, starting with Singapore and India where we have the strongest market research data.
</recommendation>

This response demonstrates the power of the "Let Claude think":

Structured analysis: The <thinking> tags contain a step-by-step analysis that logically breaks down the decision-making process. This allows the board to follow Claude's reasoning and assess each point. This also allows us, the developers, to gain insight into Claude's thinking process and add on or revise the prompt if needed.
Key insights: Claude highlights critical factors like market size, competitive landscape, regulatory challenges, and financial projections. These insights demonstrate a deep understanding of the business context.
Clear recommendation: The <recommendation> tags contain a concise, actionable recommendation supported by key points from the analysis. The phased rollout strategy shows strategic thinking, balancing ambition with prudence. The <recommendation> tags also make it easy for us to extract the relevant "final" information for use elsewhere.
By using the "Let Claude think" technique, we've transformed what could have been a vague, one-sentence response into a comprehensive, boardroom-ready analysis. This level of detail and clarity is invaluable for high-stakes business decisions like international expansion.

Key Takeaways
When to use this technique
For complex reasoning tasks
When you need to understand Claude's logic (for debugging purposes)
To best guide Claude through multi-step problem-solving
Problems it solves
Lack of transparency in decision-making
Logical inconsistencies
Skipping steps in complex processes
5. Give Claude a role
What it is:
Claude is a highly capable assistant, but sometimes it benefits from having additional information about the role it should play in a given conversation. By assigning a role to Claude, you can prime it to respond in a specific way, improve its accuracy and performance, and tailor its tone and demeanor to match the desired context. This technique is also known as role prompting.

One option is to simply add role prompting language to your main user prompt, but we recommend putting role information in the system_prompt. Please note that role information is the only type of information we recommend putting in the system_prompt.

Why it matters:
With production prompts, consistency is key: Claude's tone, level of expertise, and role information should remain consistent. Using role prompting with the system prompt ensures Claude maintains a consistent voice and level of expertise across multiple interactions.

Role prompting is particularly useful in the following situations:

Highly technical tasks: If you need Claude to perform complex tasks related to logic, mathematics, or coding, assigning an appropriate role can help it excel at the task, even if it might have struggled without the role prompt.
Specific communication styles: When you require a particular tone or style in Claude's responses, role prompting can be an effective way to achieve the desired output.
Enhancing baseline performance: Unless you are severely limited by token count, there is rarely a reason not to use role prompting if you want to try improving Claude’s performance beyond its baseline capabilities.
Bad example: Responding to a product crisis
Your company, AcmeEV, has just discovered a critical software bug in its latest electric vehicle model that can cause unintended acceleration. Your PR team needs to draft a public statement, but they're overwhelmed. They ask Claude for help without providing much guidance:

Role	Content
User	Our new EV has a bug that can cause sudden acceleration. We need a public statement ASAP. Can you write something for us? It's a serious issue, so make sure it sounds ok.
This prompt is problematic because:

It lacks any role or context for Claude, so it might not grasp the seriousness of the situation.
"Make sure it sounds ok" is vague. Claude needs to know what "ok" means in this critical context.
There's no guidance on the tone, key messages, or target audience, which could result in a response that doesn't align with the company's crisis communication strategy.
Good example: Responding to a product crisis
Now, let's use role prompting to get a more appropriate response:

Role	Content
System	You are the Chief Communications Officer (CCO) at AcmeEV, a leading electric vehicle manufacturer known for innovation and safety. You have 20 years of experience in crisis communications, having handled issues from product recalls to CEO scandals. Your communication style is empathetic yet authoritative, always prioritizing public safety while maintaining brand integrity.
User	We've discovered a critical software bug in our new Model E that can cause unintended acceleration. The issue affects 70% of cars sold in the last quarter. Our engineering team is working on it, but the fix may take up to two weeks. Draft a public statement for immediate release.
<crisis_communication_guidelines>
1. Acknowledge the issue promptly and express concern for affected parties.
2. Clearly state the problem and its potential impact, avoiding technical jargon.
3. Outline immediate actions taken to ensure safety.
4. Provide a clear timeline for resolution and regular updates.
5. Reaffirm company values (safety, innovation) and commitment to customers.
6. Offer a direct line of communication for concerns.
7. Close with a forward-looking statement to rebuild trust.

Remember, in a crisis, speed, transparency, and empathy are key. Your words will be scrutinized by the media, customers, and shareholders alike. The goal is to protect public safety, maintain brand reputation, and set the stage for recovery.
</crisis_communication_guidelines>

Follow these steps:
1. Review the provided information and identify key facts (percentage affected, timeframe for fix).
2. Draft the statement following our crisis communication guidelines. Use a tone that balances concern with confidence.
3. Include a quote from the CEO that reinforces our commitment to safety.

Write your analysis within <analysis></analysis> tags, and your final statement within <statement></statement> tags.
Key Takeaways
When to use this technique
To set a specific context or perspective
When you need responses with particular expertise
To influence the tone or style of responses
Problems it solves
Lack of context-appropriate responses
Inconsistent tone or style
Responses not aligned with specific expertise
6. Long-context prompting
Claude’s extended context window enables it to handle complex tasks that require processing large amounts of data. When combining large chunks of information (particularly 30K+ tokens) with instructions in your prompt, it's important to structure your prompts in a way that clearly separates the input data from the instructions. We recommend using XML tags to encapsulate each document so that it's clear to Claude when the input data ends and the instructions, examples, or other parts of the prompt begin.

Additionally, we recommend putting long documents and context first in your prompt, with the instructions and examples coming later. Claude generally performs noticeably better if the documents are placed up top, above the detailed instructions or user query.

Good example: Analyzing market trends
Role	Content
User	I need a comprehensive analysis for our upcoming launch of 'AcmeAI', an AI-powered CRM. Please review these market research reports and provide insights:

<reports>
<report_1>
{{GLOBAL_TECH_TRENDS_2023}}
</report_1>
<report_2>
{{CRM_MARKET_ANALYSIS}}
</report_2>
<report_3>
{{COMPETITOR_LANDSCAPE}}
</report_3>
</reports>

Generate a detailed report with the following sections:

1. Executive Summary (100-150 words): Synthesize the key findings and their implications for AcmeAI's launch.
2. Market Opportunity:
- Global CRM market size and growth rate
- AI adoption in CRM: current and projected
- Region-wise market potential (focus on North America, Europe, and Asia-Pacific)
3. Competitor Analysis:
- Top 3 competitors' market share and growth rates
- Their AI capabilities vs. ours (use a comparison table)
- Gaps in their offerings that AcmeAI can exploit
4. Launch Strategy (timeline view):
- Q3 2024: Key milestones and marketing initiatives
- Q4 2024: Sales targets and partnership goals
- Q1 2025: Post-launch review and product roadmap
5. Risk Assessment:
- SWOT analysis focusing on AI-specific factors
- Mitigation strategies for top 3 risks

Use <section></section> tags for each main section to make the report easy to parse.
This prompt is highly effective because:

Documents up to: The long documents come first before the detailed prompt instructions.
Structured data: Each report is clearly delineated with XML tags (<report_1>, <report_2>, <report_3>), nested in a greater set of <reports> tags, making it easy for Claude to distinguish and reference different data sources.
Clear instructions: The prompt specifies exactly what Claude should extract from each report (market size, AI adoption, competitor analysis), reducing the chance of missing critical data.
XML tags for output: The <section> tags make it easy to parse Claude's response programmatically, which could be useful for integrating this output into other business processes or presentations.
Key Takeaways
When to use this technique
When dealing with large amounts of input data
Problems it solves
Difficulty handling large, complex inputs
Conclusion
These techniques form a powerful toolkit for enhancing Claude's performance across a wide range of tasks. By applying them thoughtfully, you can significantly improve the accuracy, consistency, and relevance of Claude's outputs. As you continue to work with Claude, you'll develop an intuition for which techniques to apply in different situations.

We encourage you to apply these techniques in your own projects, and to keep exploring new ways to optimize your interactions with Claude. The field of AI is rapidly evolving, and so too are the best practices for prompt engineering.

---

Lesson 3: Prompt engineering
In the first lesson, we quickly reviewed some key prompting tips. In the second lesson, we wrote a prompt that "blindly" applied all of those tips to a single prompt. Understanding these tips is critical, but it's equally important to understand the prompt engineering workflow and decision making framework.

What is prompt engineering?
Prompt engineering is the art and science of crafting effective instructions for large language models like Claude to produce desired outputs. At its core, prompt engineering involves designing, refining, and optimizing the text inputs (prompts) given to models to elicit accurate, relevant, and useful responses. It's about "communicating" with Claude in a way that maximizes the model's understanding and performance on a given task. The importance of prompt engineering cannot be overstated:

Enhancing AI capabilities: Well-engineered prompts can dramatically improve an AI's performance, enabling it to tackle complex tasks with greater accuracy and efficiency.
Bridging the gap between human intent and AI output: Prompt engineering helps translate human objectives into language that AI models can effectively interpret and act upon.
Optimizing resource usage: Skilled prompt engineering can reduce token usage, lowering costs and improving response times in production environments.
How is it different from "basic prompting"?
Let's define "basic prompting" as simply asking an AI model a question or giving it a straightforward instruction. Prompt engineering, on the other hand, is a more sophisticated and deliberate process.

Complexity: Basic prompting often involves single-turn interactions with simple queries. Prompt engineering, on the other hand, may involve multi-turn conversations, complex instructions, and carefully structured inputs and outputs.
Precision: Basic prompts might be vague or ambiguous, leading to inconsistent results. This might be fine in one-off prompt situations but won't scale to production use cases. Engineered prompts are precise, leaving little room for misinterpretation by the model.
Iterative refinement: Unlike basic prompting, which might be a one-off activity, prompt engineering involves systematic testing, analysis, and improvement of prompts over time.
Scalability: Prompt engineering aims to create prompts that can handle a wide range of inputs and use cases, making them suitable for production environments.
In essence, prompt engineering elevates the interaction with a model from a casual conversation to a carefully orchestrated exchange designed to maximize the model's potential in solving real-world problems repeatably. As we progress through this course, you'll learn the techniques and mindset needed to master this crucial skill.

The prompt engineering lifecycle
It would be nice to sit down at a blank page and craft the perfect prompt on the first try, but the reality is that prompt engineering is an iterative process that involves creating, testing, and refining prompts to achieve optimal performance.

Understanding this lifecycle is crucial for developing effective prompts and troubleshooting issues that arise.

Initial prompt creation
Testing and identifying issues
Selecting appropriate techniques
Implementing improvements
Iterating and refining
prompt_eng_lifecycle.png

Let's talk about each of these pieces in detail.

Initial prompt creation
Start by writing a "first draft" prompt that clearly articulates your end goal. Ideally, this first attempt incorporates some of the prompting techniques we've previously covered: define the objective, identify key information, structure the prompt well, etc. Either way, it's highly unlikely the final version of the prompt will resemble this first draft.

Testing and identifying issues
Now that you have a basic prompt, the next step is to test it against a variety of possible inputs. Our upcoming prompt evaluations course will cover this process in detail, but it boils down to:

Prepare test cases: create a diverse set of inputs that cover various scenarios and edge cases.
Run initial tests: Use your prompt with the prepared inputs and observe the outputs.
Analyze results: "grade" the model's responses. This can be done in a variety of ways: via code, human expertise, or using a large language model.
Throughout this testing process, key metrics to keep in mind include:

Accuracy: Are the outputs correct and relevant?
Consistency: Does the model perform similarly across different inputs? How does it handle edge cases?
Completeness: Is all required information included in the outputs?
Adherence to instructions: Does the model follow all given directions?
Selecting appropriate techniques
Once you have tested your initial prompt and have identified concrete issues, it's time to make some changes to the prompt. Instead of blindly making improvements, the ideal approach involves the following steps:

Diagnose root causes: for each identified issue, try to understand why it's occurring.
Research solutions: Based on your diagnosis, explore prompt engineering techniques that could address the problems.
Choose techniques: Select the most promising strategies to implement.
Implementing improvements
Next, actually implement the improvements in your original prompt. Modify the original prompt to incorporate the techniques that you've selected to target the previously identified problems. If you're making multiple changes, consider implementing them one at a time to better understand their individual impacts.

Iterating and refining
Repeat the above process!

Retest: Run your updated prompt through the same test cases used initially.
Compare results: Analyze how the outputs have changed and whether the targeted issues have been resolved.
Identify new issues: Look for any new problems that may have been introduced by your changes.
Repeat the cycle: Continue this process of testing, analyzing, and refining until you achieve satisfactory performance.

---

Chapter 1: Basic Prompt Structure
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt}
        ]
    )
    return message.content[0].text
     
Lesson
Anthropic offers two APIs, the legacy Text Completions API and the current Messages API. For this tutorial, we will be exclusively using the Messages API.

At minimum, a call to Claude using the Messages API requires the following parameters:

model: the API model name of the model that you intend to call

max_tokens: the maximum number of tokens to generate before stopping. Note that Claude may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate. Furthermore, this is a hard stop, meaning that it may cause Claude to stop generating mid-word or mid-sentence.

messages: an array of input messages. Our models are trained to operate on alternating user and assistant conversational turns. When creating a new Message, you specify the prior conversational turns with the messages parameter, and the model then generates the next Message in the conversation.

Each input message must be an object with a role and content. You can specify a single user-role message, or you can include multiple user and assistant messages (they must alternate, if so). The first message must always use the user role.
There are also optional parameters, such as:

system: the system prompt - more on this below.

temperature: the degree of variability in Claude's response. For these lessons and exercises, we have set temperature to 0.

For a complete list of all API parameters, visit our API documentation.

Examples
Let's take a look at how Claude responds to some correctly-formatted prompts. For each of the following cells, run the cell (shift+enter), and Claude's response will appear below the block.


# Prompt
PROMPT = "Hi Claude, how are you?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Can you tell me the color of the ocean?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "What year was Celine Dion born in?"

# Print Claude's response
print(get_completion(PROMPT))
     
Now let's take a look at some prompts that do not include the correct Messages API formatting. For these malformatted prompts, the Messages API returns an error.

First, we have an example of a Messages API call that lacks role and content fields in the messages array.


# Get Claude's response
response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        messages=[
          {"Hi Claude, how are you?"}
        ]
    )

# Print Claude's response
print(response[0].text)
     
Here's a prompt that fails to alternate between the user and assistant roles.


# Get Claude's response
response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        messages=[
          {"role": "user", "content": "What year was Celine Dion born in?"},
          {"role": "user", "content": "Also, can you tell me some other facts about her?"}
        ]
    )

# Print Claude's response
print(response[0].text)
     
user and assistant messages MUST alternate, and messages MUST start with a user turn. You can have multiple user & assistant pairs in a prompt (as if simulating a multi-turn conversation). You can also put words into a terminal assistant message for Claude to continue from where you left off (more on that in later chapters).

System Prompts
You can also use system prompts. A system prompt is a way to provide context, instructions, and guidelines to Claude before presenting it with a question or task in the "User" turn.

Structurally, system prompts exist separately from the list of user & assistant messages, and thus belong in a separate system parameter (take a look at the structure of the get_completion helper function in the Setup section of the notebook).

Within this tutorial, wherever we might utilize a system prompt, we have provided you a system field in your completions function. Should you not want to use a system prompt, simply set the SYSTEM_PROMPT variable to an empty string.

System Prompt Example

# System prompt
SYSTEM_PROMPT = "Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question."

# Prompt
PROMPT = "Why is the sky blue?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     
Why use a system prompt? A well-written system prompt can improve Claude's performance in a variety of ways, such as increasing Claude's ability to follow rules and instructions. For more information, visit our documentation on how to use system prompts with Claude.

Now we'll dive into some exercises. If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 1.1 - Counting to Three
Exercise 1.2 - System Prompt
Exercise 1.1 - Counting to Three
Using proper user / assistant formatting, edit the PROMPT below to get Claude to count to three. The output will also indicate whether your solution is correct.


# Prompt - this is the only field you should change
PROMPT = "[Replace this text]"

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)
    return bool(pattern.match(text))

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_1_1_hint; print(exercise_1_1_hint)
     
Exercise 1.2 - System Prompt
Modify the SYSTEM_PROMPT to make Claude respond like it's a 3 year old child.


# System prompt - this is the only field you should change
SYSTEM_PROMPT = "[Replace this text]"

# Prompt
PROMPT = "How big is the sky?"

# Get Claude's response
response = get_completion(PROMPT, SYSTEM_PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search(r"giggles", text) or re.search(r"soo", text))

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_1_2_hint; print(exercise_1_2_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Prompt
PROMPT = "Hi Claude, how are you?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Can you tell me the color of the ocean?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "What year was Celine Dion born in?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Get Claude's response
response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        messages=[
          {"Hi Claude, how are you?"}
        ]
    )

# Print Claude's response
print(response[0].text)
     

# Get Claude's response
response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        messages=[
          {"role": "user", "content": "What year was Celine Dion born in?"},
          {"role": "user", "content": "Also, can you tell me some other facts about her?"}
        ]
    )

# Print Claude's response
print(response[0].text)
     

# System prompt
SYSTEM_PROMPT = "Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question."

# Prompt
PROMPT = "Why is the sky blue?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))

---

Chapter 2: Being Clear and Direct
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

# Note that we changed max_tokens to 4K just for this lesson to allow for longer completions in the exercises
def get_completion(prompt: str, system_prompt=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=4000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt}
        ]
    )
    return message.content[0].text
     
Lesson
Claude responds best to clear and direct instructions.

Think of Claude like any other human that is new to the job. Claude has no context on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to Claude, the better and more accurate Claude's response will be." When in doubt, follow the Golden Rule of Clear Prompting:

Show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the result you want. If they're confused, Claude's confused.
Examples
Let's take a task like writing poetry. (Ignore any syllable mismatch - LLMs aren't great at counting syllables yet.)


# Prompt
PROMPT = "Write a haiku about robots."

# Print Claude's response
print(get_completion(PROMPT))
     
This haiku is nice enough, but users may want Claude to go directly into the poem without the "Here is a haiku" preamble.

How do we achieve that? We ask for it!


# Prompt
PROMPT = "Write a haiku about robots. Skip the preamble; go straight into the poem."

# Print Claude's response
print(get_completion(PROMPT))
     
Here's another example. Let's ask Claude who's the best basketball player of all time. You can see below that while Claude lists a few names, it doesn't respond with a definitive "best".


# Prompt
PROMPT = "Who is the best basketball player of all time?"

# Print Claude's response
print(get_completion(PROMPT))
     
Can we get Claude to make up its mind and decide on a best player? Yes! Just ask!


# Prompt
PROMPT = "Who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?"

# Print Claude's response
print(get_completion(PROMPT))
     
If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 2.1 - Spanish
Exercise 2.2 - One Player Only
Exercise 2.3 - Write a Story
Exercise 2.1 - Spanish
Modify the SYSTEM_PROMPT to make Claude output its answer in Spanish.


# System prompt - this is the only field you should change
SYSTEM_PROMPT = "[Replace this text]"

# Prompt
PROMPT = "Hello Claude, how are you?"

# Get Claude's response
response = get_completion(PROMPT, SYSTEM_PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return "hola" in text.lower()

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_2_1_hint; print(exercise_2_1_hint)
     
Exercise 2.2 - One Player Only
Modify the PROMPT so that Claude doesn't equivocate at all and responds with ONLY the name of one specific player, with no other words or punctuation.


# Prompt - this is the only field you should change
PROMPT = "[Replace this text]"

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return text == "Michael Jordan"

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_2_2_hint; print(exercise_2_2_hint)
     
Exercise 2.3 - Write a Story
Modify the PROMPT so that Claude responds with as long a response as you can muster. If your answer is over 800 words, Claude's response will be graded as correct.


# Prompt - this is the only field you should change
PROMPT = "[Replace this text]"

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    trimmed = text.strip()
    words = len(trimmed.split())
    return words >= 800

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_2_3_hint; print(exercise_2_3_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Prompt
PROMPT = "Write a haiku about robots."

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Write a haiku about robots. Skip the preamble; go straight into the poem."

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Who is the best basketball player of all time?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?"

# Print Claude's response
print(get_completion(PROMPT))

---

Chapter 3: Assigning Roles (Role Prompting)
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt}
        ]
    )
    return message.content[0].text
     
Lesson
Continuing on the theme of Claude having no context aside from what you say, it's sometimes important to prompt Claude to inhabit a specific role (including all necessary context). This is also known as role prompting. The more detail to the role context, the better.

Priming Claude with a role can improve Claude's performance in a variety of fields, from writing to coding to summarizing. It's like how humans can sometimes be helped when told to "think like a ______". Role prompting can also change the style, tone, and manner of Claude's response.

Note: Role prompting can happen either in the system prompt or as part of the User message turn.

Examples
In the example below, we see that without role prompting, Claude provides a straightforward and non-stylized answer when asked to give a single sentence perspective on skateboarding.

However, when we prime Claude to inhabit the role of a cat, Claude's perspective changes, and thus Claude's response tone, style, content adapts to the new role.

Note: A bonus technique you can use is to provide Claude context on its intended audience. Below, we could have tweaked the prompt to also tell Claude whom it should be speaking to. "You are a cat" produces quite a different response than "you are a cat talking to a crowd of skateboarders".

Here is the prompt without role prompting in the system prompt:


# Prompt
PROMPT = "In one sentence, what do you think about skateboarding?"

# Print Claude's response
print(get_completion(PROMPT))
     
Here is the same user question, except with role prompting.


# System prompt
SYSTEM_PROMPT = "You are a cat."

# Prompt
PROMPT = "In one sentence, what do you think about skateboarding?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     
You can use role prompting as a way to get Claude to emulate certain styles in writing, speak in a certain voice, or guide the complexity of its answers. Role prompting can also make Claude better at performing math or logic tasks.

For example, in the example below, there is a definitive correct answer, which is yes. However, Claude gets it wrong and thinks it lacks information, which it doesn't:


# Prompt
PROMPT = "Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?"

# Print Claude's response
print(get_completion(PROMPT))
     
Now, what if we prime Claude to act as a logic bot? How will that change Claude's answer?

It turns out that with this new role assignment, Claude gets it right. (Although notably not for all the right reasons)


# System prompt
SYSTEM_PROMPT = "You are a logic bot designed to answer complex logic problems."

# Prompt
PROMPT = "Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     
Note: What you'll learn throughout this course is that there are many prompt engineering techniques you can use to derive similar results. Which techniques you use is up to you and your preference! We encourage you to experiment to find your own prompt engineering style.

If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 3.1 - Math Correction
Exercise 3.1 - Math Correction
In some instances, Claude may struggle with mathematics, even simple mathematics. Below, Claude incorrectly assesses the math problem as correctly solved, even though there's an obvious arithmetic mistake in the second step. Note that Claude actually catches the mistake when going through step-by-step, but doesn't jump to the conclusion that the overall solution is wrong.

Modify the PROMPT and / or the SYSTEM_PROMPT to make Claude grade the solution as incorrectly solved, rather than correctly solved.


# System prompt - if you don't want to use a system prompt, you can leave this variable set to an empty string
SYSTEM_PROMPT = ""

# Prompt
PROMPT = """Is this equation solved correctly below?

2x - 3 = 9
2x = 6
x = 3"""

# Get Claude's response
response = get_completion(PROMPT, SYSTEM_PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    if "incorrect" in text or "not correct" in text.lower():
        return True
    else:
        return False

# Print Claude's response and the corresponding grade
print(response)
print("\n--------------------------- GRADING ---------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_3_1_hint; print(exercise_3_1_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Prompt
PROMPT = "In one sentence, what do you think about skateboarding?"

# Print Claude's response
print(get_completion(PROMPT))
     

# System prompt
SYSTEM_PROMPT = "You are a cat."

# Prompt
PROMPT = "In one sentence, what do you think about skateboarding?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     

# Prompt
PROMPT = "Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?"

# Print Claude's response
print(get_completion(PROMPT))
     

# System prompt
SYSTEM_PROMPT = "You are a logic bot designed to answer complex logic problems."

# Prompt
PROMPT = "Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don’t know if Anne is married. Is a married person looking at an unmarried person?"

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))

---

Chapter 4: Separating Data and Instructions
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt}
        ]
    )
    return message.content[0].text
     
Lesson
Oftentimes, we don't want to write full prompts, but instead want prompt templates that can be modified later with additional input data before submitting to Claude. This might come in handy if you want Claude to do the same thing every time, but the data that Claude uses for its task might be different each time.

Luckily, we can do this pretty easily by separating the fixed skeleton of the prompt from variable user input, then substituting the user input into the prompt before sending the full prompt to Claude.

Below, we'll walk step by step through how to write a substitutable prompt template, as well as how to substitute in user input.

Examples
In this first example, we're asking Claude to act as an animal noise generator. Notice that the full prompt submitted to Claude is just the PROMPT_TEMPLATE substituted with the input (in this case, "Cow"). Notice that the word "Cow" replaces the ANIMAL placeholder via an f-string when we print out the full prompt.

Note: You don't have to call your placeholder variable anything in particular in practice. We called it ANIMAL in this example, but just as easily, we could have called it CREATURE or A (although it's generally good to have your variable names be specific and relevant so that your prompt template is easy to understand even without the substitution, just for user parseability). Just make sure that whatever you name your variable is what you use for the prompt template f-string.


# Variable content
ANIMAL = "Cow"

# Prompt template with a placeholder for the variable content
PROMPT = f"I will tell you the name of an animal. Please respond with the noise that animal makes. {ANIMAL}"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
Why would we want to separate and substitute inputs like this? Well, prompt templates simplify repetitive tasks. Let's say you build a prompt structure that invites third party users to submit content to the prompt (in this case the animal whose sound they want to generate). These third party users don't have to write or even see the full prompt. All they have to do is fill in variables.

We do this substitution here using variables and f-strings, but you can also do it with the format() method.

Note: Prompt templates can have as many variables as desired!

When introducing substitution variables like this, it is very important to make sure Claude knows where variables start and end (vs. instructions or task descriptions). Let's look at an example where there is no separation between the instructions and the substitution variable.

To our human eyes, it is very clear where the variable begins and ends in the prompt template below. However, in the fully substituted prompt, that delineation becomes unclear.


# Variable content
EMAIL = "Show up at 6am tomorrow because I'm the CEO and I say so."

# Prompt template with a placeholder for the variable content
PROMPT = f"Yo Claude. {EMAIL} <----- Make this email more polite but don't change anything else about it."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
Here, Claude thinks "Yo Claude" is part of the email it's supposed to rewrite! You can tell because it begins its rewrite with "Dear Claude". To the human eye, it's clear, particularly in the prompt template where the email begins and ends, but it becomes much less clear in the prompt after substitution.

How do we solve this? Wrap the input in XML tags! We did this below, and as you can see, there's no more "Dear Claude" in the output.

XML tags are angle-bracket tags like <tag></tag>. They come in pairs and consist of an opening tag, such as <tag>, and a closing tag marked by a /, such as </tag>. XML tags are used to wrap around content, like this: <tag>content</tag>.

Note: While Claude can recognize and work with a wide range of separators and delimeters, we recommend that you use specifically XML tags as separators for Claude, as Claude was trained specifically to recognize XML tags as a prompt organizing mechanism. Outside of function calling, there are no special sauce XML tags that Claude has been trained on that you should use to maximally boost your performance. We have purposefully made Claude very malleable and customizable this way.


# Variable content
EMAIL = "Show up at 6am tomorrow because I'm the CEO and I say so."

# Prompt template with a placeholder for the variable content
PROMPT = f"Yo Claude. {EMAIL} <----- Make this email more polite but don't change anything else about it."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
Let's see another example of how XML tags can help us.

In the following prompt, Claude incorrectly interprets what part of the prompt is the instruction vs. the input. It incorrectly considers Each is about an animal, like rabbits to be part of the list due to the formatting, when the user (the one filling out the SENTENCES variable) presumably did not want that.


# Variable content
SENTENCES = """- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs"""

# Prompt template with a placeholder for the variable content
PROMPT = f"""Below is a list of sentences. Tell me the second item on the list.

- Each is about an animal, like rabbits.
{SENTENCES}"""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
To fix this, we just need to surround the user input sentences in XML tags. This shows Claude where the input data begins and ends despite the misleading hyphen before Each is about an animal, like rabbits.


# Variable content
SENTENCES = """- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs"""

# Prompt template with a placeholder for the variable content
PROMPT = f""" Below is a list of sentences. Tell me the second item on the list.

- Each is about an animal, like rabbits.

{SENTENCES}
"""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
Note: In the incorrect version of the "Each is about an animal" prompt, we had to include the hyphen to get Claude to respond incorrectly in the way we wanted to for this example. This is an important lesson about prompting: small details matter! It's always worth it to scrub your prompts for typos and grammatical errors. Claude is sensitive to patterns (in its early years, before finetuning, it was a raw text-prediction tool), and it's more likely to make mistakes when you make mistakes, smarter when you sound smart, sillier when you sound silly, and so on.

If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 4.1 - Haiku Topic
Exercise 4.2 - Dog Question with Typos
Exercise 4.3 - Dog Question Part 2
Exercise 4.1 - Haiku Topic
Modify the PROMPT so that it's a template that will take in a variable called TOPIC and output a haiku about the topic. This exercise is just meant to test your understanding of the variable templating structure with f-strings.


# Variable content
TOPIC = "Pigs"

# Prompt template with a placeholder for the variable content
PROMPT = f""

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search("pigs", text.lower()) and re.search("haiku", text.lower()))

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_4_1_hint; print(exercise_4_1_hint)
     
Exercise 4.2 - Dog Question with Typos
Fix the PROMPT by adding XML tags so that Claude produces the right answer.

Try not to change anything else about the prompt. The messy and mistake-ridden writing is intentional, so you can see how Claude reacts to such mistakes.


# Variable content
QUESTION = "ar cn brown?"

# Prompt template with a placeholder for the variable content
PROMPT = f"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx"

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search("brown", text.lower()))

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_4_2_hint; print(exercise_4_2_hint)
     
Exercise 4.3 - Dog Question Part 2
Fix the PROMPT WITHOUT adding XML tags. Instead, remove only one or two words from the prompt.

Just as with the above exercises, try not to change anything else about the prompt. This will show you what kind of language Claude can parse and understand.


# Variable content
QUESTION = "ar cn brown?"

# Prompt template with a placeholder for the variable content
PROMPT = f"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx"

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search("brown", text.lower()))

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_4_3_hint; print(exercise_4_3_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Variable content
ANIMAL = "Cow"

# Prompt template with a placeholder for the variable content
PROMPT = f"I will tell you the name of an animal. Please respond with the noise that animal makes. {ANIMAL}"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     

# Variable content
EMAIL = "Show up at 6am tomorrow because I'm the CEO and I say so."

# Prompt template with a placeholder for the variable content
PROMPT = f"Yo Claude. {EMAIL} <----- Make this email more polite but don't change anything else about it."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     

# Variable content
EMAIL = "Show up at 6am tomorrow because I'm the CEO and I say so."

# Prompt template with a placeholder for the variable content
PROMPT = f"Yo Claude. {EMAIL} <----- Make this email more polite but don't change anything else about it."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     

# Variable content
SENTENCES = """- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs"""

# Prompt template with a placeholder for the variable content
PROMPT = f"""Below is a list of sentences. Tell me the second item on the list.

- Each is about an animal, like rabbits.
{SENTENCES}"""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     

# Variable content
SENTENCES = """- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs"""

# Prompt template with a placeholder for the variable content
PROMPT = f""" Below is a list of sentences. Tell me the second item on the list.

- Each is about an animal, like rabbits.

{SENTENCES}
"""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))

---

Chapter 5: Formatting Output and Speaking for Claude
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

# New argument added for prefill text, with a default value of an empty string
def get_completion(prompt: str, system_prompt="", prefill=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt},
          {"role": "assistant", "content": prefill}
        ]
    )
    return message.content[0].text
     
Lesson
Claude can format its output in a wide variety of ways. You just need to ask for it to do so!

One of these ways is by using XML tags to separate out the response from any other superfluous text. You've already learned that you can use XML tags to make your prompt clearer and more parseable to Claude. It turns out, you can also ask Claude to use XML tags to make its output clearer and more easily understandable to humans.

Examples
Remember the 'poem preamble problem' we solved in Chapter 2 by asking Claude to skip the preamble entirely? It turns out we can also achieve a similar outcome by telling Claude to put the poem in XML tags.


# Variable content
ANIMAL = "Rabbit"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Put it in  tags."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     
Why is this something we'd want to do? Well, having the output in XML tags allows the end user to reliably get the poem and only the poem by writing a short program to extract the content between XML tags.

An extension of this technique is to put the first XML tag in the assistant turn. When you put text in the assistant turn, you're basically telling Claude that Claude has already said something, and that it should continue from that point onward. This technique is called "speaking for Claude" or "prefilling Claude's response."

Below, we've done this with the first <haiku> XML tag. Notice how Claude continues directly from where we left off.


# Variable content
ANIMAL = "Cat"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Put it in  tags."

# Prefill for Claude's response
PREFILL = ""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN:")
print(PROMPT)
print("\nASSISTANT TURN:")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
Claude also excels at using other output formatting styles, notably JSON. If you want to enforce JSON output (not deterministically, but close to it), you can also prefill Claude's response with the opening bracket, {}.


# Variable content
ANIMAL = "Cat"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Use JSON format with the keys as \"first_line\", \"second_line\", and \"third_line\"."

# Prefill for Claude's response
PREFILL = "{"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
Below is an example of multiple input variables in the same prompt AND output formatting specification, all done using XML tags.


# First input variable
EMAIL = "Hi Zack, just pinging you for a quick update on that prompt you were supposed to write."

# Second input variable
ADJECTIVE = "olde english"

# Prompt template with a placeholder for the variable content
PROMPT = f"Hey Claude. Here is an email: {EMAIL}. Make this email more {ADJECTIVE}. Write the new version in <{ADJECTIVE}_email> XML tags."

# Prefill for Claude's response (now as an f-string with a variable)
PREFILL = f"<{ADJECTIVE}_email>"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
Bonus lesson
If you are calling Claude through the API, you can pass the closing XML tag to the stop_sequences parameter to get Claude to stop sampling once it emits your desired tag. This can save money and time-to-last-token by eliminating Claude's concluding remarks after it's already given you the answer you care about.

If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 5.1 - Steph Curry GOAT
Exercise 5.2 - Two Haikus
Exercise 5.3 - Two Haikus, Two Animals
Exercise 5.1 - Steph Curry GOAT
Forced to make a choice, Claude designates Michael Jordan as the best basketball player of all time. Can we get Claude to pick someone else?

Change the PREFILL variable to compell Claude to make a detailed argument that the best basketball player of all time is Stephen Curry. Try not to change anything except PREFILL as that is the focus of this exercise.


# Prompt template with a placeholder for the variable content
PROMPT = f"Who is the best basketball player of all time? Please choose one specific player."

# Prefill for Claude's response
PREFILL = ""

# Get Claude's response
response = get_completion(PROMPT, prefill=PREFILL)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search("Warrior", text))

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_5_1_hint; print(exercise_5_1_hint)
     
Exercise 5.2 - Two Haikus
Modify the PROMPT below using XML tags so that Claude writes two haikus about the animal instead of just one. It should be clear where one poem ends and the other begins.


# Variable content
ANIMAL = "cats"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Put it in  tags."

# Prefill for Claude's response
PREFILL = ""

# Get Claude's response
response = get_completion(PROMPT, prefill=PREFILL)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(
        (re.search("cat", text.lower()) and re.search("", text))
        and (text.count("\n") + 1) > 5
    )

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_5_2_hint; print(exercise_5_2_hint)
     
Exercise 5.3 - Two Haikus, Two Animals
Modify the PROMPT below so that Claude produces two haikus about two different animals. Use {ANIMAL1} as a stand-in for the first substitution, and {ANIMAL2} as a stand-in for the second substitution.


# First input variable
ANIMAL1 = "Cat"

# Second input variable
ANIMAL2 = "Dog"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL1}. Put it in  tags."

# Get Claude's response
response = get_completion(PROMPT)

# Function to grade exercise correctness
def grade_exercise(text):
    return bool(re.search("tail", text.lower()) and re.search("cat", text.lower()) and re.search("", text))

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(response)
print("\n------------------------------------------ GRADING ------------------------------------------")
print("This exercise has been correctly solved:", grade_exercise(response))
     
❓ If you want a hint, run the cell below!


from hints import exercise_5_3_hint; print(exercise_5_3_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Variable content
ANIMAL = "Rabbit"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Put it in  tags."

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print(PROMPT)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT))
     

# Variable content
ANIMAL = "Cat"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Put it in  tags."

# Prefill for Claude's response
PREFILL = ""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN:")
print(PROMPT)
print("\nASSISTANT TURN:")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     

# Variable content
ANIMAL = "Cat"

# Prompt template with a placeholder for the variable content
PROMPT = f"Please write a haiku about {ANIMAL}. Use JSON format with the keys as \"first_line\", \"second_line\", and \"third_line\"."

# Prefill for Claude's response
PREFILL = "{"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     

# First input variable
EMAIL = "Hi Zack, just pinging you for a quick update on that prompt you were supposed to write."

# Second input variable
ADJECTIVE = "olde english"

# Prompt template with a placeholder for the variable content
PROMPT = f"Hey Claude. Here is an email: {EMAIL}. Make this email more {ADJECTIVE}. Write the new version in <{ADJECTIVE}_email> XML tags."

# Prefill for Claude's response (now as an f-string with a variable)
PREFILL = f"<{ADJECTIVE}_email>"

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     

---

Chapter 6: Precognition (Thinking Step by Step)
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt="", prefill=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt},
          {"role": "assistant", "content": prefill}
        ]
    )
    return message.content[0].text
     
Lesson
If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to think through your answer first.

Guess what? Claude is the same way.

Giving Claude time to think step by step sometimes makes Claude more accurate, particularly for complex tasks. However, thinking only counts when it's out loud. You cannot ask Claude to think but output only the answer - in this case, no thinking has actually occurred.

Examples
In the prompt below, it's clear to a human reader that the second sentence belies the first. But Claude takes the word "unrelated" too literally.


# Prompt
PROMPT = """Is this movie review sentiment positive or negative?

This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900."""

# Print Claude's response
print(get_completion(PROMPT))
     
To improve Claude's response, let's allow Claude to think things out first before answering. We do that by literally spelling out the steps that Claude should take in order to process and think through its task. Along with a dash of role prompting, this empowers Claude to understand the review more deeply.


# System prompt
SYSTEM_PROMPT = "You are a savvy reader of movie reviews."

# Prompt
PROMPT = """Is this review sentiment positive or negative? First, write the best arguments for each side in  and  XML tags, then answer.

This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900."""

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     
Claude is sometimes sensitive to ordering. This example is on the frontier of Claude's ability to understand nuanced text, and when we swap the order of the arguments from the previous example so that negative is first and positive is second, this changes Claude's overall assessment to positive.

In most situations (but not all, confusingly enough), Claude is more likely to choose the second of two options, possibly because in its training data from the web, second options were more likely to be correct.


# Prompt
PROMPT = """Is this review sentiment negative or positive? First write the best arguments for each side in  and  XML tags, then answer.

This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since 1900."""

# Print Claude's response
print(get_completion(PROMPT))
     
Letting Claude think can shift Claude's answer from incorrect to correct. It's that simple in many cases where Claude makes mistakes!

Let's go through an example where Claude's answer is incorrect to see how asking Claude to think can fix that.


# Prompt
PROMPT = "Name a famous movie starring an actor who was born in the year 1956."

# Print Claude's response
print(get_completion(PROMPT))
     
Let's fix this by asking Claude to think step by step, this time in <brainstorm> tags.


# Prompt
PROMPT = "Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in  tags, then give your answer."

# Print Claude's response
print(get_completion(PROMPT))
     
If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 6.1 - Classifying Emails
Exercise 6.2 - Email Classification Formatting
Exercise 6.1 - Classifying Emails
In this exercise, we'll be instructing Claude to sort emails into the following categories:

(A) Pre-sale question
(B) Broken or defective item
(C) Billing question
(D) Other (please explain)
For the first part of the exercise, change the PROMPT to make Claude output the correct classification and ONLY the classification. Your answer needs to include the letter (A - D) of the correct choice, with the parentheses, as well as the name of the category.

Refer to the comments beside each email in the EMAILS list to know which category that email should be classified under.


# Prompt template with a placeholder for the variable content
PROMPT = """Please classify this email as either green or blue: {email}"""

# Prefill for Claude's response, if any
PREFILL = ""

# Variable content stored as a list
EMAILS = [
    "Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.", # (B) Broken or defective item
    "Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?", # (A) Pre-sale question OR (D) Other (please explain)
    "I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???", # (C) Billing question
    "How did I get here I am not good with computer.  Halp." # (D) Other (please explain)
]

# Correct categorizations stored as a list of lists to accommodate the possibility of multiple correct categorizations per email
ANSWERS = [
    ["B"],
    ["A","D"],
    ["C"],
    ["D"]
]

# Dictionary of string values for each category to be used for regex grading
REGEX_CATEGORIES = {
    "A": "A\) P",
    "B": "B\) B",
    "C": "C\) B",
    "D": "D\) O"
}

# Iterate through list of emails
for i,email in enumerate(EMAILS):
    
    # Substitute the email text into the email placeholder variable
    formatted_prompt = PROMPT.format(email=email)
   
    # Get Claude's response
    response = get_completion(formatted_prompt, prefill=PREFILL)

    # Grade Claude's response
    grade = any([bool(re.search(REGEX_CATEGORIES[ans], response)) for ans in ANSWERS[i]])
    
    # Print Claude's response
    print("--------------------------- Full prompt with variable substutions ---------------------------")
    print("USER TURN")
    print(formatted_prompt)
    print("\nASSISTANT TURN")
    print(PREFILL)
    print("\n------------------------------------- Claude's response -------------------------------------")
    print(response)
    print("\n------------------------------------------ GRADING ------------------------------------------")
    print("This exercise has been correctly solved:", grade, "\n\n\n\n\n\n")
     
❓ If you want a hint, run the cell below!


from hints import exercise_6_1_hint; print(exercise_6_1_hint)
     
Still stuck? Run the cell below for an example solution.


from hints import exercise_6_1_solution; print(exercise_6_1_solution)
     
Exercise 6.2 - Email Classification Formatting
In this exercise, we're going to refine the output of the above prompt to yield an answer formatted exactly how we want it.

Use your favorite output formatting technique to make Claude wrap JUST the letter of the correct classification in <answer></answer> tags. For instance, the answer to the first email should contain the exact string <answer>B</answer>.

Refer to the comments beside each email in the EMAILS list if you forget which letter category is correct for each email.


# Prompt template with a placeholder for the variable content
PROMPT = """Please classify this email as either green or blue: {email}"""

# Prefill for Claude's response, if any
PREFILL = ""

# Variable content stored as a list
EMAILS = [
    "Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.", # (B) Broken or defective item
    "Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?", # (A) Pre-sale question OR (D) Other (please explain)
    "I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???", # (C) Billing question
    "How did I get here I am not good with computer.  Halp." # (D) Other (please explain)
]

# Correct categorizations stored as a list of lists to accommodate the possibility of multiple correct categorizations per email
ANSWERS = [
    ["B"],
    ["A","D"],
    ["C"],
    ["D"]
]

# Dictionary of string values for each category to be used for regex grading
REGEX_CATEGORIES = {
    "A": "A",
    "B": "B",
    "C": "C",
    "D": "D"
}

# Iterate through list of emails
for i,email in enumerate(EMAILS):
    
    # Substitute the email text into the email placeholder variable
    formatted_prompt = PROMPT.format(email=email)
   
    # Get Claude's response
    response = get_completion(formatted_prompt, prefill=PREFILL)

    # Grade Claude's response
    grade = any([bool(re.search(REGEX_CATEGORIES[ans], response)) for ans in ANSWERS[i]])
    
    # Print Claude's response
    print("--------------------------- Full prompt with variable substutions ---------------------------")
    print("USER TURN")
    print(formatted_prompt)
    print("\nASSISTANT TURN")
    print(PREFILL)
    print("\n------------------------------------- Claude's response -------------------------------------")
    print(response)
    print("\n------------------------------------------ GRADING ------------------------------------------")
    print("This exercise has been correctly solved:", grade, "\n\n\n\n\n\n")
     
❓ If you want a hint, run the cell below!


from hints import exercise_6_2_hint; print(exercise_6_2_hint)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Prompt
PROMPT = """Is this movie review sentiment positive or negative?

This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900."""

# Print Claude's response
print(get_completion(PROMPT))
     

# System prompt
SYSTEM_PROMPT = "You are a savvy reader of movie reviews."

# Prompt
PROMPT = """Is this review sentiment positive or negative? First, write the best arguments for each side in  and  XML tags, then answer.

This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900."""

# Print Claude's response
print(get_completion(PROMPT, SYSTEM_PROMPT))
     

# Prompt
PROMPT = """Is this review sentiment negative or positive? First write the best arguments for each side in  and  XML tags, then answer.

This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since 1900."""

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Name a famous movie starring an actor who was born in the year 1956."

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = "Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in  tags, then give your answer."

# Print Claude's response
print(get_completion(PROMPT))
     
---

Chapter 7: Using Examples (Few-Shot Prompting)
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt="", prefill=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt},
          {"role": "assistant", "content": prefill}
        ]
    )
    return message.content[0].text
     
Lesson
Giving Claude examples of how you want it to behave (or how you want it not to behave) is extremely effective for:

Getting the right answer
Getting the answer in the right format
This sort of prompting is also called "few shot prompting". You might also encounter the phrase "zero-shot" or "n-shot" or "one-shot". The number of "shots" refers to how many examples are used within the prompt.

Examples
Pretend you're a developer trying to build a "parent bot" that responds to questions from kids. Claude's default response is quite formal and robotic. This is going to break a child's heart.


# Prompt
PROMPT = "Will Santa bring me presents on Christmas?"

# Print Claude's response
print(get_completion(PROMPT))
     
You could take the time to describe your desired tone, but it's much easier just to give Claude a few examples of ideal responses.


# Prompt
PROMPT = """Please complete the conversation by writing the next line, speaking as "A".
Q: Is the tooth fairy real?
A: Of course, sweetie. Wrap up your tooth and put it under your pillow tonight. There might be something waiting for you in the morning.
Q: Will Santa bring me presents on Christmas?"""

# Print Claude's response
print(get_completion(PROMPT))
     
In the following formatting example, we could walk Claude step by step through a set of formatting instructions on how to extract names and professions and then format them exactly the way we want, or we could just provide Claude with some correctly-formatted examples and Claude can extrapolate from there. Note the <individuals> in the assistant turn to start Claude off on the right foot.


# Prompt template with a placeholder for the variable content
PROMPT = """Silvermist Hollow, a charming village, was home to an extraordinary group of individuals.
Among them was Dr. Liam Patel, a neurosurgeon who revolutionized surgical techniques at the regional medical center.
Olivia Chen was an innovative architect who transformed the village's landscape with her sustainable and breathtaking designs.
The local theater was graced by the enchanting symphonies of Ethan Kovacs, a professionally-trained musician and composer.
Isabella Torres, a self-taught chef with a passion for locally sourced ingredients, created a culinary sensation with her farm-to-table restaurant, which became a must-visit destination for food lovers.
These remarkable individuals, each with their distinct talents, contributed to the vibrant tapestry of life in Silvermist Hollow.

1. Dr. Liam Patel [NEUROSURGEON]
2. Olivia Chen [ARCHITECT]
3. Ethan Kovacs [MISICIAN AND COMPOSER]
4. Isabella Torres [CHEF]


At the heart of the town, Chef Oliver Hamilton has transformed the culinary scene with his farm-to-table restaurant, Green Plate. Oliver's dedication to sourcing local, organic ingredients has earned the establishment rave reviews from food critics and locals alike.
Just down the street, you'll find the Riverside Grove Library, where head librarian Elizabeth Chen has worked diligently to create a welcoming and inclusive space for all. Her efforts to expand the library's offerings and establish reading programs for children have had a significant impact on the town's literacy rates.
As you stroll through the charming town square, you'll be captivated by the beautiful murals adorning the walls. These masterpieces are the work of renowned artist, Isabella Torres, whose talent for capturing the essence of Riverside Grove has brought the town to life.
Riverside Grove's athletic achievements are also worth noting, thanks to former Olympic swimmer-turned-coach, Marcus Jenkins. Marcus has used his experience and passion to train the town's youth, leading the Riverside Grove Swim Team to several regional championships.

1. Oliver Hamilton [CHEF]
2. Elizabeth Chen [LIBRARIAN]
3. Isabella Torres [ARTIST]
4. Marcus Jenkins [COACH]


Oak Valley, a charming small town, is home to a remarkable trio of individuals whose skills and dedication have left a lasting impact on the community.
At the town's bustling farmer's market, you'll find Laura Simmons, a passionate organic farmer known for her delicious and sustainably grown produce. Her dedication to promoting healthy eating has inspired the town to embrace a more eco-conscious lifestyle.
In Oak Valley's community center, Kevin Alvarez, a skilled dance instructor, has brought the joy of movement to people of all ages. His inclusive dance classes have fostered a sense of unity and self-expression among residents, enriching the local arts scene.
Lastly, Rachel O'Connor, a tireless volunteer, dedicates her time to various charitable initiatives. Her commitment to improving the lives of others has been instrumental in creating a strong sense of community within Oak Valley.
Through their unique talents and unwavering dedication, Laura, Kevin, and Rachel have woven themselves into the fabric of Oak Valley, helping to create a vibrant and thriving small town."""

# Prefill for Claude's response
PREFILL = ""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN:")
print(PROMPT)
print("\nASSISTANT TURN:")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 7.1 - Email Formatting via Examples
Exercise 7.1 - Email Formatting via Examples
We're going to redo Exercise 6.2, but this time, we're going to edit the PROMPT to use "few-shot" examples of emails + proper classification (and formatting) to get Claude to output the correct answer. We want the last letter of Claude's output to be the letter of the category.

Refer to the comments beside each email in the EMAILS list if you forget which letter category is correct for each email.

Remember that these are the categories for the emails:

(A) Pre-sale question
(B) Broken or defective item
(C) Billing question
(D) Other (please explain)

# Prompt template with a placeholder for the variable content
PROMPT = """Please classify this email as either green or blue: {email}"""

# Prefill for Claude's response
PREFILL = ""

# Variable content stored as a list
EMAILS = [
    "Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.", # (B) Broken or defective item
    "Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?", # (A) Pre-sale question OR (D) Other (please explain)
    "I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???", # (C) Billing question
    "How did I get here I am not good with computer.  Halp." # (D) Other (please explain)
]

# Correct categorizations stored as a list of lists to accommodate the possibility of multiple correct categorizations per email
ANSWERS = [
    ["B"],
    ["A","D"],
    ["C"],
    ["D"]
]

# Iterate through list of emails
for i,email in enumerate(EMAILS):
    
    # Substitute the email text into the email placeholder variable
    formatted_prompt = PROMPT.format(email=email)
   
    # Get Claude's response
    response = get_completion(formatted_prompt, prefill=PREFILL)

    # Grade Claude's response
    grade = any([bool(re.search(ans, response[-1])) for ans in ANSWERS[i]])
    
    # Print Claude's response
    print("--------------------------- Full prompt with variable substutions ---------------------------")
    print("USER TURN")
    print(formatted_prompt)
    print("\nASSISTANT TURN")
    print(PREFILL)
    print("\n------------------------------------- Claude's response -------------------------------------")
    print(response)
    print("\n------------------------------------------ GRADING ------------------------------------------")
    print("This exercise has been correctly solved:", grade, "\n\n\n\n\n\n")
     
❓ If you want a hint, run the cell below!


from hints import exercise_7_1_hint; print(exercise_7_1_hint)
     
Still stuck? Run the cell below for an example solution.


from hints import exercise_7_1_solution; print(exercise_7_1_solution)
     
Congrats!
If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!

Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


# Prompt
PROMPT = "Will Santa bring me presents on Christmas?"

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt
PROMPT = """Please complete the conversation by writing the next line, speaking as "A".
Q: Is the tooth fairy real?
A: Of course, sweetie. Wrap up your tooth and put it under your pillow tonight. There might be something waiting for you in the morning.
Q: Will Santa bring me presents on Christmas?"""

# Print Claude's response
print(get_completion(PROMPT))
     

# Prompt template with a placeholder for the variable content
PROMPT = """Silvermist Hollow, a charming village, was home to an extraordinary group of individuals.
Among them was Dr. Liam Patel, a neurosurgeon who revolutionized surgical techniques at the regional medical center.
Olivia Chen was an innovative architect who transformed the village's landscape with her sustainable and breathtaking designs.
The local theater was graced by the enchanting symphonies of Ethan Kovacs, a professionally-trained musician and composer.
Isabella Torres, a self-taught chef with a passion for locally sourced ingredients, created a culinary sensation with her farm-to-table restaurant, which became a must-visit destination for food lovers.
These remarkable individuals, each with their distinct talents, contributed to the vibrant tapestry of life in Silvermist Hollow.

1. Dr. Liam Patel [NEUROSURGEON]
2. Olivia Chen [ARCHITECT]
3. Ethan Kovacs [MISICIAN AND COMPOSER]
4. Isabella Torres [CHEF]


At the heart of the town, Chef Oliver Hamilton has transformed the culinary scene with his farm-to-table restaurant, Green Plate. Oliver's dedication to sourcing local, organic ingredients has earned the establishment rave reviews from food critics and locals alike.
Just down the street, you'll find the Riverside Grove Library, where head librarian Elizabeth Chen has worked diligently to create a welcoming and inclusive space for all. Her efforts to expand the library's offerings and establish reading programs for children have had a significant impact on the town's literacy rates.
As you stroll through the charming town square, you'll be captivated by the beautiful murals adorning the walls. These masterpieces are the work of renowned artist, Isabella Torres, whose talent for capturing the essence of Riverside Grove has brought the town to life.
Riverside Grove's athletic achievements are also worth noting, thanks to former Olympic swimmer-turned-coach, Marcus Jenkins. Marcus has used his experience and passion to train the town's youth, leading the Riverside Grove Swim Team to several regional championships.

1. Oliver Hamilton [CHEF]
2. Elizabeth Chen [LIBRARIAN]
3. Isabella Torres [ARTIST]
4. Marcus Jenkins [COACH]


Oak Valley, a charming small town, is home to a remarkable trio of individuals whose skills and dedication have left a lasting impact on the community.
At the town's bustling farmer's market, you'll find Laura Simmons, a passionate organic farmer known for her delicious and sustainably grown produce. Her dedication to promoting healthy eating has inspired the town to embrace a more eco-conscious lifestyle.
In Oak Valley's community center, Kevin Alvarez, a skilled dance instructor, has brought the joy of movement to people of all ages. His inclusive dance classes have fostered a sense of unity and self-expression among residents, enriching the local arts scene.
Lastly, Rachel O'Connor, a tireless volunteer, dedicates her time to various charitable initiatives. Her commitment to improving the lives of others has been instrumental in creating a strong sense of community within Oak Valley.
Through their unique talents and unwavering dedication, Laura, Kevin, and Rachel have woven themselves into the fabric of Oak Valley, helping to create a vibrant and thriving small town."""

# Prefill for Claude's response
PREFILL = ""

# Print Claude's response
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN:")
print(PROMPT)
print("\nASSISTANT TURN:")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))

---

Chapter 9: Complex Prompts from Scratch
Lesson
Exercises
Example Playground
Setup
Run the following setup cell to load your API key and establish the get_completion helper function.


%pip install anthropic

# Import python's built-in regular expression library
import re
import anthropic

# Retrieve the API_KEY & MODEL_NAME variables from the IPython store
%store -r API_KEY
%store -r MODEL_NAME

client = anthropic.Anthropic(api_key=API_KEY)

def get_completion(prompt: str, system_prompt="", prefill=""):
    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2000,
        temperature=0.0,
        system=system_prompt,
        messages=[
          {"role": "user", "content": prompt},
          {"role": "assistant", "content": prefill}
        ]
    )
    return message.content[0].text
     
Lesson
Congratulations on making it to the last chapter! Now time to put everything together and learn how to create unique and complex prompts.

Below, you will be using a guided structure that we recommend for complex prompts. In latter parts of this chapter, we will show you some industry-specific prompts and explain how those prompts are similarly structured.

Note: Not all prompts need every element of the following complex structure. We encourage you to play around with and include or disinclude elements and see how it affects Claude's response. It is usually best to use many prompt elements to get your prompt working first, then refine and slim down your prompt afterward.

Example - Career Coach Chatbot
The following structure combines multiple prompt engineering elements and is a good starting point for complex prompts. The ordering matters for some elements, not for others. We will note when best practices indicate ordering matters, but in general, if you stick to this ordering, it will be a good start to a stellar prompt.

For the following example, we will be building a prompt for a controlled roleplay wherein Claude takes on a situational role with a specific task. Our goal is to prompt Claude to act as a friendly career coach.

Read then run the cell below to compile the various prompt elements into one whole prompt.


######################################## INPUT VARIABLES ########################################

# First input variable - the conversation history (this can also be added as preceding `user` and `assistant` messages in the API call)
HISTORY = """Customer: Give me two possible careers for sociology majors.

Joe: Here are two potential careers for sociology majors:

- Social worker - Sociology provides a strong foundation for understanding human behavior and social systems. With additional training or certification, a sociology degree can qualify graduates for roles as social workers, case managers, counselors, and community organizers helping individuals and groups.

- Human resources specialist - An understanding of group dynamics and organizational behavior from sociology is applicable to careers in human resources. Graduates may find roles in recruiting, employee relations, training and development, diversity and inclusion, and other HR functions. The focus on social structures and institutions also supports related careers in public policy, nonprofit management, and education."""

# Second input variable - the user's question
QUESTION = "Which of the two careers requires more than a Bachelor's degree?"



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = "You will be acting as an AI career coach named Joe created by the company AdAstra Careers. Your goal is to give career advice to users. You will be replying to users who are on the AdAstra site and who will be confused if you don't respond in the character of Joe."

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = "You should maintain a friendly customer service tone."

##### Prompt element 4: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = """Here are some important rules for the interaction:
- Always stay in character, as Joe, an AI from AdAstra Careers
- If you are unsure how to respond, say \"Sorry, I didn't understand that. Could you rephrase your question?\"
- If someone asks something irrelevant, say, \"Sorry, I am Joe and I give career advice. Do you have a career question today I can help you with?\""""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = """Here is an example of how to respond in a standard interaction:

Customer: Hi, how were you created and what do you do?
Joe: Hello! My name is Joe, and I was created by AdAstra Careers to give career advice. What can I help you with today?
"""

##### Prompt element 6: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = f"""Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:

{HISTORY}


Here is the user's question:

{QUESTION}
"""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = "How do you respond to the user's question?"

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = "Think about your answer first before you respond."

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = "Put your response in  tags."

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = "[Joe] "



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
     
Now let's run the prompt! Run the cell below to see Claude's output.


print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
Example - Legal Services
Prompts within the legal profession can be quite complex due to the need to:

Parse long documents
Deal with complex topics
Format output in very specific ways
Follow multi-step analytical processes
Let's see how we can use the complex prompt template to structure a prompt for a specific legal use-case. Below, we've detailed out an example prompt for a legal use-case wherein we ask Claude to answer questions about a legal issue using information from a legal document.

We've changed around the ordering of a few elements to showcase that prompt structure can be flexible!

Prompt engineering is about scientific trial and error. We encourage you to mix and match, move things around (the elements where ordering doesn't matter), and see what works best for you and your needs.


######################################## INPUT VARIABLES ########################################

# First input variable - the legal document
LEGAL_RESEARCH = """

The animal health industry became caught up in a number of patent and trademark lawsuits during the past year. In 1994, Barclay Slocum obtained patents for the tibial plateau leveling osteotomy procedure, which is used in the treatment of dogs with cranial cruciate ligament rupture, and for the devices used in the procedure. During 2006, Slocum Enterprises filed a patent infringement suit against New Generation Devices, arguing that the Unity Cruciate Plate manufactured by New Generation infringed on the patent for the Slocum TPLO plate. However, the court never reached a decision on the issue of patent infringement, ruling that it did not have jurisdiction on the basis of the small number of plates sold in the state in which the case was filed and the information provided on a Web site maintained by Slocum Enterprises. Other patent battles waged during 2006 concerned the use of laser technology for onychectomy in cats, pet identification chips, pig vaccines, and pet “deshedding” tools.


In Canada, the British Columbia Veterinary Medical Association brought suit against a nonveterinarian, claiming that he engaged in cutting or otherwise removing hooks from horses' teeth and floating horses' teeth with power and manual tools, provided advice and diagnoses in return for a fee, and held himself out as being qualified and willing to provide treatment with respect to these activities. The court held that the intention of the legislature in passing the Veterinary Profession Act was the protection of the public and animals and further held that monopolistic statutes serve the purpose of protecting the public. In addition, the court concluded that dentistry, at its core, relates to the health of the teeth and gums; is distinct from cosmetic and other types of care of animals; and, therefore, falls under the definition of the practice of veterinary medicine. The nonveterinarian was enjoined from providing services without a veterinarian supervising the procedures.


The aftermath of Hurricane Katrina, which hit the Gulf Coast of the United States during 2005, spurred changes to the way animals are treated during natural disasters. In 2006, Hawaii, Louisiana, and New Hampshire all enacted laws that address issues regarding the care of animals during disasters, such as providing shelters for pets and allowing service animals to be kept with the people they serve. In addition, Congress passed, and the President signed, the Pet Evacuation and Transportation Standards Act during 2006, which requires state and local emergency preparedness authorities to include in their evacuation plans information on how they will accommodate household pets and service animals in case of a disaster. California passed a law that will require its Office of Emergency Services, Department of Agriculture, and other agencies involved with disaster response preparation to develop a plan for the needs of service animals, livestock, equids, and household pets in the event of a disaster or major emergency.

"""

# Second input variable - the user's question
QUESTION = "Are there any laws about what to do with pets during a hurricane?"



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = "You are an expert lawyer."

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = ""

##### Prompt element 4: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = f"""Here is some research that's been compiled. Use it to answer a legal question from the user.

{LEGAL_RESEARCH}
"""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = """When citing the legal research in your answer, please use brackets containing the search index ID, followed by a period. Put these at the end of the sentence that's doing the citing. Examples of proper citation format:



The statute of limitations expires after 10 years for crimes like this. [3].


However, the protection does not apply when it has been specifically waived by both parties. [5].

"""

##### Prompt element 6: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = f"""Write a clear, concise answer to this question:


{QUESTION}


It should be no more than a couple of paragraphs. If possible, it should conclude with a single sentence directly answering the user's question. However, if there is not sufficient information in the compiled research to produce such an answer, you may demur and write "Sorry, I do not have sufficient information at hand to answer this question."."""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = ""

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = "Before you answer, pull out the most relevant quotes from the research in  tags."

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = "Put your two-paragraph response in  tags."

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = ""



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
     
Now let's run the prompt! Run the cell below to see Claude's output.


print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the Example Playground.

Exercises
Exercise 9.1 - Financial Services Chatbot
Exercise 9.2 - Codebot
Exercise 9.1 - Financial Services Chatbot
Prompts within the financial profession can also be quite complex due to reasons similar to legal prompts. Here's an exercise for a financial use-case, wherein Claude is used to analyze tax information and answer questions. Just like with the legal services example, we've changed around the ordering of a few elements, as our solution prompt makes more sense with a different flow (however, other structures would also work).

We suggest you read through the variable content (in this case, {QUESTION} and {TAX_CODE}) to understand what content Claude is expected to work with. Be sure to reference {QUESTION} and {TAX_CODE} directly in your prompt somewhere (using f-string syntax like in the other examples) so that the actual variable content can be substituted in.

Fill in the prompt element fields with content that match the description and the examples you've seen in the preceding examples of complex prompts. Once you have filled out all the prompt elements that you want to fill out, run the cell to see the concatenated prompt as well as Claude's response.

Remember that prompt engineering is rarely purely formulaic, especially for large and complex prompts! It's important to develop test cases and try a variety of prompts and prompt structures to see what works best for each situation. Note that if you do change the ordering of the prompt elements, you should also remember to change the ordering of the concatenaton in the COMBINE ELEMENTS section.


######################################## INPUT VARIABLES ########################################

# First input variable - the user's question
QUESTION = "How long do I have to make an 83b election?"

# Second input variable - the tax code document that Claude will be using to answer the user's question
TAX_CODE = """
(a)General rule
If, in connection with the performance of services, property is transferred to any person other than the person for whom such services are performed, the excess of—
(1)the fair market value of such property (determined without regard to any restriction other than a restriction which by its terms will never lapse) at the first time the rights of the person having the beneficial interest in such property are transferable or are not subject to a substantial risk of forfeiture, whichever occurs earlier, over
(2)the amount (if any) paid for such property,
shall be included in the gross income of the person who performed such services in the first taxable year in which the rights of the person having the beneficial interest in such property are transferable or are not subject to a substantial risk of forfeiture, whichever is applicable. The preceding sentence shall not apply if such person sells or otherwise disposes of such property in an arm’s length transaction before his rights in such property become transferable or not subject to a substantial risk of forfeiture.
(b)Election to include in gross income in year of transfer
(1)In general
Any person who performs services in connection with which property is transferred to any person may elect to include in his gross income for the taxable year in which such property is transferred, the excess of—
(A)the fair market value of such property at the time of transfer (determined without regard to any restriction other than a restriction which by its terms will never lapse), over
(B)the amount (if any) paid for such property.
If such election is made, subsection (a) shall not apply with respect to the transfer of such property, and if such property is subsequently forfeited, no deduction shall be allowed in respect of such forfeiture.
(2)Election
An election under paragraph (1) with respect to any transfer of property shall be made in such manner as the Secretary prescribes and shall be made not later than 30 days after the date of such transfer. Such election may not be revoked except with the consent of the Secretary.

(c)Special rules
For purposes of this section—
(1)Substantial risk of forfeiture
The rights of a person in property are subject to a substantial risk of forfeiture if such person’s rights to full enjoyment of such property are conditioned upon the future performance of substantial services by any individual.

(2)Transferability of property
The rights of a person in property are transferable only if the rights in such property of any transferee are not subject to a substantial risk of forfeiture.

(3)Sales which may give rise to suit under section 16(b) of the Securities Exchange Act of 1934
So long as the sale of property at a profit could subject a person to suit under section 16(b) of the Securities Exchange Act of 1934, such person’s rights in such property are—
(A)subject to a substantial risk of forfeiture, and
(B)not transferable.
(4)For purposes of determining an individual’s basis in property transferred in connection with the performance of services, rules similar to the rules of section 72(w) shall apply.
(d)Certain restrictions which will never lapse
(1)Valuation
In the case of property subject to a restriction which by its terms will never lapse, and which allows the transferee to sell such property only at a price determined under a formula, the price so determined shall be deemed to be the fair market value of the property unless established to the contrary by the Secretary, and the burden of proof shall be on the Secretary with respect to such value.

(2)Cancellation
If, in the case of property subject to a restriction which by its terms will never lapse, the restriction is canceled, then, unless the taxpayer establishes—
(A)that such cancellation was not compensatory, and
(B)that the person, if any, who would be allowed a deduction if the cancellation were treated as compensatory, will treat the transaction as not compensatory, as evidenced in such manner as the Secretary shall prescribe by regulations,
the excess of the fair market value of the property (computed without regard to the restrictions) at the time of cancellation over the sum of—
(C)the fair market value of such property (computed by taking the restriction into account) immediately before the cancellation, and
(D)the amount, if any, paid for the cancellation,
shall be treated as compensation for the taxable year in which such cancellation occurs.
(e)Applicability of section
This section shall not apply to—
(1)a transaction to which section 421 applies,
(2)a transfer to or from a trust described in section 401(a) or a transfer under an annuity plan which meets the requirements of section 404(a)(2),
(3)the transfer of an option without a readily ascertainable fair market value,
(4)the transfer of property pursuant to the exercise of an option with a readily ascertainable fair market value at the date of grant, or
(5)group-term life insurance to which section 79 applies.
(f)Holding period
In determining the period for which the taxpayer has held property to which subsection (a) applies, there shall be included only the period beginning at the first time his rights in such property are transferable or are not subject to a substantial risk of forfeiture, whichever occurs earlier.

(g)Certain exchanges
If property to which subsection (a) applies is exchanged for property subject to restrictions and conditions substantially similar to those to which the property given in such exchange was subject, and if section 354, 355, 356, or 1036 (or so much of section 1031 as relates to section 1036) applied to such exchange, or if such exchange was pursuant to the exercise of a conversion privilege—
(1)such exchange shall be disregarded for purposes of subsection (a), and
(2)the property received shall be treated as property to which subsection (a) applies.
(h)Deduction by employer
In the case of a transfer of property to which this section applies or a cancellation of a restriction described in subsection (d), there shall be allowed as a deduction under section 162, to the person for whom were performed the services in connection with which such property was transferred, an amount equal to the amount included under subsection (a), (b), or (d)(2) in the gross income of the person who performed such services. Such deduction shall be allowed for the taxable year of such person in which or with which ends the taxable year in which such amount is included in the gross income of the person who performed such services.

(i)Qualified equity grants
(1)In general
For purposes of this subtitle—
(A)Timing of inclusion
If qualified stock is transferred to a qualified employee who makes an election with respect to such stock under this subsection, subsection (a) shall be applied by including the amount determined under such subsection with respect to such stock in income of the employee in the taxable year determined under subparagraph (B) in lieu of the taxable year described in subsection (a).

(B)Taxable year determined
The taxable year determined under this subparagraph is the taxable year of the employee which includes the earliest of—
(i)the first date such qualified stock becomes transferable (including, solely for purposes of this clause, becoming transferable to the employer),
(ii)the date the employee first becomes an excluded employee,
(iii)the first date on which any stock of the corporation which issued the qualified stock becomes readily tradable on an established securities market (as determined by the Secretary, but not including any market unless such market is recognized as an established securities market by the Secretary for purposes of a provision of this title other than this subsection),
(iv)the date that is 5 years after the first date the rights of the employee in such stock are transferable or are not subject to a substantial risk of forfeiture, whichever occurs earlier, or
(v)the date on which the employee revokes (at such time and in such manner as the Secretary provides) the election under this subsection with respect to such stock.
(2)Qualified stock
(A)In general
For purposes of this subsection, the term “qualified stock” means, with respect to any qualified employee, any stock in a corporation which is the employer of such employee, if—
(i)such stock is received—
(I)in connection with the exercise of an option, or
(II)in settlement of a restricted stock unit, and
(ii)such option or restricted stock unit was granted by the corporation—
(I)in connection with the performance of services as an employee, and
(II)during a calendar year in which such corporation was an eligible corporation.
(B)Limitation
The term “qualified stock” shall not include any stock if the employee may sell such stock to, or otherwise receive cash in lieu of stock from, the corporation at the time that the rights of the employee in such stock first become transferable or not subject to a substantial risk of forfeiture.

(C)Eligible corporation
For purposes of subparagraph (A)(ii)(II)—
(i)In general
The term “eligible corporation” means, with respect to any calendar year, any corporation if—
(I)no stock of such corporation (or any predecessor of such corporation) is readily tradable on an established securities market (as determined under paragraph (1)(B)(iii)) during any preceding calendar year, and
(II)such corporation has a written plan under which, in such calendar year, not less than 80 percent of all employees who provide services to such corporation in the United States (or any possession of the United States) are granted stock options, or are granted restricted stock units, with the same rights and privileges to receive qualified stock.
(ii)Same rights and privileges
For purposes of clause (i)(II)—
(I)except as provided in subclauses (II) and (III), the determination of rights and privileges with respect to stock shall be made in a similar manner as under section 423(b)(5),
(II)employees shall not fail to be treated as having the same rights and privileges to receive qualified stock solely because the number of shares available to all employees is not equal in amount, so long as the number of shares available to each employee is more than a de minimis amount, and
(III)rights and privileges with respect to the exercise of an option shall not be treated as the same as rights and privileges with respect to the settlement of a restricted stock unit.
(iii)Employee
For purposes of clause (i)(II), the term “employee” shall not include any employee described in section 4980E(d)(4) or any excluded employee.

(iv)Special rule for calendar years before 2018
In the case of any calendar year beginning before January 1, 2018, clause (i)(II) shall be applied without regard to whether the rights and privileges with respect to the qualified stock are the same.

(3)Qualified employee; excluded employee
For purposes of this subsection—
(A)In general
The term “qualified employee” means any individual who—
(i)is not an excluded employee, and
(ii)agrees in the election made under this subsection to meet such requirements as are determined by the Secretary to be necessary to ensure that the withholding requirements of the corporation under chapter 24 with respect to the qualified stock are met.
(B)Excluded employee
The term “excluded employee” means, with respect to any corporation, any individual—
(i)who is a 1-percent owner (within the meaning of section 416(i)(1)(B)(ii)) at any time during the calendar year or who was such a 1 percent owner at any time during the 10 preceding calendar years,
(ii)who is or has been at any prior time—
(I)the chief executive officer of such corporation or an individual acting in such a capacity, or
(II)the chief financial officer of such corporation or an individual acting in such a capacity,
(iii)who bears a relationship described in section 318(a)(1) to any individual described in subclause (I) or (II) of clause (ii), or
(iv)who is one of the 4 highest compensated officers of such corporation for the taxable year, or was one of the 4 highest compensated officers of such corporation for any of the 10 preceding taxable years, determined with respect to each such taxable year on the basis of the shareholder disclosure rules for compensation under the Securities Exchange Act of 1934 (as if such rules applied to such corporation).
(4)Election
(A)Time for making election
An election with respect to qualified stock shall be made under this subsection no later than 30 days after the first date the rights of the employee in such stock are transferable or are not subject to a substantial risk of forfeiture, whichever occurs earlier, and shall be made in a manner similar to the manner in which an election is made under subsection (b).

(B)Limitations
No election may be made under this section with respect to any qualified stock if—
(i)the qualified employee has made an election under subsection (b) with respect to such qualified stock,
(ii)any stock of the corporation which issued the qualified stock is readily tradable on an established securities market (as determined under paragraph (1)(B)(iii)) at any time before the election is made, or
(iii)such corporation purchased any of its outstanding stock in the calendar year preceding the calendar year which includes the first date the rights of the employee in such stock are transferable or are not subject to a substantial risk of forfeiture, unless—
(I)not less than 25 percent of the total dollar amount of the stock so purchased is deferral stock, and
(II)the determination of which individuals from whom deferral stock is purchased is made on a reasonable basis.
(C)Definitions and special rules related to limitation on stock redemptions
(i)Deferral stock
For purposes of this paragraph, the term “deferral stock” means stock with respect to which an election is in effect under this subsection.

(ii)Deferral stock with respect to any individual not taken into account if individual holds deferral stock with longer deferral period
Stock purchased by a corporation from any individual shall not be treated as deferral stock for purposes of subparagraph (B)(iii) if such individual (immediately after such purchase) holds any deferral stock with respect to which an election has been in effect under this subsection for a longer period than the election with respect to the stock so purchased.

(iii)Purchase of all outstanding deferral stock
The requirements of subclauses (I) and (II) of subparagraph (B)(iii) shall be treated as met if the stock so purchased includes all of the corporation’s outstanding deferral stock.

(iv)Reporting
Any corporation which has outstanding deferral stock as of the beginning of any calendar year and which purchases any of its outstanding stock during such calendar year shall include on its return of tax for the taxable year in which, or with which, such calendar year ends the total dollar amount of its outstanding stock so purchased during such calendar year and such other information as the Secretary requires for purposes of administering this paragraph.

(5)Controlled groups
For purposes of this subsection, all persons treated as a single employer under section 414(b) shall be treated as 1 corporation.

(6)Notice requirement
Any corporation which transfers qualified stock to a qualified employee shall, at the time that (or a reasonable period before) an amount attributable to such stock would (but for this subsection) first be includible in the gross income of such employee—
(A)certify to such employee that such stock is qualified stock, and
(B)notify such employee—
(i)that the employee may be eligible to elect to defer income on such stock under this subsection, and
(ii)that, if the employee makes such an election—
(I)the amount of income recognized at the end of the deferral period will be based on the value of the stock at the time at which the rights of the employee in such stock first become transferable or not subject to substantial risk of forfeiture, notwithstanding whether the value of the stock has declined during the deferral period,
(II)the amount of such income recognized at the end of the deferral period will be subject to withholding under section 3401(i) at the rate determined under section 3402(t), and
(III)the responsibilities of the employee (as determined by the Secretary under paragraph (3)(A)(ii)) with respect to such withholding.
(7)Restricted stock units
This section (other than this subsection), including any election under subsection (b), shall not apply to restricted stock units.
"""



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = ""

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = ""

##### Prompt element 4: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = ""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = ""

##### Prompt element 6: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = ""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = ""

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = ""

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = ""

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = ""



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
❓ If you want to see a possible solution, run the cell below!


from hints import exercise_9_1_solution; print(exercise_9_1_solution)
     
Exercise 9.2 - Codebot
In this exercise, we will write up a prompt for a coding assistance and teaching bot that reads code and offers guiding corrections when appropriate. Fill in the prompt element fields with content that match the description and the examples you've seen in the preceding examples of complex prompts. Once you have filled out all the prompt elements that you want to fill out, run the cell to see the concatenated prompt as well as Claude's response.

We suggest you read through the variable content (in this case, {CODE}) to understand what content Claude is expected to work with. Be sure to reference {CODE} directly in your prompt somewhere (using f-string syntax like in the other examples) so that the actual variable content can be substituted in.


######################################## INPUT VARIABLES ########################################

# Input variable - the code that Claude needs to read and assist the user with correcting
CODE = """
# Function to print multiplicative inverses
def print_multiplicative_inverses(x, n):
  for i in range(n):
    print(x / i) 
"""



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = ""

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = ""

##### Prompt element 4: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = ""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = ""

##### Prompt element 6: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = ""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = ""

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = ""

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = ""

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = ""



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     
❓ If you want to see a possible solution, run the cell below!


from hints import exercise_9_2_solution; print(exercise_9_2_solution)
     
Congratulations & Next Steps!
If you made it through all the exercises, you are now in the top 0.1% of LLM whisperers. One of the elite!

The techniques you've learned, from thinking step by step to assigning roles to using examples to general all-around clear writing, can be merged, remixed, and adapted in countless ways.

Prompt engineering is a very new discipline, so keep an open mind. You could be the one to discover the next great prompting trick.

If you want to see more examples of good prompts for inspiration:

Learn from examples of production-ready prompts from our cookbook
Read through our prompting guide
Check out our prompt library for inspiration
Try our experimental metaprompt to get Claude to write prompt templates for you!
Ask questions in our discord server
Learn about the Anthropic API parameters like temperature and max_tokens
If you're feeling academic, read some papers on prompt engineering
Practice building prompts to get Claude to do something you're interested in
If you want to learn about some truly advanced prompting techniques beyond the scope of this tutorial, click through to the appendix! But first, run the cell below.


# Prompt
PROMPT = "Write an ode to a fabulous student who has just completed a course on prompt engineering, in the form of a sonnet."

# Print Claude's response
print(get_completion(PROMPT))
     
Example Playground
This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses.


######################################## INPUT VARIABLES ########################################

# First input variable - the conversation history (this can also be added as preceding `user` and `assistant` messages in the API call)
HISTORY = """Customer: Give me two possible careers for sociology majors.

Joe: Here are two potential careers for sociology majors:

- Social worker - Sociology provides a strong foundation for understanding human behavior and social systems. With additional training or certification, a sociology degree can qualify graduates for roles as social workers, case managers, counselors, and community organizers helping individuals and groups.

- Human resources specialist - An understanding of group dynamics and organizational behavior from sociology is applicable to careers in human resources. Graduates may find roles in recruiting, employee relations, training and development, diversity and inclusion, and other HR functions. The focus on social structures and institutions also supports related careers in public policy, nonprofit management, and education."""

# Second input variable - the user's question
QUESTION = "Which of the two careers requires more than a Bachelor's degree?"



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = "You will be acting as an AI career coach named Joe created by the company AdAstra Careers. Your goal is to give career advice to users. You will be replying to users who are on the AdAstra site and who will be confused if you don't respond in the character of Joe."

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = "You should maintain a friendly customer service tone."

##### Prompt element 4: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = """Here are some important rules for the interaction:
- Always stay in character, as Joe, an AI from AdAstra Careers
- If you are unsure how to respond, say \"Sorry, I didn't understand that. Could you rephrase your question?\"
- If someone asks something irrelevant, say, \"Sorry, I am Joe and I give career advice. Do you have a career question today I can help you with?\""""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = """Here is an example of how to respond in a standard interaction:

Customer: Hi, how were you created and what do you do?
Joe: Hello! My name is Joe, and I was created by AdAstra Careers to give career advice. What can I help you with today?
"""

##### Prompt element 6: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = f"""Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:

{HISTORY}


Here is the user's question:

{QUESTION}
"""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = "How do you respond to the user's question?"

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = "Think about your answer first before you respond."

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = "Put your response in  tags."

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = "[Joe] "



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))
     

######################################## INPUT VARIABLES ########################################

# First input variable - the legal document
LEGAL_RESEARCH = """

The animal health industry became caught up in a number of patent and trademark lawsuits during the past year. In 1994, Barclay Slocum obtained patents for the tibial plateau leveling osteotomy procedure, which is used in the treatment of dogs with cranial cruciate ligament rupture, and for the devices used in the procedure. During 2006, Slocum Enterprises filed a patent infringement suit against New Generation Devices, arguing that the Unity Cruciate Plate manufactured by New Generation infringed on the patent for the Slocum TPLO plate. However, the court never reached a decision on the issue of patent infringement, ruling that it did not have jurisdiction on the basis of the small number of plates sold in the state in which the case was filed and the information provided on a Web site maintained by Slocum Enterprises. Other patent battles waged during 2006 concerned the use of laser technology for onychectomy in cats, pet identification chips, pig vaccines, and pet “deshedding” tools.


In Canada, the British Columbia Veterinary Medical Association brought suit against a nonveterinarian, claiming that he engaged in cutting or otherwise removing hooks from horses' teeth and floating horses' teeth with power and manual tools, provided advice and diagnoses in return for a fee, and held himself out as being qualified and willing to provide treatment with respect to these activities. The court held that the intention of the legislature in passing the Veterinary Profession Act was the protection of the public and animals and further held that monopolistic statutes serve the purpose of protecting the public. In addition, the court concluded that dentistry, at its core, relates to the health of the teeth and gums; is distinct from cosmetic and other types of care of animals; and, therefore, falls under the definition of the practice of veterinary medicine. The nonveterinarian was enjoined from providing services without a veterinarian supervising the procedures.


The aftermath of Hurricane Katrina, which hit the Gulf Coast of the United States during 2005, spurred changes to the way animals are treated during natural disasters. In 2006, Hawaii, Louisiana, and New Hampshire all enacted laws that address issues regarding the care of animals during disasters, such as providing shelters for pets and allowing service animals to be kept with the people they serve. In addition, Congress passed, and the President signed, the Pet Evacuation and Transportation Standards Act during 2006, which requires state and local emergency preparedness authorities to include in their evacuation plans information on how they will accommodate household pets and service animals in case of a disaster. California passed a law that will require its Office of Emergency Services, Department of Agriculture, and other agencies involved with disaster response preparation to develop a plan for the needs of service animals, livestock, equids, and household pets in the event of a disaster or major emergency.

"""

# Second input variable - the user's question
QUESTION = "Are there any laws about what to do with pets during a hurricane?"



######################################## PROMPT ELEMENTS ########################################

##### Prompt element 1: `user` role
# Make sure that your Messages API call always starts with a `user` role in the messages array.
# The get_completion() function as defined above will automatically do this for you.

##### Prompt element 2: Task context
# Give Claude context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.
# It's best to put context early in the body of the prompt.
TASK_CONTEXT = "You are an expert lawyer."

##### Prompt element 3: Tone context
# If important to the interaction, tell Claude what tone it should use.
# This element may not be necessary depending on the task.
TONE_CONTEXT = ""

##### Prompt element 4: Input data to process
# If there is data that Claude needs to process within the prompt, include it here within relevant XML tags.
# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.
# This element may not be necessary depending on task. Ordering is also flexible.
INPUT_DATA = f"""Here is some research that's been compiled. Use it to answer a legal question from the user.

{LEGAL_RESEARCH}
"""

##### Prompt element 5: Examples
# Provide Claude with at least one example of an ideal response that it can emulate. Encase this in  XML tags. Feel free to provide multiple examples.
# If you do provide multiple examples, give Claude context about what it is an example of, and enclose each example in its own set of XML tags.
# Examples are probably the single most effective tool in knowledge work for getting Claude to behave as desired.
# Make sure to give Claude examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.
# Generally more examples = better.
EXAMPLES = """When citing the legal research in your answer, please use brackets containing the search index ID, followed by a period. Put these at the end of the sentence that's doing the citing. Examples of proper citation format:



The statute of limitations expires after 10 years for crimes like this. [3].


However, the protection does not apply when it has been specifically waived by both parties. [5].

"""

##### Prompt element 6: Detailed task description and rules
# Expand on the specific tasks you want Claude to do, as well as any rules that Claude might have to follow.
# This is also where you can give Claude an "out" if it doesn't have an answer or doesn't know.
# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.
TASK_DESCRIPTION = """Write a clear, concise answer to this question:


{QUESTION}


It should be no more than a couple of paragraphs. If possible, it should conclude with a single sentence directly answering the user's question. However, if there is not sufficient information in the compiled research to produce such an answer, you may demur and write "Sorry, I do not have sufficient information at hand to answer this question."."""

##### Prompt element 7: Immediate task description or request #####
# "Remind" Claude or tell Claude exactly what it's expected to immediately do to fulfill the prompt's task.
# This is also where you would put in additional variables like the user's question.
# It generally doesn't hurt to reiterate to Claude its immediate task. It's best to do this toward the end of a long prompt.
# This will yield better results than putting this at the beginning.
# It is also generally good practice to put the user's query close to the bottom of the prompt.
IMMEDIATE_TASK = ""

##### Prompt element 8: Precognition (thinking step by step)
# For tasks with multiple steps, it's good to tell Claude to think step by step before giving an answer
# Sometimes, you might have to even say "Before you give your answer..." just to make sure Claude does this first.
# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.
PRECOGNITION = "Before you answer, pull out the most relevant quotes from the research in  tags."

##### Prompt element 9: Output formatting
# If there is a specific way you want Claude's response formatted, clearly tell Claude what that format is.
# This element may not be necessary depending on the task.
# If you include it, putting it toward the end of the prompt is better than at the beginning.
OUTPUT_FORMATTING = "Put your two-paragraph response in  tags."

##### Prompt element 10: Prefilling Claude's response (if any)
# A space to start off Claude's answer with some prefilled words to steer Claude's behavior or response.
# If you want to prefill Claude's response, you must put this in the `assistant` role in the API call.
# This element may not be necessary depending on the task.
PREFILL = ""



######################################## COMBINE ELEMENTS ########################################

PROMPT = ""

if TASK_CONTEXT:
    PROMPT += f"""{TASK_CONTEXT}"""

if TONE_CONTEXT:
    PROMPT += f"""\n\n{TONE_CONTEXT}"""

if INPUT_DATA:
    PROMPT += f"""\n\n{INPUT_DATA}"""

if EXAMPLES:
    PROMPT += f"""\n\n{EXAMPLES}"""

if TASK_DESCRIPTION:
    PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

if IMMEDIATE_TASK:
    PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

if PRECOGNITION:
    PROMPT += f"""\n\n{PRECOGNITION}"""

if OUTPUT_FORMATTING:
    PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""

# Print full prompt
print("--------------------------- Full prompt with variable substutions ---------------------------")
print("USER TURN")
print(PROMPT)
print("\nASSISTANT TURN")
print(PREFILL)
print("\n------------------------------------- Claude's response -------------------------------------")
print(get_completion(PROMPT, prefill=PREFILL))

---


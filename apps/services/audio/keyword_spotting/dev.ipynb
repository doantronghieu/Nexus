{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clap-ipa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from clap.encoders import PhoneEncoder, SpeechEncoder\n",
    "from transformers import AutoProcessor, DebertaV2Tokenizer\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load encoders and move to device\n",
    "speech_encoder = SpeechEncoder.from_pretrained('anyspeech/clap-ipa-tiny-speech')\n",
    "phone_encoder = PhoneEncoder.from_pretrained('anyspeech/clap-ipa-tiny-phone')\n",
    "phone_encoder.eval().to(device)\n",
    "speech_encoder.eval().to(device)\n",
    "\n",
    "# Initialize tokenizer and processor\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('charsiu/IPATokenizer')\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-tiny')\n",
    "\n",
    "# Process inputs\n",
    "audio_input = processor(some_audio)\n",
    "ipa_input = tokenizer(some_ipa_string)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    speech_embed = speech_encoder(audio_input)\n",
    "    phone_embed = phone_encoder(ipa_input)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = F.cosine_similarity(speech_embed, phone_embed, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from clap.encoders import PhoneEncoder, SpeechEncoder\n",
    "from transformers import AutoProcessor, DebertaV2Tokenizer\n",
    "\n",
    "# Global configuration\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        execution_time = (time.time() - start_time) * 1000\n",
    "        print(f\"{func.__name__} took {execution_time:.2f} ms\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self):\n",
    "        self.speech_encoder = None\n",
    "        self.phone_encoder = None\n",
    "        self.tokenizer = None\n",
    "        self.processor = None\n",
    "\n",
    "    @measure_time\n",
    "    def initialize_models(self) -> None:\n",
    "        \"\"\"Initialize and configure all required models and tokenizers.\"\"\"\n",
    "        # Load models\n",
    "        self.speech_encoder = SpeechEncoder.from_pretrained('anyspeech/clap-ipa-tiny-speech')\n",
    "        self.phone_encoder = PhoneEncoder.from_pretrained('anyspeech/clap-ipa-tiny-phone')\n",
    "\n",
    "        # Configure models for inference\n",
    "        self._setup_inference_mode()\n",
    "\n",
    "        # Load tokenizers\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained('charsiu/IPATokenizer')\n",
    "        self.processor = AutoProcessor.from_pretrained('openai/whisper-tiny')\n",
    "\n",
    "    def _setup_inference_mode(self) -> None:\n",
    "        \"\"\"Configure models for optimal inference performance.\"\"\"\n",
    "        for model in [self.speech_encoder, self.phone_encoder]:\n",
    "            model.eval()\n",
    "            model.to(DEVICE)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "    @measure_time\n",
    "    def process_audio(self, audio_path: str, cache_dir: str = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Process audio file and prepare for model input.\"\"\"\n",
    "        sample, _ = sf.read(audio_path)\n",
    "        sample = np.array(sample, dtype=np.float32)\n",
    "\n",
    "        audio_input = self.processor(\n",
    "            sample,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {k: v.to(DEVICE) for k, v in audio_input.items()}\n",
    "\n",
    "    @measure_time\n",
    "    def process_ipa(self, ipa_list: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Process IPA text and prepare for model input.\"\"\"\n",
    "        ipa_string = ''.join(ipa_list)\n",
    "        transcript_tokens = torch.tensor(\n",
    "            self.tokenizer(\n",
    "                ipa_string,\n",
    "                return_attention_mask=False,\n",
    "                return_length=True,\n",
    "                return_token_type_ids=False,\n",
    "                add_special_tokens=False\n",
    "            )['input_ids'],\n",
    "            device=DEVICE\n",
    "        )\n",
    "        return transcript_tokens\n",
    "\n",
    "    @measure_time\n",
    "    def compute_similarity(\n",
    "        self,\n",
    "        audio_input: Dict[str, torch.Tensor],\n",
    "        transcript_tokens: torch.Tensor\n",
    "    ) -> float:\n",
    "        \"\"\"Compute similarity between audio and IPA transcription.\"\"\"\n",
    "        audio_input = {k: v.to(DEVICE) for k, v in audio_input.items()}\n",
    "        transcript_tokens = transcript_tokens.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            speech_features = self._get_speech_features(audio_input)\n",
    "            phone_features = self._get_phone_features(transcript_tokens)\n",
    "            \n",
    "            speech_embed = torch.mean(speech_features, dim=0, keepdim=True)\n",
    "            phone_embed = torch.mean(phone_features, dim=0, keepdim=True)\n",
    "            \n",
    "            similarity = F.cosine_similarity(speech_embed, phone_embed, dim=1)\n",
    "            \n",
    "        return similarity.item()\n",
    "\n",
    "    def _get_speech_features(self, audio_input: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Extract features from speech input.\"\"\"\n",
    "        return self.speech_encoder(**audio_input, return_dict=True).last_hidden_state.squeeze(0)\n",
    "\n",
    "    def _get_phone_features(self, transcript_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features from phone input.\"\"\"\n",
    "        return self.phone_encoder(transcript_tokens.unsqueeze(0)).last_hidden_state.squeeze(0)\n",
    "\n",
    "def main():\n",
    "    processor = AudioProcessor()\n",
    "\n",
    "    # Initialize models\n",
    "    print(\"\\nInitializing models and tokenizers...\")\n",
    "    processor.initialize_models()\n",
    "\n",
    "    # Process audio\n",
    "    print(\"\\nProcessing audio...\")\n",
    "    audio_path = \"data/audio/intro_model_chatGPT_35.mp3\"\n",
    "    audio_input = processor.process_audio(audio_path)\n",
    "\n",
    "    # Process IPA string\n",
    "    print(\"\\nProcessing IPA string...\")\n",
    "    ipa_string = ['d íi', 'pi', 'ti']\n",
    "    transcript_tokens = processor.process_ipa(ipa_string)\n",
    "\n",
    "    # Compute similarity\n",
    "    print(\"\\nComputing similarity...\")\n",
    "    start_total = time.time()\n",
    "    similarity_score = processor.compute_similarity(audio_input, transcript_tokens)\n",
    "    total_time = (time.time() - start_total) * 1000\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Similarity score: {similarity_score:.4f}\")\n",
    "    print(f\"Total inference time: {total_time:.2f} ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref\n",
    "\n",
    "- https://github.com/search?q=Keyword+spotting&type=repositories&s=stars&o=desc\n",
    "  - https://github.com/wenet-e2e/wekws\n",
    "  - https://github.com/harvard-edge/multilingual_kws\n",
    "  - https://github.com/lingjzhu/clap-ipa\n",
    "- https://paperswithcode.com/task/keyword-spotting\n",
    "- https://huggingface.co/anyspeech?sort_models=likes#models\n",
    "- https://huggingface.co/search/full-text?q=keyword+spotting&type=model&type=space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
